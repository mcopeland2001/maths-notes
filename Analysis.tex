\documentclass[MathsNotesBase.tex]{subfiles}




\date{\vspace{-6ex}}


\begin{document}
\searchableSubsection{\chapterTitle{Analysis}}{analysis}{\bigskip\bigskip}

	\searchableSubsection{\sectionTitle{Supremum and Infimum}}{analysis}{\bigskip}
	
	\bigskip\bigskip
	\searchableSubsection{Definitions}{analysis}{\bigskip
		\boxeddefinition{An upper bound on a set A is a value $x$ such that,
			\[ \forall a \in A, \, a \leq x \]
			and a lower bound is similarly defined as a value $y$ such that,
			\[ \forall a \in A, \, a \geq y. \]
			A set is said to be \textbf{upper-bounded} if there exists some upper-bound on the set and is said to be \textbf{lower-bounded} if there exists some lower bound on the set. If there exists both upper and lower bounds then the set is said to be \textbf{bounded}.
		}
		\boxeddefinition{The \textbf{supremum} of a upper-bounded set $A$ is a value $\sigma_A$ such that $\sigma_A$ is an upper bound on $A$ and, 
			\[ \sigma_A' < \sigma_A \iff \exists a \in A \suchthat a > \sigma_A' \]
			which is to say that if $\sigma_A' < \sigma_A$ then $\sigma_A'$ is not an upper bound on $A$ and, if $\sigma_A'$ is not an upper bound on $A$ then it must be less than $\sigma_A$ since $\sigma_A$ is an upper bound on A.\\
			An alternative, equivalent definition is,
			\[ \forall \epsilon > 0, \, \exists a \in A \suchthat a > \sigma_A - \epsilon. \]
		}
		\paragraph{Issue} Note that there is an apparent paradox here: This second definition implies that
		\begin{align*}
			&&  \forall \epsilon > 0 \logicsep \exists a \in A &\suchthat a + \epsilon > \sigma_A  \\
			&\iff &\exists a \in A &\suchthat a \geq \sigma_A  &\sidecomment{}
		\end{align*}
		which result, when combined with the upper-bound property, gives
		\begin{align*}
			&& \exists a \in A &\suchthat (a \geq \sigma_A) \wedge (a \leq \sigma_A) \\
			&\iff &\exists a \in A &\suchthat a = \sigma_A  &\sidecomment{}
		\end{align*}
		which says that there is always an element in the bounded set that is equal to the supremum. This is not correct - the supremum may be in the set or external to it.\\
		The initial implication is not true, however. We cannot infer that ${ \exists a \in A \suchthat a \geq \sigma_A }$. This can be seen with another rearrangement,
		\begin{align*}
			&&  \forall \epsilon > 0 \logicsep \exists a \in A &\suchthat a + \epsilon > \sigma_A  \\
			&\iff &\forall \epsilon > 0 \logicsep \exists a \in A &\suchthat \epsilon > \sigma_A - a  &\sidecomment{}
		\end{align*}
		which shows us that for any positive epsilon there needs to be an $a$ close enough to the value of $\sigma_A$ that the difference in their values is less than epsilon. Since $a$ can approach arbitrarily close to $\sigma_A$ this is achievable for any positive epsilon. This property seems to be equivalent to the fact that $\sigma_A$ is a \textit{limit point} of $A$ but that will be covered properly in Topology.
		\bigskip\bigskip
		
		\boxeddefinition{The \textbf{infimum} of a lower-bounded set $A$ is defined similarly to the supremum: as a value $\tau_A$ such that $\tau_A$ is a lower bound on $A$ and, 
			\[ \tau_A' > \tau_A \iff \exists a \in A \suchthat a < \tau_A' \]
			or alternatively,
			\[ \forall \epsilon > 0, \, \exists a \in A \suchthat a < \tau_A + \epsilon. \]
		}
		\notation{The supremum of $A$ is denoted $sup \, A$ and the infimum is denoted $inf \, A$.}
	}
	\searchableSubsection{Deductions using the supremum and infimum}{analysis}{\bigskip
		\labeledProposition{If a bounded set $A \subset \R{}$ has the property that,
			\[ \forall x,y \in A \logicsep \abs{x - y} < 1 \]
			then it follows that,
			\[ (sup \, A - inf \, A) \leq 1. \]
		}{sup_minus_inf_max_difference}
		\begin{proof}
			Let $ \sigma_A = sup \, A $ and $ \tau_A = inf \, A $ and \WLOG assume that $x > y$. By the definitions of the supremum and infimum we have,
			\begin{align*}
			&& \forall \epsilon > 0 \logicsep \exists x,y \in A &\logicsep (x > \sigma_A - \epsilon) \wedge (y < \tau_A + \epsilon)  \\
			&\iff & \forall \epsilon > 0 \logicsep \exists x,y \in A &\logicsep (x > \sigma_A - \epsilon) \wedge (-y > -\tau_A - \epsilon)  &\sidecomment{}\\
			&\iff & \forall \epsilon > 0 \logicsep \exists x,y \in A &\logicsep (x - y) > (\sigma_A - \tau_A) - 2\epsilon  &\sidecomment{}\\
			\end{align*}
			Now suppose, for contradiction, that $ (\sigma_A - \tau_A) > 1 $ then we can say that,
			\[ \exists r > 0 \logicsep (\sigma_A - \tau_A) = 1 + r. \] 
			If we then constrict $\epsilon$ such that,
			\[ \epsilon < \frac{r}{2} \iff 2\epsilon < r \iff r - 2\epsilon > 0 \]
			then the previous result tells us that, for $ 0 < \epsilon < \frac{r}{2} $,
			\begin{align*}
			&& \exists x,y \in A &\logicsep (x - y) > (\sigma_A - \tau_A) - 2\epsilon  \\
			&\iff & \exists x,y \in A &\logicsep (x - y) > 1 + r - 2\epsilon > 1  &\sidecomment{}
			\end{align*}
			which contradicts the set property that $ \forall x,y \in A, \, \abs{x - y} < 1 $. So this shows that $ (\sigma_A - \tau_A) \leq 1 $.
		\end{proof}
		\bigskip
		\labeledProposition{Let $A \subset \R{}$ be a bounded set and let $B$ be the set defined by
			\[ B = \setc{b}{b = f(a), \; a \in A} \]
			where the function $f$ is some strictly monotonic function.
			Then it follows that,
			\[ sup \, B = f(sup \, A). \]
		}{monotonic_functions_preserve_supremum}
		\begin{proof}
			A is bounded and so $\sigma_A = sup \, A$ exists. So, using the supremum properties we have,
			\begin{align*}
			&& \forall a \in A \logicsep a &\leq \sigma_A  \\
			&\iff & \forall a \in A \logicsep f(a)  &\leq f(\sigma_A)  &\sidecomment{by monotonicity of f}\\
			&\iff & \forall b \in B \logicsep b  &\leq f(\sigma_A) 
			\end{align*}
			which is to say that $\sigma_B = f(\sigma_A)$ is an upper bound on $B$.\\
			Furthermore, using the other supremum property, we have that,
			\begin{align*}
			&& \sigma_A' < \sigma_A &\implies \exists a \in A \suchthat a > \sigma_A' \\
			&\iff & f(\sigma_A') < f(\sigma_A)  &\implies \exists a \in A \suchthat f(a) > f(\sigma_A')  &\sidecomment{by strict monotonicity of f}\\
			&\iff & \sigma_B' < \sigma_B  &\implies \exists b \in B \suchthat b > \sigma_B'.	
			\end{align*}
			Therefore $\sigma_B$ satisfies both requirements of the supremum and we have shown that,
			\[ sup \, B = f(sup \, A). \]
		\end{proof}
	}
	
	
	\pagebreak
	\searchableSubsection{\sectionTitle{Limits}}{analysis}{\bigskip}
	
	\bigskip\bigskip
	\searchableSubsection{Limits of sequences}{analysis}{\bigskip
		\subsubsection{Problems with the informal description of a limit}
		If we say that a sequence tends to some value $L$ when the terms of the sequence \textit{gets closer and closer to} $L$ we have the following problems:
		\begin{itemize}
			\item{that the sequence gets closer and closer to many numbers so that this does not specify a single specific limit.}
			\item{that the sequence can have a limit but it's not the case that every term is closer than the previous term to the limit. For example,
				\[ a_{2k} = 1/k, \; a_{2k - 1} = \frac{1}{k+1} \]
				tends to 0 but $ a_{2k} > a_{2k - 1} $.
			}
		\end{itemize}
		\bigskip
		\boxeddefinition{A sequence $a_n$ is said to \textbf{tend} to $L$ or have the \textbf{limit} $L$ iff,
			\[ \forall \epsilon > 0 \in \R{} ,\, \exists N \in \N{} \suchthat \forall n > N ,\, \abs{a_{n} - L} < \epsilon. \]
		}
		\boxeddefinition{The interval $ (L - \epsilon, L + \epsilon) $ is called the \textbf{\mbox{$\epsilon$-neighbourhood of $L$}}.}
		
		\boxeddefinition{A sequence $a_n$ is said to \textbf{tend to infinity} iff,
			\[ \forall M > 0 \in \R{} ,\, \exists N \in \N{} \suchthat \forall n > N ,\, a_{n} > M \]
			and \textbf{tend to minus-infinity} iff,
			\[ \forall M < 0 \in \R{} ,\, \exists N \in \N{} \suchthat \forall n > N ,\, a_{n} < M. \]
		}
	
		\boxeddefinition{A sequence that has a limit is called \textbf{convergent} and otherwise is called \textbf{divergent}. Note that \textbf{divergent} sequences include both sequences that remain bounded but oscillate without converging and those that tend to infinity (or minus-infinity).}
		
		\subsubsection{Examples of Convergence and Divergence}\bigskip
			\begin{exe}
				\ex {\textbf{Non-convergent Oscillation}\\\\
				The sequence $ a_n = (-1)^n $ is divergent despite always remaining bounded within the interval $ [-1, 1] $ as it neither converges to 1 or to -1.}\label{ex:flipping_sign}
				\ex {\textbf{Limit of an Infinite Recurrence}\\\\
					Take the sequence given by,
					\[ a_1 = 1, \; a_{n+1} = \frac{a_n}{2} + \frac{3}{2a_n} \;\; (n \ge 1). \]
					Assume there is an equilibrium value, $a^*$, then
					\begin{align*}
						&&a^* &= \frac{a^*}{2} + \frac{3}{2a^*}  \\
						&\iff &2(a^*)^2  &= (a^*)^2 + 3  &\sidecomment{}\\
						&\iff &(a^*)^2  &= 3  &\sidecomment{}\\
						&\iff &a^*  &= \sqrt{3}  &\sidecomment{$\forall n, a_n \geq 0$}\\
					\end{align*}
					So $\sqrt{3}$ is the steady-state value that this recurrence converges to as $n \to \infty$. If the recurrence didn't converge then the assumption of an equilibrium value would result in a contradiction. Note, however, that the fact that there is an equilibrium value does \textit{not}, by itself, prove that this sequence converges (although this sequence does).
				}\label{ex:infinite_recurrence}
			\end{exe}
			
		\bigskip	
		\labeledProposition{A sequence has at most one limit. In other words, a sequence can only converge, if at all, to a single unique value.}{uniqueness_of_limits}
		\begin{proof}
			Let $L$ and $L'$ both be limits of the sequence $a_n$, and the constant $\alpha = L - L'$. Then,
			\[ \forall \epsilon > 0 \in \R{} ,\, \exists N \in \N{} \suchthat \forall n > N ,\, \abs{a_{n} - L} < \epsilon \] and
			\[ \forall \epsilon' > 0 \in \R{} ,\, \exists N' \in \N{} \suchthat \forall n' > N' ,\, \abs{a_{n'} - L'} < \epsilon'. \]
			But also we have,
			\[ \abs{a_n - L'} = \abs{(a_n - L) + (L - L')} = \abs{(L - L') + (a_n - L)} \]
			and using the triangle inequality,
			\begin{align*}
			&& \abs{L - L'} = \abs{(L - L') + (a_n - L) - (a_n - L)} &\leq \abs{(L - L') + (a_n - L)} + \abs{-(a_n - L)} \\
			&\iff & \abs{L - L'}  &\leq \abs{(L - L') + (a_n - L)} + \abs{a_n - L}\\
			&\iff & \abs{L - L'} - \abs{a_n - L}  &\leq \abs{(L - L') + (a_n - L)}\\
			&\iff & \abs{\alpha} - \abs{a_n - L}  &\leq \abs{\alpha + (a_n - L)}\\
			\end{align*}
			Since $\alpha = L - L'$ is constant we can consider the situation when $\epsilon = \frac{\abs{\alpha}}{2}$ then we have that,
			\begin{align*}
			&& \exists N \in \N{} \suchthat \forall n > N ,\, \abs{a_{n} - L} &< \epsilon = \frac{\abs{\alpha}}{2} \\
			&\iff &  -\abs{a_{n} - L} &> -\frac{\abs{\alpha}}{2}  &\sidecomment{}\\
			&\iff & \abs{\alpha} - \abs{a_{n} - L} &> \abs{\alpha} - \frac{\abs{\alpha}}{2}  &\sidecomment{}\\
			&\iff & \abs{\alpha} - \abs{a_{n} - L} &> \frac{\abs{\alpha}}{2}.  &\sidecomment{}\\
			\end{align*}
			Combining this with the previous result gives, for $\forall n > N$,
			\[ \frac{\abs{\alpha}}{2} < \abs{\alpha} - \abs{(a_n - L)}  \leq \abs{\alpha + (a_n - L)} = \abs{a_n - L'} \]
			which, by rearranging a little, is,
			\[ \forall n > N,\, \abs{a_n - L'} > \frac{\abs{\alpha}}{2}. \]
			But this means that if we also choose $\epsilon' = \frac{\abs{\alpha}}{2}$ then there is no $N'$ such that $\forall n' > N' ,\, \abs{a_{n'} - L'} < \epsilon'$ which contradicts the hypothesis that $L'$ is also a limit of $a_n$.
		\end{proof}
		\begin{proof}
			Another quicker way of proving the proposition is by letting $ \epsilon = \epsilon' = \frac{\abs{\alpha}}{2} $ so that,
			\[ 2\epsilon = \abs{\alpha} = \abs{L - L'} = \abs{L - a_n + a_n - L'} \leq \abs{L - a_n} + \abs{a_n - L'} = \abs{a_n - L} + \abs{a_n - L'} \]
			which gives us,
			\[ 2\epsilon \leq \abs{a_n - L} + \abs{a_n - L'}. \]
			But by the limit definition,
			\[ \abs{a_n - L} + \abs{a_n - L'} < \epsilon + \epsilon' \]
			and since we have set $ \epsilon = \epsilon' $ then,
			\[ \abs{a_n - L} + \abs{a_n - L'} < 2\epsilon \]
			which contradicts $ 2\epsilon \leq \abs{a_n - L} + \abs{a_n - L'} $.
		\end{proof}
	
		\bigskip\bigskip
		\boxeddefinition{If $a_n$ is a sequence and $ S = \setc{a_n}{n \in \N{}} $ then $a_n$ is said to be \textbf{bounded below} if $S$ has a lower bound and \textbf{bounded above} if $S$ has an upper bound, and \textbf{bounded} if it is bounded above and below.}
		
		\bigskip
		\begin{lemma}
			Any finite set of elements from an ordered field has a minimum and a maximum.
		\end{lemma}
		\begin{proof}
			This can be proven quite easily using induction. Taking the base case of a set of cardinality one, clearly there is a maximum and a minimum both of which are the sole element of the set. Then, the induction step is to say, given a set $S$ that has a maximum, $s_{max}$, and a minimum, $s_{min}$, if we add a new element $e$, then if $e$ is greater than $s_{max}$ it is the maximum of the new set and if it is less than $s_{min}$ it is the minimum of the new set. Otherwise, the previous maximum and minimum also pertain to the new set. Therefore, adding a new element to a set that has a maximum and a minimum creates a new set with a maximum and a minimum.
		\end{proof}
		\bigskip
		\labeledProposition{Any convergent sequence is bounded.}{convergent_seqs_bounded}
		\begin{proof}
			Firstly, we need to prove that any finite sequence is bounded. We can do this simply by observing that any finite set of elements from an ordered field,
			\[ S = \{a_1, a_2, \dots , a_n\} \]
			has a minimum and a maximum.\\
			Now, let $a_n$ be an arbitrary convergent sequence so that,
			\[ \forall \epsilon > 0 \logicsep \exists N \in \N{} \logicsep \forall n > N \logicsep \abs{a_n - L} < \epsilon \]
			for some $L \in \R{}$.\\\\
			Then, let $S_{max}$ and $S_{min}$ be the maximum and minimum respectively of the first $N$ terms of $a_n$, $S = \{a_1, a_2, \dots , a_N\}$, and,
			\[ \exists \epsilon > 0 \logicsep \forall n > N \logicsep \abs{a_n - L} < \epsilon \]
			so that, for $n > N$, the sequence $a_n$ is bounded in the $\epsilon$-neighbourhood of $L$.\\
			So, if we define ${ m = min\{S_{min}, L - \epsilon\} }$ and ${ M = max\{S_{max}, L + \epsilon\} }$, then the whole sequence $a_n$ for all $n \in \N{}$ is bounded below by $m$ and bounded above by $M$.\\
			Therefore $a_n$ is bounded. 
		\end{proof}
	
		\bigskip
		\boxeddefinition{An \textbf{increasing} sequence is a sequence $a_n$ such that,
			\[ \forall n \in \N{} \logicsep a_{n+1} \geq a_n \]
			and \textbf{decreasing} if,
			\[ \forall n \in \N{} \logicsep a_{n+1} \leq a_n \]
			and \textbf{monotonic} if either increasing or decreasing.
		}
		
		\bigskip
		\labeledProposition{Any increasing sequence that is bounded above has a limit.}{increasing_bounded_seqs_have_limit}
		\begin{proof}
			Let $a_n$ be an increasing sequence that is bounded above. Then,
			\[ \forall n \in \N{} \logicsep a_{n+1} \geq a_n \]
			and let ${ S = \setc{a_n}{n \in \N{}} }$. Since $a_n$ is bounded above it has a supremum. Let $ \sigma = sup \; S $ so that,
			\[ \forall a_n \in S \logicsep a_n \leq \sigma \eqand \forall \epsilon > 0 \logicsep \exists a_n \in S \logicsep a_n > \sigma - \epsilon. \]
			Therefore, for some arbitrary fixed $\epsilon > 0$,
			\[ \exists a_n \in S \logicsep a_n > \sigma - \epsilon \] 
			and setting $ N = n $ so that $ a_N > \sigma - \epsilon $, we have,
			\[ \forall n > N \in \N{} \logicsep a_n \geq a_N > \sigma - \epsilon \]
			and so, recalling that $\sigma$ is an upper bound on $S$,
			\begin{align*}
			&& \exists N \logicsep \forall n > N \in \N{} &\logicsep (a_n > \sigma - \epsilon) \wedge (a_n \leq \sigma) \\
			&\iff &\exists N \logicsep \forall n > N \in \N{} &\logicsep a_n \leq \sigma < a_n + \epsilon  &\sidecomment{}\\
			&\iff &\exists N \logicsep \forall n > N \in \N{} &\logicsep 0 \leq \sigma - a_n < \epsilon  &\sidecomment{}\\
			&\implies &\exists N \logicsep \forall n > N \in \N{} &\logicsep \abs{\sigma - a_n} < \epsilon.
			\end{align*}
			But $\epsilon$ was an arbitrary positive value so,
			\[ \forall \epsilon > 0 \logicsep \exists N \logicsep \forall n > N \in \N{} \logicsep \abs{\sigma - a_n} < \epsilon \]
			and $\sigma$ is, therefore, the limit of $a_n.$
		\end{proof}
		
		\bigskip
		\begin{corollary}
			Any increasing sequence that is bounded above converges to the supremum of its elements (terms, values, etc.).
		\end{corollary}
		\begin{corollary}
			A decreasing sequence that is bounded below converges to the infimum of its elements.
		\end{corollary}
	}

	\bigskip\bigskip
	\searchableSubsection{Algebra of limits of sequences}{analysis}{\bigskip
		\labeledProposition{
			Let $a_n$ and $b_n$ be convergent sequences with limits $ a $ and $ b $,
			respectively. Let $ C $ be a real number and let $ k $ be a positive integer. Then as $n \to \infty$,
			\begin{enumerate}[label=\alph*)]
				\item{$ Ca_n \to Ca $}
				\item{$ \abs{a_n} \to \abs{a} $}
				\item{$ a_n + b_n \to a + b $}
				\item{$ a_nb_n \to ab $}
				\item{$ a_n^k \to a^k $}
				\item{if, for all $n,\, b_n \neq 0$ and $b \neq 0$, then $\frac{1}{b_n} \to \frac{1}{b}$}.
			\end{enumerate}
			\smallskip
		}{seq_limit_properties}	
		\begin{proof}
			We prove each property individually in the given order.
			
			\subsubsection{Proof of (a) $ Ca_n \to Ca $} 
			If $ C = 0 $ then $ Ca_n = 0 = Ca $ for all $n$ and the proposition holds trivially. If $ C \neq 0 $ then, since $a_n \to a$,
			\[ \forall \epsilon' > 0 \logicsep \exists N \logicsep \forall n > N \in \N{} \logicsep \abs{a_n - a} < \epsilon'. \]
			Now let $ \epsilon = \abs{C}\epsilon' $. Then,
			\begin{align*}
			&& \forall \epsilon > 0 \logicsep \exists N \logicsep \forall n > N \in \N{} &\logicsep \abs{C}\abs{a_n - a} < \abs{C}\epsilon' = \epsilon  \\
			&\iff & \forall \epsilon > 0 \logicsep \exists N \logicsep \forall n > N \in \N{} &\logicsep \abs{Ca_n - Ca} < \epsilon. &\sidecomment{$\abs{x}\abs{y} = \abs{xy}$} \\
			\end{align*}
			
			\subsubsection{Proof of (b) $ \abs{a_n} \to \abs{a} $} 
			\begin{align*}
			&& \abs{a_n} = \abs{a_n - a + a} &\leq \abs{a_n - a} + \abs{a}  &\sidecomment{the "triangle inequality"}\\
			&\iff & \abs{a_n} - \abs{a} &\leq \abs{a_n - a}. &\sidecomment{(1)} \\
			\end{align*}
			\begin{align*}
			&& \abs{a} = \abs{a - a_n + a_n} &\leq \abs{a - a_n} + \abs{a_n}  &\sidecomment{the "triangle inequality"}\\
			&\iff & \abs{a} - \abs{a_n} &\leq \abs{a - a_n} = \abs{a_n - a} &\sidecomment{} \\
			&\iff & \abs{a_n} - \abs{a} &\geq -\abs{a_n - a}. &\sidecomment{(2)} \\
			\end{align*}
			Putting (1) and (2) together we have,
			\begin{align*}
			&& -\abs{a_n - a} &\leq \abs{a_n} - \abs{a} \leq \abs{a_n - a} &\sidecomment{the "triangle inequality"}\\
			&\iff & \abs{\abs{a_n} - \abs{a}} &\leq \abs{a_n - a}. \\
			\end{align*}
			The fact that $a_n$ converges to $a$ implies that $ \abs{a_n - a} $ converges to zero. Since it is an upper bound on the value of ${ \abs{\abs{a_n} - \abs{a}} }$, the value ${ \abs{\abs{a_n} - \abs{a}} }$ must also converge to zero. Specifically any value of $ n, \epsilon $ such that ${ \abs{a_n - a} < \epsilon }$ will also satisfy ${ \abs{\abs{a_n} - \abs{a}} \leq \abs{a_n - a} < \epsilon }$.
			
			\subsubsection{Proof of (c) $ a_n + b_n \to a + b $}
			Using, again, the "triangle inequality",
			\[ \abs{(a_n + b_n) - (a + b)} = \abs{(a_n - a) + (b_n - b)} \leq \abs{a_n - a} + \abs{b_n - b}. \]
			So, ${ \abs{a_n - a} + \abs{b_n - b} }$ is an upper bound on the value of ${ \abs{(a_n + b_n) - (a + b)} }$. If we take any arbitrary ${ \epsilon > 0 }$ then,
			\[ \exists N_1 \logicsep \forall n > N_1 \in \N{} \logicsep \abs{a_n - a} < \frac{\epsilon}{2} \]
			and
			\[ \exists N_2 \logicsep \forall n > N_2 \in \N{} \logicsep \abs{b_n - b} < \frac{\epsilon}{2}. \]
			Then, if we take $ N = max\{N_1, N_2\} $, we have,
			\[ \forall n > N \in \N{} \logicsep \abs{(a_n + b_n) - (a + b)} \leq \abs{a_n - a} + \abs{b_n - b} < \epsilon. \]
			
			\subsubsection{Proof of (d) $ a_nb_n \to ab $}
			\begin{align*}
			&& \abs{a_nb_n - ab} = \abs{a_nb_n - ab_n + ab_n - ab} &\leq \abs{b_n(a_n - a)} + \abs{a(b_n - b)}  &\sidecomment{the "triangle inequality"}\\
			&\iff & \abs{a_nb_n - ab} &\leq \abs{b_n}\abs{a_n - a} + \abs{a}\abs{b_n - b}  &\sidecomment{} \\
			\end{align*}
			Since $b_n$ converges, by \autoref{prop:convergent_seqs_bounded}, it is bounded. Therefore, $\abs{b_n}$ has some upper bound which we shall call $B$. Then,
			\[ \forall \epsilon > 0 \logicsep \exists N_1 \logicsep \forall n > N_1 \in \N{} \logicsep \abs{a_n - a} < \frac{\epsilon}{2B} \]
			and
			\[ \forall \epsilon > 0 \logicsep \exists N_2 \logicsep \forall n > N_2 \in \N{} \logicsep \abs{b_n - b} < \frac{\epsilon}{2\abs{a}}. \]
			Now, let ${ N = max{N_1, N_2}. }$ Then,
			\[ \forall n > N \in \N{} \logicsep B\abs{a_n - a} + \abs{a}\abs{b_n - b} < \epsilon. \]
			
			\subsubsection{Proof of (e) $ a_n^k \to a^k $}
			Using (d) - and because $k$ is a positive integer - we can do induction on the power $k$.\\
			Base cases 0 and 1 are clearly true as ${ k = 0 }$ results in $a_n$ being the constant 1 for all $n$ and so trivially converges to ${ a = 1 }$; and ${ k = 1 }$ results in the same sequence as $a_n$.\\
			So, we perform the induction step for ${ k \geq 2 }$. Then, by the induction hypothesis, ${ a_n^{k-1} \to a^{k-1} }$. But ${ a_n^k = a_n^{k-1}a_n }$ and, by (d) and the induction hypothesis, we have that ${ a_n^{k-1}a_n \to a^{k-1}a = a^k }$. Therefore, ${ a_n^k \to a^k. }$
			
			\subsubsection{Proof of (f) $ \forall n \logicsep b_n, b \neq 0 \implies \frac{1}{b_n} \to \frac{1}{b} $}
			Again invoking \autoref{prop:convergent_seqs_bounded} and letting the upper bound on the sequence $b_n$ be $B$,
			\[ \abs{\frac{1}{b_n} - \frac{1}{b}} = \abs{\frac{b - b_n}{b_nb}} = \abs{\frac{b_n - b}{b_nb}} = \frac{ \abs{b_n - b} }{ \abs{b_n}\abs{b} } \leq \frac{1}{B\abs{b}}\abs{b_n - b}. \]
			Now, since $\frac{1}{B\abs{b}}$ is a constant we can define the constant ${ C = \frac{1}{B\abs{b}} }$ and then we see that in (a) we have already proven that ${ C\abs{b_n - b} }$ converges to 0. In (a) we used that to prove that ${ Cb_n \to Cb }$ but here it proves that ${ \frac{1}{b_n} \to \frac{1}{b}. }$
		\end{proof}
	}

	\bigskip\bigskip
	\searchableSubsection{Some theorems on limits of sequences}{analysis}{\bigskip 
		\labeledTheorem{If ${ \abs{a} < 1 }$ then ${ \lim_{n \to \infty} a^n = 0. }$}{geom_prog_common_ratio_less_than_one_tends_to_zero}
		\begin{proof}
			First of all, note that if ${ \abs{a} = 0 }$ then ${ a = 0 = a^n }$ for all n and so the limit holds trivially. For this reason, from here on, we will consider only the case where ${ a \neq 0 }$.\\
			There are 3 parts to this proof:
			\begin{enumerate}
				\item{${ \abs{a} < 1 \implies \lim_{n \to \infty} \abs{a}^n = 0, }$ }
				\item{${ \abs{a}^n = \abs{a^n}, }$ }
				\item{${ \lim_{n \to \infty} \abs{a^n} = 0 \implies \lim_{n \to \infty} a^n = 0 }$.}
			\end{enumerate}
			\begin{enumerate}
			\item{${ \bm{\abs{a} < 1 \implies \lim_{n \to \infty} \abs{a}^n = 0} }$}\label{geom_prog_common_ratio_less_than_one_tends_to_zero_1}
			\subitem{
				It would be natural to prove this by showing that,
				\[ \forall \epsilon > 0 \logicsep \exists N \logicsep \forall n > N \in \N{} \logicsep \abs{\abs{a}^n - 0} = \abs{a}^n < \epsilon. \]
				We can show this by "reverse engineering" the value of $N$ from the requirement that ${ \abs{a}^n }$ be less than $\epsilon$,
				\[ \abs{a}^n < \epsilon \iff n\ln{\abs{a}} < \ln{\epsilon} \iff n > \frac{ \ln{\epsilon} }{ \ln{\abs{a}} } \]
				with the last step changing the direction of the inequality because we divide by $\ln{\abs{a}}$ which - remembering that ${ \abs{a} < 1 }$ - is a negative value. So, in this way, we have shown that ${ N = \frac{ \ln{\epsilon} }{ \ln{\abs{a}} } }$ is a general formula that relates a value of $N$ with the required property with any arbitrary $\epsilon$.\\
				However, this proof is not valid because it uses the concept of the logarithm which requires a lot of analysis that has not been proven at this stage. Since we are trying to build the fundamental basis of analysis, at this point we can only use concepts that are pre-requisites (axiomatic) in analysis or have been proven at this stage.
			}
			\subitem{
				An alternative way to show this, using the properties of limits of sequences just proven, is as follows: Let $x_n$ be the sequence ${ x_n = \abs{a}^n }$. Then, because ${ 0 < \abs{a} < 1 }$,
				\[ x_{n+1} = x_n \cdot x = \abs{a}^n\abs{a} < \abs{a}^n = x_n \]
				so that $x_n$ is a decreasing sequence. Additionally, ${ \forall n \in \R{} \logicsep \abs{a}^n \geq 0 }$ so 0 is a lower bound on the sequence. Therefore, the sequence converges to a limit (note we haven't yet established that 0 is the limit - only that it is a candidate). Furthermore, ${ x_{n+1} = \abs{a}^{n+1} = \abs{a}^n\abs{a} }$ and, if ${ x_n \to L }$ then ${ x_{n+1} \to L }$ also. But, putting these two facts together, along with property (d) of limits of sequences, means that,
				\[ L = \lim_{n \to \infty} \abs{a}^{n+1} = \lim_{n \to \infty} \abs{a}^n\abs{a} = (\lim_{n \to \infty} \abs{a}^n)(\lim_{n \to \infty} \abs{a}) = \abs{a}(\lim_{n \to \infty} \abs{a}^n) = \abs{a}L. \]
				So,
				\[ L = \abs{a}L \iff L(1 - \abs{a}) = 0 \]
				and, since we know that ${ \abs{a} \neq 1 }$, therefore $L$ must be 0.
			}
			
			\item{${\bm{ \abs{a}^n = \abs{a^n} }}$}\label{geom_prog_common_ratio_less_than_one_tends_to_zero_2}
			\subitem{
				For ${ a \in \R{}, n \in \N{} }$ it's easy to see that ${ \abs{a}\abs{a}\dots\abs{a} = \abs{aa \dots a} }$.\\
				In actual fact, this appears to hold even for ${ n \in \Q{} }$, e.g.
				\[ \abs{ (-1)^{\frac{1}{2}} } = \abs{i} = 1 = \abs{ 1^{\frac{1}{2}} } = \abs{1} = 1 \]
				but this should be checked when studying complex numbers more thoroughly. Also, the base $a$, can it also be complex?
			}
			
			\item{${\bm{ \lim_{n \to \infty} \abs{a^n} = 0 \implies \lim_{n \to \infty} a^n = 0 }}$}
			\subitem{
				This can be proved directly from the definition of the limit.
				\begin{align*}
				&& \lim_{n \to \infty} \abs{a^n} = 0 &\iff \forall \epsilon > 0 \logicsep \exists N \logicsep \forall n > N \in \N{} \logicsep \abs{\abs{a^n} - 0} < \epsilon  \\
				&&&\iff \forall \epsilon > 0 \logicsep \exists N \logicsep \forall n > N \in \N{} \logicsep \abs{a^n} < \epsilon  &\sidecomment{} \\[3pt]
				&&&\iff \forall \epsilon > 0 \logicsep \exists N \logicsep \forall n > N \in \N{} \logicsep \abs{a^n - 0} < \epsilon  &\sidecomment{} \\[3pt]
				&&&\iff \lim_{n \to \infty} a^n = 0.  &\sidecomment{} \\
				\end{align*}
			}
			\end{enumerate}
								
			Bear in mind that, in general, ${ \lim_{n \to \infty} \abs{x_n} = L \centernot\implies \lim_{n \to \infty} x_n = L }$. For example, if $x_n$ converges to $-L$ then $\abs{x_n}$ will converge to $L$.\\
			\href{https://github.com/mcopeland2001/maths-notes/blob/master/resources/code/python/converge_to_minus_2.py}{\scriptsize\ttfamily{code example}}\\\\
			
			Furthermore, the example \ref{ex:infinite_recurrence} showed how the fact of $a_n$ and $a_{n+1}$ converging to the same limit produces - when the sequence is expressed as a recurrence - an equilibrium value.
		\end{proof}
	
		\subsubsection{The Sandwich Theorem}
		\labeledProposition{Let ${ a_n, b_n, c_n }$ be sequences such that,
			\[ \text{for all } n,\; a_n \leq b_n \leq c_n \hspace{10pt} \text{and} \hspace{10pt} \lim_{n \to \infty} a_n = L = \lim_{n \to \infty} c_n. \]
			Then ${ \lim_{n \to \infty} b_n = L. }$
		}{sandwich_theorem}
		\begin{proof}
			${ \lim_{n \to \infty} a_n = L }$ means that,
			\begin{align*}
			&& \forall \epsilon > 0 \logicsep \exists N_1 \logicsep \forall n > N_1 \in \N{} &\logicsep \abs{a_n - L} < \epsilon  &\sidecomment{} \\
			&\iff & \forall \epsilon > 0 \logicsep \exists N_1 \logicsep \forall n > N_1 \in \N{} &\logicsep -\epsilon < a_n - L < \epsilon  &\sidecomment{} \\
			&\iff & \forall \epsilon > 0 \logicsep \exists N_1 \logicsep \forall n > N_1 \in \N{} &\logicsep L - \epsilon < a_n < L + \epsilon.  &\sidecomment{}
			\end{align*}
			By the same reasoning we also have,
			\[ \forall \epsilon > 0 \logicsep \exists N_2 \logicsep \forall n > N_2 \in \N{} \logicsep L - \epsilon < c_n < L + \epsilon. \]
			So, if we let ${ N = max\{N_1, N_2\} }$ then we have,
			\[ \forall \epsilon > 0 \logicsep \exists N \logicsep \forall n > N \in \N{} \logicsep L - \epsilon < a_n, c_n < L + \epsilon \]
			and since we also know that ${ a_n \leq b_n \leq c_n }$ it follows that,
			\[ \forall \epsilon > 0 \logicsep \exists N \logicsep \forall n > N \in \N{} \logicsep L - \epsilon < a_n \leq b_n \leq c_n < L + \epsilon. \]
			This shows that,
			\[ \forall \epsilon > 0 \logicsep \exists N \logicsep \forall n > N \in \N{} \logicsep \abs{b_n - L} < \epsilon. \]
		\end{proof}
	
		\subsubsection{An example application of the Sandwich Theorem}
		\begin{exe}
			\ex {\textbf{Prove that ${\bm{ \abs{x} < 1 \implies \lim_{n \to \infty} x^n = 0 }}$}\\\\
				We have already proven this using the properties of limits in \autoref{theo:geom_prog_common_ratio_less_than_one_tends_to_zero} but here we are going to prove it using the Sandwich Theorem.\\
				\begin{proof}
					Firstly, as we showed in \ref{geom_prog_common_ratio_less_than_one_tends_to_zero_2}, ${ \abs{x^n - 0} = \abs{x^n} = \abs{x}^n. }$ So to show that $x^n$ tends to zero we can show that $\abs{x}^n$ tends to zero. So, \WLOG we take ${ x > 0 }$ (since ${ x = 0 }$ makes the proposition trivially true). Then, notice that ${ 0 < x < 1 \implies x = \frac{1}{1 + h} }$ for some ${ h > 0 }$. Then, we can show inductively that ${ (1 + h)^n \geq 1 + hn }$ as follows.\\
					\paragraph{Base cases 0, 1:} ${ (1 + h)^0 = 1 = 1 + h(0), \; (1 + h)^1 = 1 + h = 1 + h(1). }$
					\paragraph{Induction step ${\bm{ k > 1 }}$}
					\begin{align*}
					&& (1 + h)^k  &\geq 1 + hk  \\
					&\iff & (1 + h)(1 + h)^k  &\geq (1 + h)(1 + hk) &\sidecomment{} \\
					&\iff & (1 + h)^{k+1}  &\geq 1 + hk + h + h^2k = 1 + h(k + 1) + h^2k > 1 + h(k + 1) &\sidecomment{} \\
					\end{align*}
					This result implies that,
					\[ x^n = \frac{1}{(1 + h)^n} \leq \frac{1}{1 + hn} \]
					so that,
					\[ 0 < x^n \leq \frac{1}{1 + hn}. \]
					Since $h$ is some fixed value, clearly,
					\[ \lim_{n \to \infty} \frac{1}{1 + hn} = 0 \]
					and, obviously, the limit of the constant 0 is always 0 so, by the Sandwich Theorem,
					\[ \lim_{n \to \infty} x^n = 0. \]
				\end{proof}
			}\label{ex:geom_prog_common_ratio_less_than_one_tends_to_zero}
		\end{exe}
	}

	\pagebreak
	\searchableSubsection{Subsequences}{analysis}{\bigskip
		\boxeddefinition{Let ${ (a_n)_{n \in \N{}} }$ be a sequence and consider some strictly increasing natural
			numbers ${ (k_1, k_2, k_3, \dots) }$ that is, ${ (k_1 < k_2 < k_3 < k_4 < \dots). }$ Then the sequence ${ (a_{k_n})_{n \in \N{}} }$ is
			called a \textbf{subsequence} of the sequence ${ (a_n)_{n \in \N{}} }$.\\
			Note that a \textbf{subsequence} is always infinite (I think).
		}
	
		\labeledTheorem{If $a_n$ is a sequence that tends to a limit, then any subsequence of it tends to the same limit.}{subseq_tends_to_same_limit}
		\begin{proof}
			Firstly, notice that if the $n$th index of some subsequence is $k_n$ then $k_n \geq n$ (because the subsequence can only skip terms of the original - it can't add in terms). So then, if we have a sequence $a_n$ that tends to a limit $a$ and an arbitrary subsequence $a_{k_n}$ then,
			\[ \forall \epsilon > 0 \logicsep \exists N \logicsep \forall n > N \in \N{} \logicsep \abs{a_n - a} < \epsilon \implies \abs{a_{k_n} - a} < \epsilon \]
			because ${ k_n \geq n > N }$.
		\end{proof}
	
		\labeledTheorem{Every sequence has a monotonic subsequence.}{every_seq_has_monotonic_subseq}
		\begin{proof}
			Either there is an infinite number of terms that are greater than all the following terms or there is not. If there is not, then after the last such term all terms have a term that follows them that is greater than or equal to them.\\
			In the first case we have a strict monotonic decreasing sequence and in the second we have a non-strict monotonic increasing sequence.
		\end{proof}
	
		If we put this theorem together with what we learned about bounded sequences in \autoref{prop:increasing_bounded_seqs_have_limit} and its corollaries - that monotonic bounded sequences are convergent - then we get one of the most famous results in analysis, The Bolzano-Weierstrass Theorem.
		
		\labeledTheorem{\textbf{The Bolzano-Weierstrass Theorem}\\
			Every bounded sequence has a convergent subsequence.
		}{every_bounded_seq_has_convergent_subseq}
	}

	\pagebreak
	\searchableSubsection{Some problems on limits of sequences}{analysis}{\bigskip
		\subsubsection{\small{ Let ${ (a_n)_{n \in \N{}} }$ be a sequence, and let ${ (b_n)_{n \in \N{}} }$ be the sequence defined by ${ b_n = \abs{a_n} }$ for ${ n \in \N{} }$. Which of the following two statements implies the other? }}
		\textbf{\small{ 
			\begin{enumerate}[label=\alph*)]
				\item{${ (a_n) }$ converges.}
				\item{${ (b_n) }$ converges.}
			\end{enumerate}
		}}
		Answer: ${ a_n }$ converges ${ \implies b_n }$ converges also but ${ b_n }$ converges ${ \centernot\implies a_n }$ converges.\\
		The first implication is because,
		\begin{align*}
		&& \abs{a_n} = \abs{a_n - a + a} &\leq \abs{a_n - a} + \abs{a}  \\
		&\iff &  \abs{a_n} - \abs{a} &\leq \abs{a_n - a}  &\sidecomment{} \\
		\end{align*}
		\begin{align*}
		&& \abs{a} = \abs{a - a_n + a_n} &\leq \abs{a_n - a} + \abs{a_n}  \\
		&\iff & \abs{a} - \abs{a_n} &\leq \abs{a_n - a}  &\sidecomment{} \\
		&\iff & \abs{a_n} - \abs{a} &\geq -\abs{a_n - a}  &\sidecomment{} \\
		\end{align*}
		which both together imply that ${ \abs{ \abs{a_n} - \abs{a} } \leq \abs{a_n - a} }$.\\
		The latter non-implication is easy to see if one thinks of a sequence that consists of two subsequences that converge to 2 and -2. Then, their absolute value would converge to 2 but their values do not converge. Remember \autoref{theo:subseq_tends_to_same_limit}, for a sequence to converge to a limit, every subsequence of it must converge to the same limit.
	}
	
	\bigskip\bigskip
	\searchableSubsection{Limits of functions}{analysis}{\bigskip
		\subsubsection{Definition of the limit of a function}
		\boxeddefinition{Let ${ f: \R{} \mapsto \R{} }$ be a function. We say that $L$ is the \textbf{limit of ${\bm{ f(x) }}$ as ${\bm{ x }}$ approaches ${\bm{ a }}$} if, for each ${ \epsilon > 0 }$ there exists ${ \delta > 0 }$ such that,
			\[ 0 < \abs{x - a} < \delta \implies \abs{f(x) - L} < \epsilon. \]
		}
		\boxeddefinition{Let ${ f: \R{} \mapsto \R{} }$ be a function. We say that \textbf{${\bm{ f(x) }}$ tends to infinity as ${\bm{ x }}$ approaches ${\bm{ a }}$} if, for each ${ K }$ there exists ${ \delta > 0 }$ such that,
			\[ 0 < \abs{x - a} < \delta \implies f(x) > K. \]
		}
	
		\subsubsection{Examples of limits of functions}
		\begin{exe}
			\ex {\textbf{Prove that if ${\bm{ f: \R{} \mapsto \R{} \suchthat f(x) = x^2 + x }}$ then ${\bm{ \lim_{x \to 2} f(x) = 6 }}$.}\\\\
				Let ${ 0 < \abs{x - 2} <\delta }$ and consider some arbitrary ${ \epsilon > 0 }$. Then we have,
				\[ \abs{(x^2 + x) - 6} = \abs{(x - 2)(x + 3)} \leq \abs{x - 2}\abs{x + 3} < \delta \abs{x + 3}. \]
				It's tempting at this point to say that, since we are examining the behaviour when $x$ approaches 2 so we can assume ${ x \approx 2 \iff (x+3) \approx 5 }$ and then we can set ${ \delta = \frac{\epsilon}{6} }$ so that,
				\[ 0 < \abs{x - 2} < \delta = \frac{\epsilon}{6} \implies \abs{f(x) - 6} < \frac{\abs{x + 3}}{6} \epsilon < \epsilon. \]
				\textit{However}, there is a subtle logical problem here: We have considered any arbitrary ${ \epsilon > 0 }$, meaning that $\epsilon$ could be large. Then, when we set ${ \delta = \frac{\epsilon}{6} }$ we linked the value of $\delta$ to that of $\epsilon$ so that $\delta$ may also be arbitrarily large. But ${ 0 < \abs{x - 2} < \delta }$ so that ${ \abs{x - 2} }$ may be arbitrarily large also. This contradicts the assumption that ${ x \approx 2 }$ and so the argument we have here is only valid for small $\epsilon$.\\\\
				So, we need an alternative approach. Consider that,
				\[ \abs{x + 3} = \abs{x - 2 + 2 + 3} \leq \abs{x - 2} + \abs{2 + 3} = \abs{x - 2} + 5 < \delta + 5. \]
				This means that,
				\[ \abs{(x^2 + x) - 6} < \delta \abs{x + 3} < \delta (\delta + 5). \]
				But note, do \textbf{not} start trying to solve a quadratic. There is a much better way.\\\\
				Now - here comes the clever bit - if we set ${ \delta = min\{1, \frac{\epsilon}{6}\} }$ then \textit{both} 1 and $\frac{\epsilon}{6}$ are an upper bound on the value of $\delta$ so that we can say,
				\[ \abs{(x^2 + x) - 6} < \delta (\delta + 5) \leq 6 \delta \leq 6 \frac{\epsilon}{6} = \epsilon. \]
			}
		\end{exe}
	
		\bigskip
		\subsubsection{Algebra of limits of functions}
		\labeledTheorem{Let ${ f, g : \R{} \mapsto \R{} }$ be two functions and $c$ be any real number. Suppose that ${ \lim_{x \to a} f(x) = L }$ and ${ \lim_{x \to a} g(x) = M }$. Then,
			\begin{enumerate}
				\item{${ \lim_{x \to a} (cf)(x) = cL }$}
				\item{${ \lim_{x \to a} (\abs{f})(x) = \abs{L} }$}
				\item{${ \lim_{x \to a} (f + g)(x) = L + M }$}
				\item{${ \lim_{x \to a} (f - g)(x) = L - M }$}
				\item{${ \lim_{x \to a} f(x)g(x) = LM }$}
				\item{${ \lim_{x \to a} (f / g)(x) = L/M }$ provided ${ g(x) \neq 0 }$ for any $x$ in the neighbourhood of $a$.}
			\end{enumerate}
		}{func_limit_properties}
		\begin{proof}
			see: \href{http://tutorial.math.lamar.edu/Classes/CalcI/LimitProofs.aspx}{Lamar University - Paul's Online Notes - Proofs of limit properties}
		\end{proof}
	
		\bigskip
		\subsubsection{One-sided limits}
		\boxeddefinition{Let ${ f : \R{} \to \R{} }$ be a function. We say that $L$ is the \textbf{limit of ${\bm{ f(x) }}$ as ${\bm{ x }}$ approaches ${\bm{ a }}$ from the left (or from below)}, denoted by ${ \lim_{x \to a^-} f(x) = L }$, if for each ${ \epsilon > 0 }$, there exists ${ \delta > 0 }$ such that,
				\[ a - \delta < x < a \implies \abs{f(x) - L} < \epsilon. \]
		}
		\subsubsection{Limits at infinity}
		\boxeddefinition{Let ${ f : \R{} \to \R{} }$ be a function. We say that $L$ is the \textbf{limit of ${\bm{ f(x) }}$ as ${\bm{ x }}$ approaches ${\bm{ \infty }}$}, denoted by ${ \lim_{x \to \infty} f(x) = L }$, if for each ${ \epsilon > 0 }$, there exists ${ M > 0 }$ such that,
			\[ x \geq M \implies \abs{f(x) - L} < \epsilon. \]
		}
	}

	\pagebreak
	\searchableSubsection{Continuity}{analysis}{\bigskip
		\boxeddefinition{
			A function $ f $ is \textbf{continuous at a point ${\bm{ a }}$} if
			\begin{itemize}
				\item{$f(a)$ is defined,}
				\item{${ \lim_{x \to a} f(x) = f(a) }$.}
			\end{itemize}
			\smallskip
			More formally, the definition of ${ \lim_{x \to a} f(x) = L }$ is,
			\[ \forall \epsilon > 0 \logicsep \exists \delta > 0 \suchthat 0 < \abs{x - a} < \delta \implies \abs{f(x) - L} < \epsilon. \]
			But if $f(a)$ is defined then,
			\[ 0 = \abs{x - a} \implies x = a \implies f(x) = f(a) \implies \abs{f(x) - f(a)} = 0. \]
			Therefore, if ${ \lim_{x \to a} f(x) = f(a) }$, then
			\[ \forall \epsilon > 0 \logicsep \exists \delta > 0 \suchthat \abs{x - a} < \delta \implies \abs{f(x) - f(a)} < \epsilon. \]		
		}
		
		\boxeddefinition{A function is \textbf{continuous} if it is \textbf{continuous at every point}.}
		\boxeddefinition{A function is \textbf{continuous on the closed interval ${\bm{ [a, b] }}$} if it is
			\begin{itemize}
				\item{continuous at every point in the open interval ${ (a, b) }$,}
				\item{${ \lim_{x \to a^+} f(x) = f(a) }$,}
				\item{${ \lim_{x \to b^-} f(x) = f(b) }$.}
			\end{itemize}
		}
	
		\labeledTheorem{Let ${ f,g : \R{} \to \R{} }$ be functions that are continuous at ${ a \in R }$ and $ c $ be
			any real number. Then ${ \abs{f}, (cf), (f - g), (f + g), (f(x)g(x)) }$ are all continuous at $ a $, and
			${ (f/g)  }$ is continuous provided ${ g(x) \neq 0 }$ for any $ x $ in some neighbourhood of $ a $.}{}
		\begin{proof}
			This follows from the algebra of limits of functions given in \autoref{theo:func_limit_properties}.
		\end{proof}
	
		\begin{corollary}
			It follows then that every polynomial is continuous. This can be seen as the most simple polynomial is a constant - which is clearly continuous; also ${ f(x) = x }$ is clearly continuous. Then powers of $x$ are continuous as they are products of continuous functions and when multiplied by coefficients this is a constant multiplying a continuous function so the resultant function is continuous. Then any polynomial is a summation of such terms so the result is continuous as the sum of continuous functions is continuous.
		\end{corollary}
	
		\labeledTheorem{If $ g $ is a function which is continuous at $ a $, and $ f $ is a function which is
			continuous at $ g(a) $. Then ${ (f \circ g)  }$ is continuous at $ a $.}{comps_continuous_functions_are_continuous}
		\begin{proof}
			Continuity of $f$ at $g(a)$ guarantees that for any ${ \epsilon > 0 }$ there exists some ${ \delta' > 0 }$ such that ${ \abs{x' - g(a)} < \delta' \implies \abs{f(x') - f(g(a))} < \epsilon }$ and continuity of $g$ at $a$ guarantees that for any ${ \delta' > 0 }$ there exists some ${ \delta > 0 }$ such that ${ \abs{x - a} < \delta \implies \abs{g(x) - g(a)} = \abs{x' - g(a)} < \delta' }$.
		\end{proof}
		\begin{corollary}
			${ \lim_{x \to a} (f \circ g)(x) = \lim_{x \to a} f(g(x)) = f(\lim_{x \to a} g(x)) }$
		\end{corollary}
	
		\subsubsection{Unifying continuity of functions with sequences}
		We now give an alternative definition of continuity which ties in the concept of limits for sequences.
		\labeledTheorem{A function $ f $ is continuous at $ a $ if and only if for each sequence $ (x_n) $ such that ${ \lim_{n \to \infty} x_n = a  }$ we have ${ \lim_{n \to \infty} f(x_n) = f(a) }$.}{continuity_of_functions_over_sequences}
		\note{
			Before the proof, an important point to note is that this theorem applies to "each sequence" with the described limit. This is important as it is possible to find an individual sequence such that the inference is not valid. For example, the constant sequence ${ \forall n \in \N{} \logicsep x_n = a }$ clearly tends to $a$ as ${ n \to \infty }$ but this would not imply continuity of $f$ as the limit of $f(x_n)$ for such a sequence would amount to saying that ${ f(a) = f(a) }$. The definition of continuity is assertion of the equality of the limit of $f$ over values in the neighbourhood of $a$ (but not at $a$ itself) with the value of $f$ at $a$. So, continuity is implied by the fact that the limit of $f(x_n)$ for the constant sequence $x_n = a$ is equal to the limit of $f(x_n)$ for all other sequences $x_n$ whose limit is $a$.\\
			Another way of looking at it is that $f$ is continuous at $a$ because the limit there equals $f(a)$ however the argument converges to $a$.
		}
		\begin{proof}
			Breaking it down into two propositions we have,
			\begin{equation}
				(\forall x_n \suchthat \lim_{n \to \infty} x_n = a) \; \lim_{n \to \infty} f(x_n) = f(a) \tag{$P_1$}
			\end{equation}
			\begin{equation}
				\forall \epsilon > 0 \logicsep \exists \delta > 0 \logicsep \abs{x - a} < \delta \implies \abs{f(x) - f(a)} < \epsilon \tag{$P_2$}
			\end{equation}
			and we need to show that ${ P_1 \iff P_2. }$\\\\
			So, to begin with we'll assume  show that ${ P_1 \implies P_2. }$\\
			Unpacking $P_1$ we have a function $f$ such that
			\[ \forall \epsilon > 0 \logicsep \exists N \logicsep \forall n > N \in \N{} \logicsep \abs{f(x_n) - f(a)} < \epsilon \tag{1} \] 
			where $x_n$ is a sequence such that
			\[ \forall \delta > 0 \logicsep \exists N' \logicsep \forall n > N' \in \N{} \logicsep \abs{x_n - a} < \delta. \tag{2} \]
			Then, if we choose a particular ${ \epsilon > 0 }$, there exists some $N$ such that for all ${ n > N }$ we have ${ \abs{f(x_n) - f(a)} < \epsilon. }$ Now there will also be some $N' \leq N$ so that ${ \forall n > N, \; n > N' }$ and so there is some $\delta$ such that ${ \abs{x_n - a} < \delta }$. Since $P_1$ says that this is the case \textit{whenever} we have an $x_n$ such as this, $P_1$ may be rewritten as
			\[ \forall \epsilon > 0 \logicsep (\exists N \logicsep \forall n > N \in \N{}) \logicsep \exists \delta > 0 \logicsep \abs{x_n - a} < \delta \implies \abs{f(x_n) - f(a)} < \epsilon. \]
			Now, if we remove reference to the number of the term of the sequences - ${ N,n }$ - we have
			\[ \forall \epsilon > 0 \logicsep \exists \delta > 0 \logicsep \abs{x - a} < \delta \implies \abs{f(x) - f(a)} < \epsilon \]
			which is the statement of continuity of $f$ in $P_2$.
			\\\\
			Next we prove ${ P_2 \implies P_1. }$\\
			So now we begin by assuming $P_2$ which was that,
			\[ \forall \epsilon > 0 \logicsep \exists \delta > 0 \logicsep \abs{x - a} < \delta \implies \abs{f(x) - f(a)} < \epsilon. \]
			Actually, it is quite easy to apply a similar logic as previously but in reverse to make the converse implication. We can choose any $\epsilon$ and there exists some $\delta$ such that $P_2$ holds. Then, as we have seen previously in (2), $P_1$ tells us that, for this value of $\delta$,
			\[ \exists N' \logicsep \forall n > N' \in \N{} \logicsep \abs{x_n - a} < \delta \]
			and $P_2$ tells us that,
			\[ \abs{x_n - a} < \delta \implies \abs{f(x_n) - f(a)} < \epsilon. \]
			Putting the two together we get,
			\[ \forall \epsilon > 0 \logicsep \exists \delta > 0 \logicsep \exists N' \logicsep \forall n > N' \in \N{} \logicsep \abs{x_n - a} < \delta \implies \abs{f(x_n) - f(a)} < \epsilon \]
			which implies that
			\[ \forall \epsilon > 0 \logicsep \exists N \logicsep \forall n > N \in \N{} \logicsep \abs{f(x_n) - f(a)} < \epsilon. \]
			So we have shown that $P_2$ implies that (2) implies (1) which is ${ P_2 \implies P_1 }$ as required.\\\\
			
			For completeness, let's consider another way of proving that ${ P_1 \implies P_2 }$ using a proof by contradiction.\\
			So, we are assuming $P_1$ but also assuming, for contradiction, that $P_2$ is false. Then we are negating the statement,
			\[ \forall \epsilon > 0 \logicsep \exists \delta > 0 \logicsep \abs{x - a} < \delta \implies \abs{f(x) - f(a)} < \epsilon \]
			so we are asserting that,
			\[ \forall \epsilon > 0 \logicsep \nexists \delta > 0 \logicsep \abs{x - a} < \delta \implies \abs{f(x) - f(a)} < \epsilon \]
			which is equivalent to
			\[ \forall \epsilon > 0 \logicsep \forall \delta > 0 \logicsep \abs{x - a} < \delta \centernot\implies \abs{f(x) - f(a)} < \epsilon \]
			or alternatively,
			\[ \forall \epsilon > 0 \logicsep \forall \delta > 0 \logicsep \exists x \logicsep (\abs{x - a} < \delta) \wedge (\abs{f(x) - f(a)} \geq \epsilon). \]
			So, this says that we can choose any arbitrary ${ \epsilon > 0 }$ and for all ${ \delta > 0 }$ there will be some $x$ in the $\delta$-neighbourhood of $a$ such that ${ \abs{f(x) - f(a)} \geq \epsilon. }$\\\\
			Now, if we choose a value of $\delta$ that depends on a natural number $n$ in such a way that ${ \delta \to 0 }$ as ${ n \to \infty }$ - for example, if ${ \delta = 1/n }$ - then the $\delta$-neighbourhoods around $a$ will get smaller as ${ n \to \infty. }$ Then we can select an $x$ from the $\delta$-neighbourhood that corresponds to a particular value of $n$ and call it $x_n$ and, in this way, we create a sequence $x_n$ that converges to $a$. So we have ${ \lim_{n \to \infty} x_n = a. }$\\
			But now, for every $\delta$ there is an $x_n$ in the $\delta$-neighbourhood of $a$ with ${ \abs{f(x_n) - f(a)} \geq \epsilon }$ so that
			\[ \lim_{n \to \infty} f(x_n) = f(a) \]
			is not true. Therefore this is a contradiction of the hypothesis $P_1$.
		\end{proof}
		\begin{corollary}
			\label{cor:lim_of_func_of_convergent_sequence_is_func_of_limit}
			${ \lim_{n \to \infty} f(x_n) = f(\lim_{n \to \infty} x_n) }$
		\end{corollary}
	
		\bigskip
		\subsubsection{Continuous Functions on Closed Intervals}
		\boxeddefinition{For a subset $X$ of the domain of a function $f$, we say that $f$ is \textit{bounded} on $X$ if there exists $M$ such that ${ \abs{f(x)} \leq M }$ for each ${ x \in X }$.}
		\boxeddefinition{We define the supremum (or maximum) of $f$ on $X$ as ${ sup\;\setc{f(x)}{x \in X} }$ (or ${ max\;\setc{f(x)}{x \in X} }$ if it exists).}
		
		\subsubsection{Examples of bounded and unbounded functions}
		\begin{exe}
			\ex{
				The indicator function for rational numbers within the reals, known as the \textbf{Dirichlet Function},
				\[ 
				f(x) =	\begin{cases} 
				0 & x \text{ is irrational} \\
				1 & x \text{ is rational}
				\end{cases}. 
				\]
				This function is nowhere continuous so that in every neighbourhood of a point $a$ you can always find values of $x$ closer in (i.e. in smaller neighbourhoods around $a$) such that ${ f(x) }$ is further away from the value of $f(a)$. So this function never converges anywhere but \textit{is} bounded because it only ever takes values of 1 or 0 so, clearly, its maximum is 1 and minimum is 0.
			}\label{ex:dirichlet_function}
			\ex{
				The function ${ f(x) = 1/x }$ over the interval ${ (0,1) }$ is continuous at every point but unbounded as it goes to infinity as ${ x \to 0 }$. If we consider the same function over the closed interval ${ [0,1] }$ then we no longer have continuity over this interval as there is a singularity at ${ x=0 }$.
			}\label{ex:reciprocal_function}
		\end{exe}
		
		\subsubsection{Extreme Value Theorem}
		\labeledTheorem{Let $f$ be continous on ${ [a,b] }$. Then $f$ is bounded on ${ [a,b] }$ and it achieves its maximum; that's to say, the supremum is equal to the maximum.}{extreme_value_theorem}
		\note{
			Note that, even if $f$ is defined at every point in ${ [a,b] }$, if it is not continuous then it \textit{may} not be bounded. There \textit{do} exist functions that are not continuous but bounded (for example the Dirichlet Function \ref{ex:dirichlet_function}) but there also exist functions that are not continuous and unbounded such as the reciprocal function \ref{ex:reciprocal_function}. So, functions that are not continuous on a closed interval may be bounded or not; and functions that are continuous on an open interval also may or may not be bounded (again, the reciprocal function is an example of a function that is continuous on an open interval but not bounded); but here we will show that functions that are continuous on a closed interval must be bounded on that interval.
		}
		\begin{proof}
			That $f$ is continuous on ${ [a,b] }$ means that, for any ${ c \in (a,b) }$,
			\[ \forall \epsilon > 0 \logicsep \exists \delta > 0 \logicsep \abs{x - c} < \delta \implies \abs{f(x) - f(c)} < \epsilon. \]
			This means that the value of $f(x)$ must be finite everywhere in ${ (a,b) }$ as, choosing any fixed point $c$ in the open interval, ${ \abs{x - c} }$ is finite and, therefore, less than some $\delta$ thus implying that ${ \abs{f(x) - f(c)} < \epsilon }$ for some finite $\epsilon$. So, continuity on the open interval implies that, if there is any point $c$ such that $f(c)$ is finite, then all $f(x)$ in the open interval must also be finite.\\
			However, it may still be the case that the function becomes infinite at one or both of the endpoints of the interval (as, in fact, we saw with the reciprocal function \ref{ex:reciprocal_function}). But the other part of the definition of continuity of $f$ on ${ [a,b] }$ is that
			\[ \lim_{x \to a^+} f(x) = f(a) \text{ and } \lim_{x \to b^-} f(x) = f(b) \]
			which, along with the requirement for continuity that $f$ be defined on all points in the closed interval, ensures that the function does not become infinite at the bounds of the interval either.\\
			Therefore, since at no point in the closed interval ${ [a,b] }$ does $f$ become infinite, we can say that
			\[ \exists M \logicsep \forall x \in [a,b] \logicsep \abs{f(x)} \leq M. \]					
		\end{proof}
	
		\bigskip
		\subsubsection{Intermediate Value Theorem}
		\labeledTheorem{Let $f$ be continuous on ${ [a,b] }$ and \WLOG assume ${ f(a) < f(b) }$. Then, for all ${ K \suchthat f(a) < K < f(b) }$, there exists some ${ c \in (a,b) }$ with ${ f(c) = K }$.
		}{intermediate_value_theorem}
		\note{Note that this theorem is \textbf{not} written for an interval such that ${ f(a) \leq f(b) }$ because if ${ f(a) = f(b) }$ then the only ${ K \suchthat f(a) \leq K \leq f(b) }$ is ${ f(a) = K = f(b) }$. But now it is \textbf{not} true to say that there exists some ${ c \in (a,b) }$ with ${ f(c) = K }$ as there is no reason why the value of $f(x)$ at the bounds of the interval should be repeated somewhere in the interior of the interval.}
		\begin{proof}
			Continuity at the interval bounds $a$ and $b$ means that,
			\[ 0 < x - a < \delta_1 \implies \abs{f(x) - f(a)} < \epsilon_1, \]
			\[ 0 < b - x < \delta_2 \implies \abs{f(x) - f(b)} < \epsilon_2. \]
%			So $f(x)$ is bound in neighbourhoods of $f(a)$ and $f(b)$ as follows,
%
%			Then, if we make the neighbourhoods large enough so that they overlap, the lower neighbourhood (the one centred around the lower value $f(a)$) will have its upper bound inside the upper neighbourhood while the lower bound of the upper neighbourhood will be inside the lower neighbourhood. 
			If we choose as the value of both $\delta_1$ and $\delta_2$ a value greater than ${ (b - a)/2 }$ then the intervals of $x$-values will overlap. If we consider the $x$-values in the intersection of the intervals, for these values of $x$, we have,
			\[ f(a) - \epsilon_1 < f(x) < f(a) + \epsilon_1, \]
			\[ f(b) - \epsilon_2 < f(x) < f(b) + \epsilon_2 \]
			and, for the overlapping section,
			\[ f(b) - \epsilon_2 < f(x) < f(a) + \epsilon_1. \]
			Then, if we make $\epsilon_1$ a function of $\epsilon_2$ as follows,
			\[ \epsilon_2 = \epsilon_1 + f(a) + f(b) \]
			then we have,
			\[ -(f(a) + \epsilon_1) < f(x) < (f(a) + \epsilon_1) \]
			
			So, we have a lower neighbourhood around $f(a)$ and an upper neighbourhood around $f(b)$ as follows,
			\[ f(a) - \epsilon_1 < f(x) < f(a) + \epsilon_1, \]
			\[ f(b) - \epsilon_2 < f(x) < f(b) + \epsilon_2 \]
			If ${ \epsilon_1 > \abs{f(a)} }$ and ${ \epsilon_2 > \abs{f(b)} }$ then both neighbourhoods contain ${ f(x) = 0 }$. In this case, there is some interval of $x$ such that ${ f(x) }$ lies inside both the lower and upper neighbourhood -- in the overlap of the two. In the overlap we have,
			\[ f(b) - \epsilon_2 < f(x) < f(a) + \epsilon_1. \]
			Now if we let $\epsilon_1$ vary freely but make $\epsilon_2$ a function of $\epsilon_1$ like so,
			\[ \epsilon_2 = f(a) + f(b) + \epsilon_1 \]
			then we still have ${ \epsilon_1 > \abs{f(a)} }$ and ${ \epsilon_2 > \abs{f(b)} }$ as,
			\[ \epsilon_1 > \abs{f(a)} \iff -\epsilon_1 < f(a) < \epsilon_1 \iff 0 < f(a) + \epsilon_1 < 2\epsilon_1 \]
			so ${ \epsilon_2 > f(b) }$ and, conversely, if we assume that ${ \epsilon_2 > \abs{f(b)} }$ then,
			\[ \epsilon_2 > \abs{f(b)} \iff -\epsilon_2 < f(b) < \epsilon_2 \iff -\epsilon_2 - f(b) < 0 < \epsilon_2 - f(b) \]
			\[ \epsilon_1 = \epsilon_2 - f(a) - f(b) > -f(a) = \abs{f(a)} \hspace{20pt}\sidecomment{since ${ f(a) < 0 }$}. \]
			Therefore,
			\[ \epsilon_2 = f(a) + f(b) + \epsilon_1 \implies [ \epsilon_1 > \abs{f(a)} \iff \epsilon_2 > \abs{f(b)} ]. \]
			Now we have,
			\[ f(b) - \epsilon_2 = -f(a) - \epsilon_1 < f(x) < f(a) + \epsilon_1 \]
			which is equivalent to
			\[ \abs{f(x)} < f(a) + \epsilon_1 = \epsilon_3. \]
			Now, since ${ f(a) + \epsilon_1 > 0 }$ we also have ${ \epsilon_3 > 0 }$ and it can become arbitrarily small as long as it is greater than $\abs{f(a)}$. So we have shown that continuity over the closed interval and ${ f(a) < f(b) }$ imply that we can find subintervals of ${ [a,b] }$ such that ${ f(x) }$ is constricted to ever-decreasing neighbourhoods of 0. In other words, for some ${c \suchthat a < c < b, \; f(x) \to 0 }$ as  ${ x \to c }$ and since $f$ is continuous on the interval this implies that ${ f(c) = 0 }$ also. 
		\end{proof}
		
	}
  
\end{document}