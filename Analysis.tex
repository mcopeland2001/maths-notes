\documentclass[MathsNotesBase.tex]{subfiles}




\date{\vspace{-6ex}}


\begin{document}
\searchableSubsection{\chapterTitle{Analysis}}{analysis}{\bigskip\bigskip}

	\searchableSubsection{\sectionTitle{Supremum and Infimum}}{analysis}{\bigskip}
	
	\bigskip\bigskip
	\searchableSubsection{Definitions}{analysis}{\bigskip
		\boxeddefinition{An upper bound on a set A is a value $x$ such that,
			\[ \forall a \in A, \, a \leq x \]
			and a lower bound is similarly defined as a value $y$ such that,
			\[ \forall a \in A, \, a \geq y. \]
			A set is said to be \textbf{upper-bounded} if there exists some upper-bound on the set and is said to be \textbf{lower-bounded} if there exists some lower bound on the set. If there exists both upper and lower bounds then the set is said to be \textbf{bounded}.
		}
		\boxeddefinition{The \textbf{supremum} of a upper-bounded set $A$ is a value $\sigma_A$ such that $\sigma_A$ is an upper bound on $A$ and, 
			\[ \sigma_A' < \sigma_A \iff \exists a \in A \suchthat a > \sigma_A' \]
			which is to say that if $\sigma_A' < \sigma_A$ then $\sigma_A'$ is not an upper bound on $A$ and, if $\sigma_A'$ is not an upper bound on $A$ then it must be less than $\sigma_A$ since $\sigma_A$ is an upper bound on A.\\
			An alternative, equivalent definition is,
			\[ \forall \epsilon > 0, \, \exists a \in A \suchthat a > \sigma_A - \epsilon. \]
		}
		\paragraph{Issue} Note that there is an apparent paradox here: This second definition implies that
		\begin{align*}
			&&  \forall \epsilon > 0 \logicsep \exists a \in A &\suchthat a + \epsilon > \sigma_A  \\
			&\iff &\exists a \in A &\suchthat a \geq \sigma_A  &\sidecomment{}
		\end{align*}
		which result, when combined with the upper-bound property, gives
		\begin{align*}
			&& \exists a \in A &\suchthat (a \geq \sigma_A) \wedge (a \leq \sigma_A) \\
			&\iff &\exists a \in A &\suchthat a = \sigma_A  &\sidecomment{}
		\end{align*}
		which says that there is always an element in the bounded set that is equal to the supremum. This is not correct - the supremum may be in the set or external to it.\\
		The initial implication is not true, however. We cannot infer that ${ \exists a \in A \suchthat a \geq \sigma_A }$. This can be seen with another rearrangement,
		\begin{align*}
			&&  \forall \epsilon > 0 \logicsep \exists a \in A &\suchthat a + \epsilon > \sigma_A  \\
			&\iff &\forall \epsilon > 0 \logicsep \exists a \in A &\suchthat \epsilon > \sigma_A - a  &\sidecomment{}
		\end{align*}
		which shows us that for any positive epsilon there needs to be an $a$ close enough to the value of $\sigma_A$ that the difference in their values is less than epsilon. Since $a$ can approach arbitrarily close to $\sigma_A$ this is achievable for any positive epsilon. This property seems to be equivalent to the fact that $\sigma_A$ is a \textit{limit point} of $A$ but that will be covered properly in Topology.
		\bigskip\bigskip
		
		\boxeddefinition{The \textbf{infimum} of a lower-bounded set $A$ is defined similarly to the supremum: as a value $\tau_A$ such that $\tau_A$ is a lower bound on $A$ and, 
			\[ \tau_A' > \tau_A \iff \exists a \in A \suchthat a < \tau_A' \]
			or alternatively,
			\[ \forall \epsilon > 0, \, \exists a \in A \suchthat a < \tau_A + \epsilon. \]
		}
		\notation{The supremum of $A$ is denoted $sup \, A$ and the infimum is denoted $inf \, A$.}
	}
	\searchableSubsection{Deductions using the supremum and infimum}{analysis}{\bigskip
		\labeledProposition{If a bounded set $A \subset \R{}$ has the property that,
			\[ \forall x,y \in A \logicsep \abs{x - y} < 1 \]
			then it follows that,
			\[ (sup \, A - inf \, A) \leq 1. \]
		}{sup_minus_inf_max_difference}
		\begin{proof}
			Let $ \sigma_A = sup \, A $ and $ \tau_A = inf \, A $ and \WLOG assume that $x > y$. By the definitions of the supremum and infimum we have,
			\begin{align*}
			&& \forall \epsilon > 0 \logicsep \exists x,y \in A &\logicsep (x > \sigma_A - \epsilon) \wedge (y < \tau_A + \epsilon)  \\
			&\iff & \forall \epsilon > 0 \logicsep \exists x,y \in A &\logicsep (x > \sigma_A - \epsilon) \wedge (-y > -\tau_A - \epsilon)  &\sidecomment{}\\
			&\iff & \forall \epsilon > 0 \logicsep \exists x,y \in A &\logicsep (x - y) > (\sigma_A - \tau_A) - 2\epsilon  &\sidecomment{}\\
			\end{align*}
			Now suppose, for contradiction, that $ (\sigma_A - \tau_A) > 1 $ then we can say that,
			\[ \exists r > 0 \logicsep (\sigma_A - \tau_A) = 1 + r. \] 
			If we then constrict $\epsilon$ such that,
			\[ \epsilon < \frac{r}{2} \iff 2\epsilon < r \iff r - 2\epsilon > 0 \]
			then the previous result tells us that, for $ 0 < \epsilon < \frac{r}{2} $,
			\begin{align*}
			&& \exists x,y \in A &\logicsep (x - y) > (\sigma_A - \tau_A) - 2\epsilon  \\
			&\iff & \exists x,y \in A &\logicsep (x - y) > 1 + r - 2\epsilon > 1  &\sidecomment{}
			\end{align*}
			which contradicts the set property that $ \forall x,y \in A, \, \abs{x - y} < 1 $. So this shows that $ (\sigma_A - \tau_A) \leq 1 $.
		\end{proof}
		\bigskip
		\labeledProposition{Let $A \subset \R{}$ be a bounded set and let $B$ be the set defined by
			\[ B = \setc{b}{b = f(a), \; a \in A} \]
			where the function $f$ is some strictly monotonic function.
			Then it follows that,
			\[ sup \, B = f(sup \, A). \]
		}{monotonic_functions_preserve_supremum}
		\begin{proof}
			A is bounded and so $\sigma_A = sup \, A$ exists. So, using the supremum properties we have,
			\begin{align*}
			&& \forall a \in A \logicsep a &\leq \sigma_A  \\
			&\iff & \forall a \in A \logicsep f(a)  &\leq f(\sigma_A)  &\sidecomment{by monotonicity of f}\\
			&\iff & \forall b \in B \logicsep b  &\leq f(\sigma_A) 
			\end{align*}
			which is to say that $\sigma_B = f(\sigma_A)$ is an upper bound on $B$.\\
			Furthermore, using the other supremum property, we have that,
			\begin{align*}
			&& \sigma_A' < \sigma_A &\implies \exists a \in A \suchthat a > \sigma_A' \\
			&\iff & f(\sigma_A') < f(\sigma_A)  &\implies \exists a \in A \suchthat f(a) > f(\sigma_A')  &\sidecomment{by strict monotonicity of f}\\
			&\iff & \sigma_B' < \sigma_B  &\implies \exists b \in B \suchthat b > \sigma_B'.	
			\end{align*}
			Therefore $\sigma_B$ satisfies both requirements of the supremum and we have shown that,
			\[ sup \, B = f(sup \, A). \]
		\end{proof}
	}
	
	
	\pagebreak
	\searchableSubsection{\sectionTitle{Limits}}{analysis}{\bigskip}
	
	\bigskip\bigskip
	\searchableSubsection{Limits of sequences}{analysis}{\bigskip
		\subsubsection{Problems with the informal description of a limit}
		If we say that a sequence tends to some value $L$ when the terms of the sequence \textit{gets closer and closer to} $L$ we have the following problems:
		\begin{itemize}
			\item{that the sequence gets closer and closer to many numbers so that this does not specify a single specific limit.}
			\item{that the sequence can have a limit but it's not the case that every term is closer than the previous term to the limit. For example,
				\[ a_{2k} = 1/k, \; a_{2k - 1} = \frac{1}{k+1} \]
				tends to 0 but $ a_{2k} > a_{2k - 1} $.
			}
		\end{itemize}
		\bigskip
		\boxeddefinition{A sequence $a_n$ is said to \textbf{tend} to $L$ or have the \textbf{limit} $L$ iff,
			\[ \forall \epsilon > 0 \in \R{} ,\, \exists N \in \N{} \suchthat \forall n > N ,\, \abs{a_{n} - L} < \epsilon. \]
		}
		\boxeddefinition{The interval $ (L - \epsilon, L + \epsilon) $ is called the \textbf{\mbox{$\epsilon$-neighbourhood of $L$}}.}
		
		\boxeddefinition{A sequence $a_n$ is said to \textbf{tend to infinity} iff,
			\[ \forall M > 0 \in \R{} ,\, \exists N \in \N{} \suchthat \forall n > N ,\, a_{n} > M \]
			and \textbf{tend to minus-infinity} iff,
			\[ \forall M < 0 \in \R{} ,\, \exists N \in \N{} \suchthat \forall n > N ,\, a_{n} < M. \]
		}
	
		\boxeddefinition{A sequence that has a limit is called \textbf{convergent} and otherwise is called \textbf{divergent}. Note that \textbf{divergent} sequences include both sequences that remain bounded but oscillate without converging and those that tend to infinity (or minus-infinity).}
		
		\subsubsection{Examples of Convergence and Divergence}\bigskip
			\begin{exe}
				\ex {\textbf{Non-convergent Oscillation}\\\\
				The sequence $ a_n = (-1)^n $ is divergent despite always remaining bounded within the interval $ [-1, 1] $ as it neither converges to 1 or to -1.}\label{ex:flipping_sign}
				\ex {\textbf{Limit of an Infinite Recurrence}\\\\
					Take the sequence given by,
					\[ a_1 = 1, \; a_{n+1} = \frac{a_n}{2} + \frac{3}{2a_n} \;\; (n \ge 1). \]
					Assume there is an equilibrium value, $a^*$, then
					\begin{align*}
						&&a^* &= \frac{a^*}{2} + \frac{3}{2a^*}  \\
						&\iff &2(a^*)^2  &= (a^*)^2 + 3  &\sidecomment{}\\
						&\iff &(a^*)^2  &= 3  &\sidecomment{}\\
						&\iff &a^*  &= \sqrt{3}  &\sidecomment{$\forall n, a_n \geq 0$}\\
					\end{align*}
					So $\sqrt{3}$ is the steady-state value that this recurrence converges to as $n \to \infty$. If the recurrence didn't converge then the assumption of an equilibrium value would result in a contradiction. Note, however, that the fact that there is an equilibrium value does \textit{not}, by itself, prove that this sequence converges (although this sequence does).
				}\label{ex:infinite_recurrence}
			\end{exe}
			
		\bigskip	
		\labeledProposition{A sequence has at most one limit. In other words, a sequence can only converge, if at all, to a single unique value.}{uniqueness_of_limits}
		\begin{proof}
			Let $L$ and $L'$ both be limits of the sequence $a_n$, and the constant $\alpha = L - L'$. Then,
			\[ \forall \epsilon > 0 \in \R{} ,\, \exists N \in \N{} \suchthat \forall n > N ,\, \abs{a_{n} - L} < \epsilon \] and
			\[ \forall \epsilon' > 0 \in \R{} ,\, \exists N' \in \N{} \suchthat \forall n' > N' ,\, \abs{a_{n'} - L'} < \epsilon'. \]
			But also we have,
			\[ \abs{a_n - L'} = \abs{(a_n - L) + (L - L')} = \abs{(L - L') + (a_n - L)} \]
			and using the triangle inequality,
			\begin{align*}
			&& \abs{L - L'} = \abs{(L - L') + (a_n - L) - (a_n - L)} &\leq \abs{(L - L') + (a_n - L)} + \abs{-(a_n - L)} \\
			&\iff & \abs{L - L'}  &\leq \abs{(L - L') + (a_n - L)} + \abs{a_n - L}\\
			&\iff & \abs{L - L'} - \abs{a_n - L}  &\leq \abs{(L - L') + (a_n - L)}\\
			&\iff & \abs{\alpha} - \abs{a_n - L}  &\leq \abs{\alpha + (a_n - L)}\\
			\end{align*}
			Since $\alpha = L - L'$ is constant we can consider the situation when $\epsilon = \frac{\abs{\alpha}}{2}$ then we have that,
			\begin{align*}
			&& \exists N \in \N{} \suchthat \forall n > N ,\, \abs{a_{n} - L} &< \epsilon = \frac{\abs{\alpha}}{2} \\
			&\iff &  -\abs{a_{n} - L} &> -\frac{\abs{\alpha}}{2}  &\sidecomment{}\\
			&\iff & \abs{\alpha} - \abs{a_{n} - L} &> \abs{\alpha} - \frac{\abs{\alpha}}{2}  &\sidecomment{}\\
			&\iff & \abs{\alpha} - \abs{a_{n} - L} &> \frac{\abs{\alpha}}{2}.  &\sidecomment{}\\
			\end{align*}
			Combining this with the previous result gives, for $\forall n > N$,
			\[ \frac{\abs{\alpha}}{2} < \abs{\alpha} - \abs{(a_n - L)}  \leq \abs{\alpha + (a_n - L)} = \abs{a_n - L'} \]
			which, by rearranging a little, is,
			\[ \forall n > N,\, \abs{a_n - L'} > \frac{\abs{\alpha}}{2}. \]
			But this means that if we also choose $\epsilon' = \frac{\abs{\alpha}}{2}$ then there is no $N'$ such that $\forall n' > N' ,\, \abs{a_{n'} - L'} < \epsilon'$ which contradicts the hypothesis that $L'$ is also a limit of $a_n$.
		\end{proof}
		\begin{proof}
			Another quicker way of proving the proposition is by letting $ \epsilon = \epsilon' = \frac{\abs{\alpha}}{2} $ so that,
			\[ 2\epsilon = \abs{\alpha} = \abs{L - L'} = \abs{L - a_n + a_n - L'} \leq \abs{L - a_n} + \abs{a_n - L'} = \abs{a_n - L} + \abs{a_n - L'} \]
			which gives us,
			\[ 2\epsilon \leq \abs{a_n - L} + \abs{a_n - L'}. \]
			But by the limit definition,
			\[ \abs{a_n - L} + \abs{a_n - L'} < \epsilon + \epsilon' \]
			and since we have set $ \epsilon = \epsilon' $ then,
			\[ \abs{a_n - L} + \abs{a_n - L'} < 2\epsilon \]
			which contradicts $ 2\epsilon \leq \abs{a_n - L} + \abs{a_n - L'} $.
		\end{proof}
	
		\bigskip\bigskip
		\boxeddefinition{If $a_n$ is a sequence and $ S = \setc{a_n}{n \in \N{}} $ then $a_n$ is said to be \textbf{bounded below} if $S$ has a lower bound and \textbf{bounded above} if $S$ has an upper bound, and \textbf{bounded} if it is bounded above and below.}
		
		\bigskip
		\begin{lemma}
			Any finite set of elements from an ordered field has a minimum and a maximum.
		\end{lemma}
		\begin{proof}
			This can be proven quite easily using induction. Taking the base case of a set of cardinality one, clearly there is a maximum and a minimum both of which are the sole element of the set. Then, the induction step is to say, given a set $S$ that has a maximum, $s_{max}$, and a minimum, $s_{min}$, if we add a new element $e$, then if $e$ is greater than $s_{max}$ it is the maximum of the new set and if it is less than $s_{min}$ it is the minimum of the new set. Otherwise, the previous maximum and minimum also pertain to the new set. Therefore, adding a new element to a set that has a maximum and a minimum creates a new set with a maximum and a minimum.
		\end{proof}
		\bigskip
		\labeledProposition{Any convergent sequence is bounded.}{convergent_seqs_bounded}
		\begin{proof}
			Firstly, we need to prove that any finite sequence is bounded. We can do this simply by observing that any finite set of elements from an ordered field,
			\[ S = \{a_1, a_2, \dots , a_n\} \]
			has a minimum and a maximum.\\
			Now, let $a_n$ be an arbitrary convergent sequence so that,
			\[ \forall \epsilon > 0 \logicsep \exists N \in \N{} \logicsep \forall n > N \logicsep \abs{a_n - L} < \epsilon \]
			for some $L \in \R{}$.\\\\
			Then, let $S_{max}$ and $S_{min}$ be the maximum and minimum respectively of the first $N$ terms of $a_n$, $S = \{a_1, a_2, \dots , a_N\}$, and,
			\[ \exists \epsilon > 0 \logicsep \forall n > N \logicsep \abs{a_n - L} < \epsilon \]
			so that, for $n > N$, the sequence $a_n$ is bounded in the $\epsilon$-neighbourhood of $L$.\\
			So, if we define ${ m = min\{S_{min}, L - \epsilon\} }$ and ${ M = max\{S_{max}, L + \epsilon\} }$, then the whole sequence $a_n$ for all $n \in \N{}$ is bounded below by $m$ and bounded above by $M$.\\
			Therefore $a_n$ is bounded. 
		\end{proof}
	
		\bigskip
		\boxeddefinition{An \textbf{increasing} sequence is a sequence $a_n$ such that,
			\[ \forall n \in \N{} \logicsep a_{n+1} \geq a_n \]
			and \textbf{decreasing} if,
			\[ \forall n \in \N{} \logicsep a_{n+1} \leq a_n \]
			and \textbf{monotonic} if either increasing or decreasing.
		}
		
		\bigskip
		\labeledProposition{Any increasing sequence that is bounded above has a limit.}{increasing_bounded_seqs_have_limit}
		\begin{proof}
			Let $a_n$ be an increasing sequence that is bounded above. Then,
			\[ \forall n \in \N{} \logicsep a_{n+1} \geq a_n \]
			and let ${ S = \setc{a_n}{n \in \N{}} }$. Since $a_n$ is bounded above it has a supremum. Let $ \sigma = sup \; S $ so that,
			\[ \forall a_n \in S \logicsep a_n \leq \sigma \eqand \forall \epsilon > 0 \logicsep \exists a_n \in S \logicsep a_n > \sigma - \epsilon. \]
			Therefore, for some arbitrary fixed $\epsilon > 0$,
			\[ \exists a_n \in S \logicsep a_n > \sigma - \epsilon \] 
			and setting $ N = n $ so that $ a_N > \sigma - \epsilon $, we have,
			\[ \forall n > N \in \N{} \logicsep a_n \geq a_N > \sigma - \epsilon \]
			and so, recalling that $\sigma$ is an upper bound on $S$,
			\begin{align*}
			&& \exists N \logicsep \forall n > N \in \N{} &\logicsep (a_n > \sigma - \epsilon) \wedge (a_n \leq \sigma) \\
			&\iff &\exists N \logicsep \forall n > N \in \N{} &\logicsep a_n \leq \sigma < a_n + \epsilon  &\sidecomment{}\\
			&\iff &\exists N \logicsep \forall n > N \in \N{} &\logicsep 0 \leq \sigma - a_n < \epsilon  &\sidecomment{}\\
			&\implies &\exists N \logicsep \forall n > N \in \N{} &\logicsep \abs{\sigma - a_n} < \epsilon.
			\end{align*}
			But $\epsilon$ was an arbitrary positive value so,
			\[ \forall \epsilon > 0 \logicsep \exists N \logicsep \forall n > N \in \N{} \logicsep \abs{\sigma - a_n} < \epsilon \]
			and $\sigma$ is, therefore, the limit of $a_n.$
		\end{proof}
		
		\bigskip
		\begin{corollary}
			Any increasing sequence that is bounded above converges to the supremum of its elements (terms, values, etc.).
		\end{corollary}
		\begin{corollary}
			A decreasing sequence that is bounded below converges to the infimum of its elements.
		\end{corollary}
	}

	\bigskip\bigskip
	\searchableSubsection{Algebra of limits of sequences}{analysis}{\bigskip
		\labeledProposition{
			Let $a_n$ and $b_n$ be convergent sequences with limits $ a $ and $ b $,
			respectively. Let $ C $ be a real number and let $ k $ be a positive integer. Then as $n \to \infty$,
			\begin{enumerate}[label=\alph*)]
				\item{$ Ca_n \to Ca $}
				\item{$ \abs{a_n} \to \abs{a} $}
				\item{$ a_n + b_n \to a + b $}
				\item{$ a_nb_n \to ab $}
				\item{$ a_n^k \to a^k $}
				\item{if, for all $n,\, b_n \neq 0$ and $b \neq 0$, then $\frac{1}{b_n} \to \frac{1}{b}$}.
			\end{enumerate}
			\smallskip
		}{seq_limit_properties}	
		\begin{proof}
			We prove each property individually in the given order.
			
			\subsubsection{Proof of (a) $ Ca_n \to Ca $} 
			If $ C = 0 $ then $ Ca_n = 0 = Ca $ for all $n$ and the proposition holds trivially. If $ C \neq 0 $ then, since $a_n \to a$,
			\[ \forall \epsilon' > 0 \logicsep \exists N \logicsep \forall n > N \in \N{} \logicsep \abs{a_n - a} < \epsilon'. \]
			Now let $ \epsilon = \abs{C}\epsilon' $. Then,
			\begin{align*}
			&& \forall \epsilon > 0 \logicsep \exists N \logicsep \forall n > N \in \N{} &\logicsep \abs{C}\abs{a_n - a} < \abs{C}\epsilon' = \epsilon  \\
			&\iff & \forall \epsilon > 0 \logicsep \exists N \logicsep \forall n > N \in \N{} &\logicsep \abs{Ca_n - Ca} < \epsilon. &\sidecomment{$\abs{x}\abs{y} = \abs{xy}$} \\
			\end{align*}
			
			\subsubsection{Proof of (b) $ \abs{a_n} \to \abs{a} $} 
			\begin{align*}
			&& \abs{a_n} = \abs{a_n - a + a} &\leq \abs{a_n - a} + \abs{a}  &\sidecomment{the "triangle inequality"}\\
			&\iff & \abs{a_n} - \abs{a} &\leq \abs{a_n - a}. &\sidecomment{(1)} \\
			\end{align*}
			\begin{align*}
			&& \abs{a} = \abs{a - a_n + a_n} &\leq \abs{a - a_n} + \abs{a_n}  &\sidecomment{the "triangle inequality"}\\
			&\iff & \abs{a} - \abs{a_n} &\leq \abs{a - a_n} = \abs{a_n - a} &\sidecomment{} \\
			&\iff & \abs{a_n} - \abs{a} &\geq -\abs{a_n - a}. &\sidecomment{(2)} \\
			\end{align*}
			Putting (1) and (2) together we have,
			\begin{align*}
			&& -\abs{a_n - a} &\leq \abs{a_n} - \abs{a} \leq \abs{a_n - a} &\sidecomment{the "triangle inequality"}\\
			&\iff & \abs{\abs{a_n} - \abs{a}} &\leq \abs{a_n - a}. \\
			\end{align*}
			The fact that $a_n$ converges to $a$ implies that $ \abs{a_n - a} $ converges to zero. Since it is an upper bound on the value of ${ \abs{\abs{a_n} - \abs{a}} }$, the value ${ \abs{\abs{a_n} - \abs{a}} }$ must also converge to zero. Specifically any value of $ n, \epsilon $ such that ${ \abs{a_n - a} < \epsilon }$ will also satisfy ${ \abs{\abs{a_n} - \abs{a}} \leq \abs{a_n - a} < \epsilon }$.
			
			\subsubsection{Proof of (c) $ a_n + b_n \to a + b $}
			Using, again, the "triangle inequality",
			\[ \abs{(a_n + b_n) - (a + b)} = \abs{(a_n - a) + (b_n - b)} \leq \abs{a_n - a} + \abs{b_n - b}. \]
			So, ${ \abs{a_n - a} + \abs{b_n - b} }$ is an upper bound on the value of ${ \abs{(a_n + b_n) - (a + b)} }$. If we take any arbitrary ${ \epsilon > 0 }$ then,
			\[ \exists N_1 \logicsep \forall n > N_1 \in \N{} \logicsep \abs{a_n - a} < \frac{\epsilon}{2} \]
			and
			\[ \exists N_2 \logicsep \forall n > N_2 \in \N{} \logicsep \abs{b_n - b} < \frac{\epsilon}{2}. \]
			Then, if we take $ N = max\{N_1, N_2\} $, we have,
			\[ \forall n > N \in \N{} \logicsep \abs{(a_n + b_n) - (a + b)} \leq \abs{a_n - a} + \abs{b_n - b} < \epsilon. \]
			
			\subsubsection{Proof of (d) $ a_nb_n \to ab $}
			\begin{align*}
			&& \abs{a_nb_n - ab} = \abs{a_nb_n - ab_n + ab_n - ab} &\leq \abs{b_n(a_n - a)} + \abs{a(b_n - b)}  &\sidecomment{the "triangle inequality"}\\
			&\iff & \abs{a_nb_n - ab} &\leq \abs{b_n}\abs{a_n - a} + \abs{a}\abs{b_n - b}  &\sidecomment{} \\
			\end{align*}
			Since $b_n$ converges, by \autoref{prop:convergent_seqs_bounded}, it is bounded. Therefore, $\abs{b_n}$ has some upper bound which we shall call $B$. Then,
			\[ \forall \epsilon > 0 \logicsep \exists N_1 \logicsep \forall n > N_1 \in \N{} \logicsep \abs{a_n - a} < \frac{\epsilon}{2B} \]
			and
			\[ \forall \epsilon > 0 \logicsep \exists N_2 \logicsep \forall n > N_2 \in \N{} \logicsep \abs{b_n - b} < \frac{\epsilon}{2\abs{a}}. \]
			Now, let ${ N = max{N_1, N_2}. }$ Then,
			\[ \forall n > N \in \N{} \logicsep B\abs{a_n - a} + \abs{a}\abs{b_n - b} < \epsilon. \]
			
			\subsubsection{Proof of (e) $ a_n^k \to a^k $}
			Using (d) - and because $k$ is a positive integer - we can do induction on the power $k$.\\
			Base cases 0 and 1 are clearly true as ${ k = 0 }$ results in $a_n$ being the constant 1 for all $n$ and so trivially converges to ${ a = 1 }$; and ${ k = 1 }$ results in the same sequence as $a_n$.\\
			So, we perform the induction step for ${ k \geq 2 }$. Then, by the induction hypothesis, ${ a_n^{k-1} \to a^{k-1} }$. But ${ a_n^k = a_n^{k-1}a_n }$ and, by (d) and the induction hypothesis, we have that ${ a_n^{k-1}a_n \to a^{k-1}a = a^k }$. Therefore, ${ a_n^k \to a^k. }$
			
			\subsubsection{Proof of (f) $ \forall n \logicsep b_n, b \neq 0 \implies \frac{1}{b_n} \to \frac{1}{b} $}
			Again invoking \autoref{prop:convergent_seqs_bounded} and letting the upper bound on the sequence $b_n$ be $B$,
			\[ \abs{\frac{1}{b_n} - \frac{1}{b}} = \abs{\frac{b - b_n}{b_nb}} = \abs{\frac{b_n - b}{b_nb}} = \frac{ \abs{b_n - b} }{ \abs{b_n}\abs{b} } \leq \frac{1}{B\abs{b}}\abs{b_n - b}. \]
			Now, since $\frac{1}{B\abs{b}}$ is a constant we can define the constant ${ C = \frac{1}{B\abs{b}} }$ and then we see that in (a) we have already proven that ${ C\abs{b_n - b} }$ converges to 0. In (a) we used that to prove that ${ Cb_n \to Cb }$ but here it proves that ${ \frac{1}{b_n} \to \frac{1}{b}. }$
		\end{proof}
	}

	\bigskip\bigskip
	\searchableSubsection{Some theorems on limits of sequences}{analysis}{\bigskip 
		\labeledTheorem{If ${ \abs{a} < 1 }$ then ${ \lim_{n \to \infty} a^n = 0. }$}{geom_prog_common_ratio_less_than_one_tends_to_zero}
		\begin{proof}
			First of all, note that if ${ \abs{a} = 0 }$ then ${ a = 0 = a^n }$ for all n and so the limit holds trivially. For this reason, from here on, we will consider only the case where ${ a \neq 0 }$.\\
			There are 3 parts to this proof:
			\begin{enumerate}
				\item{${ \abs{a} < 1 \implies \lim_{n \to \infty} \abs{a}^n = 0, }$ }
				\item{${ \abs{a}^n = \abs{a^n}, }$ }
				\item{${ \lim_{n \to \infty} \abs{a^n} = 0 \implies \lim_{n \to \infty} a^n = 0 }$.}
			\end{enumerate}
			\begin{enumerate}
			\item{${ \bm{\abs{a} < 1 \implies \lim_{n \to \infty} \abs{a}^n = 0} }$}\label{geom_prog_common_ratio_less_than_one_tends_to_zero_1}
			\subitem{
				It would be natural to prove this by showing that,
				\[ \forall \epsilon > 0 \logicsep \exists N \logicsep \forall n > N \in \N{} \logicsep \abs{\abs{a}^n - 0} = \abs{a}^n < \epsilon. \]
				We can show this by "reverse engineering" the value of $N$ from the requirement that ${ \abs{a}^n }$ be less than $\epsilon$,
				\[ \abs{a}^n < \epsilon \iff n\ln{\abs{a}} < \ln{\epsilon} \iff n > \frac{ \ln{\epsilon} }{ \ln{\abs{a}} } \]
				with the last step changing the direction of the inequality because we divide by $\ln{\abs{a}}$ which - remembering that ${ \abs{a} < 1 }$ - is a negative value. So, in this way, we have shown that ${ N = \frac{ \ln{\epsilon} }{ \ln{\abs{a}} } }$ is a general formula that relates a value of $N$ with the required property with any arbitrary $\epsilon$.\\
				However, this proof is not valid because it uses the concept of the logarithm which requires a lot of analysis that has not been proven at this stage. Since we are trying to build the fundamental basis of analysis, at this point we can only concepts that are pre-requisites (axiomatic) in analysis or have been proven at this stage.
			}
			\subitem{
				An alternative way to show this, using the properties of limits of sequences just proven, is as follows: Let $x_n$ be the sequence ${ x_n = \abs{a}^n }$. Then, because ${ 0 < \abs{a} < 1 }$,
				\[ x_{n+1} = x_n \cdot x = \abs{a}^n\abs{a} < \abs{a}^n = x_n \]
				so that $x_n$ is a decreasing sequence. Additionally, ${ \forall n \in \R{} \logicsep \abs{a}^n \geq 0 }$ so 0 is a lower bound on the sequence. Therefore, the sequence converges to a limit (note we haven't yet established that 0 is the limit - only that it is a candidate). Furthermore, ${ x_{n+1} = \abs{a}^{n+1} = \abs{a}^n\abs{a} }$ and, if ${ x_n \to L }$ then ${ x_{n+1} \to L }$ also. But, putting these two facts together, along with property (d) of limits of sequences, means that,
				\[ L = \lim_{n \to \infty} \abs{a}^{n+1} = \lim_{n \to \infty} \abs{a}^n\abs{a} = (\lim_{n \to \infty} \abs{a}^n)(\lim_{n \to \infty} \abs{a}) = \abs{a}(\lim_{n \to \infty} \abs{a}^n) = \abs{a}L. \]
				So,
				\[ L = \abs{a}L \iff L(1 - \abs{a}) = 0 \]
				and, since we know that ${ \abs{a} \neq 1 }$, therefore $L$ must be 0.
			}
			
			\item{${\bm{ \abs{a}^n = \abs{a^n} }}$}\label{geom_prog_common_ratio_less_than_one_tends_to_zero_2}
			\subitem{
				For ${ a \in \R{}, n \in \N{} }$ it's easy to see that ${ \abs{a}\abs{a}\dots\abs{a} = \abs{aa \dots a} }$.\\
				In actual fact, this appears to hold even for ${ n \in \Q{} }$, e.g.
				\[ \abs{ (-1)^{\frac{1}{2}} } = \abs{i} = 1 = \abs{ 1^{\frac{1}{2}} } = \abs{1} = 1 \]
				but this should be checked when studying complex numbers more thoroughly. Also, the base $a$, can it also be complex?
			}
			
			\item{${\bm{ \lim_{n \to \infty} \abs{a^n} = 0 \implies \lim_{n \to \infty} a^n = 0 }}$}
			\subitem{
				This can be proved directly from the definition of the limit.
				\begin{align*}
				&& \lim_{n \to \infty} \abs{a^n} = 0 &\iff \forall \epsilon > 0 \logicsep \exists N \logicsep \forall n > N \in \N{} \logicsep \abs{\abs{a^n} - 0} < \epsilon  \\
				&&&\iff \forall \epsilon > 0 \logicsep \exists N \logicsep \forall n > N \in \N{} \logicsep \abs{a^n} < \epsilon  &\sidecomment{} \\[3pt]
				&&&\iff \forall \epsilon > 0 \logicsep \exists N \logicsep \forall n > N \in \N{} \logicsep \abs{a^n - 0} < \epsilon  &\sidecomment{} \\[3pt]
				&&&\iff \lim_{n \to \infty} a^n = 0.  &\sidecomment{} \\
				\end{align*}
			}
			\end{enumerate}
								
			Bear in mind that, in general, ${ \lim_{n \to \infty} \abs{x_n} = L \centernot\implies \lim_{n \to \infty} x_n = L }$. For example, if $x_n$ converges to $-L$ then $\abs{x_n}$ will converge to $L$.\\
			\href{https://github.com/mcopeland2001/maths-notes/blob/master/resources/code/python/converge_to_minus_2.py}{\scriptsize\ttfamily{code example}}\\\\
			
			Furthermore, the example \ref{ex:infinite_recurrence} showed how the fact of $a_n$ and $a_{n+1}$ converging to the same limit produces - when the sequence is expressed as a recurrence - an equilibrium value.
		\end{proof}
	
		\subsubsection{The Sandwich Theorem}
		\labeledProposition{Let ${ a_n, b_n, c_n }$ be sequences such that,
			\[ \text{for all } n,\; a_n \leq b_n \leq c_n \hspace{10pt} \text{and} \hspace{10pt} \lim_{n \to \infty} a_n = L = \lim_{n \to \infty} c_n. \]
			Then ${ \lim_{n \to \infty} b_n = L. }$
		}{sandwich_theorem}
		\begin{proof}
			${ \lim_{n \to \infty} a_n = L }$ means that,
			\begin{align*}
			&& \forall \epsilon > 0 \logicsep \exists N_1 \logicsep \forall n > N_1 \in \N{} &\logicsep \abs{a_n - L} < \epsilon  &\sidecomment{} \\
			&\iff & \forall \epsilon > 0 \logicsep \exists N_1 \logicsep \forall n > N_1 \in \N{} &\logicsep -\epsilon < a_n - L < \epsilon  &\sidecomment{} \\
			&\iff & \forall \epsilon > 0 \logicsep \exists N_1 \logicsep \forall n > N_1 \in \N{} &\logicsep L - \epsilon < a_n < L + \epsilon.  &\sidecomment{}
			\end{align*}
			By the same reasoning we also have,
			\[ \forall \epsilon > 0 \logicsep \exists N_2 \logicsep \forall n > N_2 \in \N{} \logicsep L - \epsilon < c_n < L + \epsilon. \]
			So, if we let ${ N = max\{N_1, N_2\} }$ then we have,
			\[ \forall \epsilon > 0 \logicsep \exists N \logicsep \forall n > N \in \N{} \logicsep L - \epsilon < a_n, c_n < L + \epsilon \]
			and since we also know that ${ a_n \leq b_n \leq c_n }$ it follows that,
			\[ \forall \epsilon > 0 \logicsep \exists N \logicsep \forall n > N \in \N{} \logicsep L - \epsilon < a_n \leq b_n \leq c_n < L + \epsilon. \]
			This shows that,
			\[ \forall \epsilon > 0 \logicsep \exists N \logicsep \forall n > N \in \N{} \logicsep \abs{b_n - L} < \epsilon. \]
		\end{proof}
	
		\subsubsection{An example application of the Sandwich Theorem}
		\begin{exe}
			\ex {\textbf{Prove that ${\bm{ \abs{x} < 1 \implies \lim_{n \to \infty} x^n = 0 }}$}\\\\
				We have already proven this using the properties of limits in \autoref{theo:geom_prog_common_ratio_less_than_one_tends_to_zero} but here we are going to prove it using the Sandwich Theorem.\\
				\begin{proof}
					Firstly, as we showed in \ref{geom_prog_common_ratio_less_than_one_tends_to_zero_2}, ${ \abs{x^n - 0} = \abs{x^n} = \abs{x}^n. }$ So to show that $x^n$ tends to zero we can show that $\abs{x}^n$ tends to zero. So, \WLOG we take ${ x > 0 }$ (since ${ x = 0 }$ makes the proposition trivially true). Then, notice that ${ 0 < x < 1 \implies x = \frac{1}{1 + h} }$ for some ${ h > 0 }$. Then, we can show inductively that ${ (1 + h)^n \geq 1 + hn }$ as follows.\\
					\paragraph{Base cases 0, 1:} ${ (1 + h)^0 = 1 = 1 + h(0), \; (1 + h)^1 = 1 + h = 1 + h(1). }$
					\paragraph{Induction step ${\bm{ k > 1 }}$}
					\begin{align*}
					&& (1 + h)^k  &\geq 1 + hk  \\
					&\iff & (1 + h)(1 + h)^k  &\geq (1 + h)(1 + hk) &\sidecomment{} \\
					&\iff & (1 + h)^{k+1}  &\geq 1 + hk + h + h^2k = 1 + h(k + 1) + h^2k > 1 + h(k + 1) &\sidecomment{} \\
					\end{align*}
					This result implies that,
					\[ x^n = \frac{1}{(1 + h)^n} \leq \frac{1}{1 + hn} \]
					so that,
					\[ 0 < x^n \leq \frac{1}{1 + hn}. \]
					Since $h$ is some fixed value, clearly,
					\[ \lim_{n \to \infty} \frac{1}{1 + hn} = 0 \]
					and, obviously, the limit of the constant 0 is always 0 so, by the Sandwich Theorem,
					\[ \lim_{n \to \infty} x^n = 0. \]
				\end{proof}
			}\label{ex:geom_prog_common_ratio_less_than_one_tends_to_zero}
		\end{exe}
	}

	\pagebreak
	\searchableSubsection{Subsequences}{analysis}{\bigskip
		\boxeddefinition{Let ${ (a_n)_{n \in \N{}} }$ be a sequence and consider some strictly increasing natural
			numbers ${ (k_1, k_2, k_3, \dots) }$ that is, ${ (k_1 < k_2 < k_3 < k_4 < \dots). }$ Then the sequence ${ (a_{k_n})_{n \in \N{}} }$ is
			called a \textbf{subsequence} of the sequence ${ (a_n)_{n \in \N{}} }$.\\
			Note that a \textbf{subsequence} is always infinite (I think).
		}
	
		\labeledTheorem{If $a_n$ is a sequence that tends to a limit, then any subsequence of it tends to the same limit.}{subseq_tends_to_same_limit}
		\begin{proof}
			Firstly, notice that if the $n$th index of some subsequence is $k_n$ then $k_n \geq n$ (because the subsequence can only skip terms of the original - it can't add in terms). So then, if we have a sequence $a_n$ that tends to a limit $a$ and an arbitrary subsequence $a_{k_n}$ then,
			\[ \forall \epsilon > 0 \logicsep \exists N \logicsep \forall n > N \in \N{} \logicsep \abs{a_n - a} < \epsilon \implies \abs{a_{k_n} - a} < \epsilon \]
			because ${ k_n \geq n > N }$.
		\end{proof}
	
		\labeledTheorem{Every sequence has a monotonic subsequence.}{every_seq_has_monotonic_subseq}
		\begin{proof}
			Either there is an infinite number of terms that are greater than all the following terms or there is not. If there is not, then after the last such term all terms have a term that follows them that is greater than or equal to them.\\
			In the first case we have a strict monotonic decreasing sequence and in the second we have a non-strict monotonic increasing sequence.
		\end{proof}
	
		If we put this theorem together with what we learned about bounded sequences in \autoref{prop:increasing_bounded_seqs_have_limit} and its corollaries - that monotonic bounded sequences are convergent - then we get one of the most famous results in analysis, The Bolzano-Weierstrass Theorem.
		
		\labeledTheorem{\textbf{The Bolzano-Weierstrass Theorem}\\
			Every bounded sequence has a convergent subsequence.
		}{every_bounded_seq_has_convergent_subseq}
	}

	\pagebreak
	\searchableSubsection{Some problems on limits of sequences}{analysis}{\bigskip
		\subsubsection{\small{ Let ${ (a_n)_{n \in \N{}} }$ be a sequence, and let ${ (b_n)_{n \in \N{}} }$ be the sequence defined by ${ b_n = \abs{a_n} }$ for ${ n \in \N{} }$. Which of the following two statements implies the other? }}
		\textbf{\small{ 
			\begin{enumerate}[label=\alph*)]
				\item{${ (a_n) }$ converges.}
				\item{${ (b_n) }$ converges.}
			\end{enumerate}
		}}
		Answer: ${ a_n }$ converges ${ \implies b_n }$ converges also but ${ b_n }$ converges ${ \centernot\implies a_n }$ converges.\\
		The first implication is because,
		\begin{align*}
		&& \abs{a_n} = \abs{a_n - a + a} &\leq \abs{a_n - a} + \abs{a}  \\
		&\iff &  \abs{a_n} - \abs{a} &\leq \abs{a_n - a}  &\sidecomment{} \\
		\end{align*}
		\begin{align*}
		&& \abs{a} = \abs{a - a_n + a_n} &\leq \abs{a_n - a} + \abs{a_n}  \\
		&\iff & \abs{a} - \abs{a_n} &\leq \abs{a_n - a}  &\sidecomment{} \\
		&\iff & \abs{a_n} - \abs{a} &\geq -\abs{a_n - a}  &\sidecomment{} \\
		\end{align*}
		which both together imply that ${ \abs{ \abs{a_n} - \abs{a} } \leq \abs{a_n - a} }$.\\
		The latter non-implication is easy to see if one thinks of a sequence that consists of two subsequences that converge to 2 and -2. Then, their absolute value would converge to 2 but their values do not converge. Remember \autoref{theo:subseq_tends_to_same_limit}, for a sequence to converge to a limit, every subsequence of it must converge to the same limit.
	}
	
	\bigskip\bigskip
	\searchableSubsection{Limits of functions}{analysis}{\bigskip
		\subsubsection{Definition of the limit of a function}
		\boxeddefinition{Let ${ f: \R{} \mapsto \R{} }$ be a function. We say that $L$ is the \textbf{limit of ${\bm{ f(x) }}$ as ${\bm{ x }}$ approaches ${\bm{ a }}$} if, for each ${ \epsilon > 0 }$ there exists ${ \delta > 0 }$ such that,
			\[ 0 < \abs{x - a} < \delta \implies \abs{f(x) - L} < \epsilon. \]
		}
		\boxeddefinition{Let ${ f: \R{} \mapsto \R{} }$ be a function. We say that \textbf{${\bm{ f(x) }}$ tends to infinity as ${\bm{ x }}$ approaches ${\bm{ a }}$} if, for each ${ K }$ there exists ${ \delta > 0 }$ such that,
			\[ 0 < \abs{x - a} < \delta \implies f(x) > K. \]
		}
	
		\subsubsection{Examples of limits of functions}
		\begin{exe}
			\ex {\textbf{Prove that if ${\bm{ f: \R{} \mapsto \R{} \suchthat f(x) = x^2 + x }}$ then ${\bm{ \lim_{x \to 2} f(x) = 6 }}$.}\\\\
				Let ${ 0 < \abs{x - 2} <\delta }$ and consider some arbitrary ${ \epsilon > 0 }$. Then we have,
				\[ \abs{(x^2 + x) - 6} = \abs{(x - 2)(x + 3)} \leq \abs{x - 2}\abs{x + 3} < \delta \abs{x + 3}. \]
				It's tempting at this point to say that, since we are examining the behaviour when $x$ approaches 2 so we can assume ${ x \approx 2 \iff (x+3) \approx 5 }$ and then we can set ${ \delta = \frac{\epsilon}{6} }$ so that,
				\[ 0 < \abs{x - 2} < \delta = \frac{\epsilon}{6} \implies \abs{f(x) - 6} < \frac{\abs{x + 3}}{6} \epsilon < \epsilon. \]
				\textit{However}, there is a subtle logical problem here: We have considered any arbitrary ${ \epsilon > 0 }$, meaning that $\epsilon$ could be large. Then, when we set ${ \delta = \frac{\epsilon}{6} }$ we linked the value of $\delta$ to that of $\epsilon$ so that $\delta$ may also be arbitrarily large. But ${ 0 < \abs{x - 2} < \delta }$ so that ${ \abs{x - 2} }$ may be arbitrarily large also. This contradicts the assumption that ${ x \approx 2 }$ and so the argument we have here is only valid for small $\epsilon$.\\\\
				So, we need an alternative approach. Consider that,
				\[ \abs{x + 3} = \abs{x - 2 + 2 + 3} \leq \abs{x - 2} + \abs{2 + 3} = \abs{x - 2} + 5 < \delta + 5. \]
				This means that,
				\[ \abs{(x^2 + x) - 6} < \delta \abs{x + 3} < \delta (\delta + 5). \]
				But note, do \textbf{not} start trying to solve a quadratic. There is a much better way.\\\\
				Now - here comes the clever bit - if we set ${ \delta = min\{1, \frac{\epsilon}{6}\} }$ then \textit{both} 1 and $\frac{\epsilon}{6}$ are an upper bound on the value of $\delta$ so that we can say,
				\[ \abs{(x^2 + x) - 6} < \delta (\delta + 5) \leq 6 \delta \leq 6 \frac{\epsilon}{6} = \epsilon. \]
			}
		\end{exe}
	
		\bigskip
		\subsubsection{Algebra of limits of functions}
		\labeledTheorem{Let ${ f, g : \R{} \mapsto \R{} }$ be two functions and $c$ be any real number. Suppose that ${ \lim_{x \to a} f(x) = L }$ and ${ \lim_{x \to a} g(x) = M }$. Then,
			\begin{enumerate}
				\item{${ \lim_{x \to a} (cf)(x) = cL }$}
				\item{${ \lim_{x \to a} (\abs{f})(x) = \abs{L} }$}
				\item{${ \lim_{x \to a} (f + g)(x) = L + M }$}
				\item{${ \lim_{x \to a} (f - g)(x) = L - M }$}
				\item{${ \lim_{x \to a} f(x)g(x) = LM }$}
				\item{${ \lim_{x \to a} (f / g)(x) = L/M }$ provided ${ g(x) \neq 0 }$ for any $x$ in the neighbourhood of $a$.}
			\end{enumerate}
		}{func_limit_properties}
		\begin{proof}
			see: \href{http://tutorial.math.lamar.edu/Classes/CalcI/LimitProofs.aspx}{Lamar University - Paul's Online Notes - Proofs of limit properties}
		\end{proof}
	
		\bigskip
		\subsubsection{One-sided limits}
		\boxeddefinition{Let ${ f : \R{} \to \R{} }$ be a function. We say that $L$ is the \textbf{limit of ${\bm{ f(x) }}$ as ${\bm{ x }}$ approaches ${\bm{ a }}$ from the left (or from below)}, denoted by ${ \lim_{x \to a^-} f(x) = L }$, if for each ${ \epsilon > 0 }$, there exists ${ \delta > 0 }$ such that,
				\[ a - \delta < x < a \implies \abs{f(x) - L} < \epsilon. \]
		}
		\subsubsection{Limits at infinity}
		\boxeddefinition{Let ${ f : \R{} \to \R{} }$ be a function. We say that $L$ is the \textbf{limit of ${\bm{ f(x) }}$ as ${\bm{ x }}$ approaches ${\bm{ \infty }}$}, denoted by ${ \lim_{x \to \infty} f(x) = L }$, if for each ${ \epsilon > 0 }$, there exists ${ M > 0 }$ such that,
			\[ x \geq M \implies \abs{f(x) - L} < \epsilon. \]
		}
	}

	\pagebreak
	\searchableSubsection{Continuity}{analysis}{\bigskip
		\boxeddefinition{A function is \textbf{continuous} at a point $a$ if $f(a)$ is defined and is equal to the value of ${ \lim_{x \to a} f(x) }$.}
		\boxeddefinition{A function is \textbf{continuous} if it is continuous at every point $a$.}
		\boxeddefinition{A function is \textbf{continuous} on the closed interval ${ [a, b] }$ if it is continuous at every point in the open interval ${ (a, b) }$ and:
			\begin{itemize}
				\item{${ \lim_{x \to a^+} f(x) = f(a) }$;}
				\item{${ \lim_{x \to b^-} f(x) = f(b) }$.}
			\end{itemize}
		}
	
		\labeledTheorem{Let ${ f,g : \R{} \to \R{} }$ be functions that are continuous at ${ a \in R }$ and $ c $ be
			any real number. Then ${ \abs{f}, (cf), (f - g), (f + g), (f(x)g(x)) }$ are all continuous at $ a $, and
			${ (f/g)  }$ is continuous provided ${ g(x) \neq 0 }$ for any $ x $ in some neighbourhood of $ a $.}{}
		\begin{proof}
			This follows from the algebra of limits of functions given in \autoref{theo:func_limit_properties}.
		\end{proof}
	
		\begin{corollary}
			It follows then that every polynomial is continuous. This can be seen as the most simple polynomial is a constant - which is clearly continuous; also ${ f(x) = x }$ is clearly continuous. Then powers of $x$ are continuous as they are products of continuous functions and when multiplied by coefficients this is a constant multiplying a continuous function so the resultant function is continuous. Then any polynomial is a summation of such terms so the result is continuous as the sum of continuous functions is continuous.
		\end{corollary}
	
		\labeledTheorem{If $ g $ is a function which is continuous at $ a $, and $ f $ is a function which is
			continuous at $ g(a) $. Then ${ (f \circ g)  }$ is continuous at $ a $.}{comps_continuous_functions_are_continuous}
		\begin{proof}
			Continuity of $f$ at $g(a)$ guarantees that for any ${ \epsilon > 0 }$ there exists some ${ \delta' > 0 }$ such that ${ \abs{x' - g(a)} < \delta' \implies \abs{f(x') - f(g(a))} < \epsilon }$ and continuity of $g$ at $a$ guarantees that for any ${ \delta' > 0 }$ there exists some ${ \delta > 0 }$ such that ${ \abs{x - a} < \delta \implies \abs{g(x) - g(a)} = \abs{x' - g(a)} < \delta' }$.
		\end{proof}
	
		\subsubsection{Unifying continuity of functions with sequences}
		We now give an alternative definition of continuity which ties in the concept of limits for sequences.
		\labeledTheorem{A function $ f $ is continuous at $ a $ if and only if for each sequence $ (x_n) $ such that ${ \lim_{n \to \infty} x_n = a  }$ we have ${ \lim_{n \to \infty} f(x_n) = f(a) }$.}{continuity_of_functions_over_sequences}
		\note{
			Before the proof, an important point to note is that this theorem applies to "each sequence" with the described limit. This is important as it is possible to find an individual sequence such that the inference is not valid. For example, the constant sequence ${ \forall n \in \N{} \logicsep x_n = a }$ clearly tends to $a$ as ${ n \to \infty }$ but this would not imply continuity of $f$ as the limit of $f(x_n)$ for such a sequence would amount to saying that ${ f(a) = f(a) }$. The definition of continuity is assertion of the equality of the limit of $f$ over values in the neighbourhood of $a$ (but not at $a$ itself) with the value of $f$ at $a$. So, continuity is implied by the fact that the limit of $f(x_n)$ for the constant sequence $x_n = a$ is equal to the limit of $f(x_n)$ for all other sequences $x_n$ whose limit is $a$.\\
			Another way of looking at it is that $f$ is continuous at $a$ because the limit there equals $f(a)$ however the argument converges to $a$.
		}
		\begin{proof}
			Breaking it down into two propositions we have,
			\begin{equation}
				(\forall x_n \suchthat \lim_{n \to \infty} x_n = a) \; \lim_{n \to \infty} f(x_n) = f(a) \tag{$P_1$}
			\end{equation}
			\begin{equation}
				\forall \epsilon > 0 \logicsep \exists \delta \logicsep 0 < \abs{x - a} < \delta \implies \abs{f(x) - f(a)} < \epsilon \tag{$P_2$}
			\end{equation}
			and we need to show that ${ P_1 \iff P_2. }$\\\\
			So, to begin with we'll show that ${ P_1 \implies P_2. }$ Unpacking the statement ${ \lim_{n \to \infty} f(x_n) = f(a) }$ we have,
			\[ \forall \epsilon > 0 \logicsep \exists N \logicsep \forall n > N \in \N{} \logicsep \abs{f(x_n) - f(a)} < \epsilon \]
			which tells us that for every ${ \epsilon > 0 }$ there is an $N$ such that the statement holds true for the range of $n$ above $N$. But since this is the case for all sequences $x_n$ such that ${ \lim_{n \to \infty} x_n = a }$ we also have
			\[ \forall \delta > 0 \logicsep \exists N \logicsep \forall n > N \in \N{} \logicsep \abs{x_n - a} < \delta \]
			so that the range of $n$ such that the statement about $f(x_n)$ holds true will also fall inside some range such that there is a $\delta$ for which ${ \abs{x_n - a} < \delta }$. And, since this is true \textit{whenever} we have a sequence $x_n$ such as this, we can say that proposition $P_1$ amounts to
			\[ \forall \epsilon > 0 \logicsep \exists \delta > 0 \logicsep \abs{x_n - a} < \delta \implies \abs{f(x_n) - f(a)} < \epsilon. \]
			We can see that this is almost the same as the assertion of continuity of $f$, proposition $P_2$, \textit{but not quite}. The difference is worth noting. The difference is that continuity is only asserted over values of $x$ such that ${ 0 < \abs{x - a} }$ TODO: change the definition of continuity !
		\end{proof}
	}

\end{document}