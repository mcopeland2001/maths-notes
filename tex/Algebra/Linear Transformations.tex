\documentclass[../MathsNotesBase.tex]{subfiles}


\date{\vspace{-6ex}}


\begin{document}
\searchableSection{Linear Transformations}{linear algebra}

	\searchableSubsection{\texorpdfstring{Basic Properties of Linear\\ Transformations}{Basic Properties of Linear Transformations}}{linear algebra}{
		\label{ssection:basic-properties-linear-transformations}
		\bigskip\bigskip
		
		The analogue for vector spaces of a homomorphism of groups is a map,
		\[ T: V \longmapsto W \]
		from one vector space over a field $\F{}$ to another, which is compatible with addition and scalar multiplication:
		\[ T(\V{v_1} + \V{v_2}) = T(\V{v_1}) + T(\V{v_2}) \eqand T(c\V{v_1}) = cT(\V{v_1}), \]
		for all ${ \V{v_1},\V{v_2} \in V,\, c \in \F{} }$.\\
		
		\note{Note that another way of describing this is that \textbf{linear combinations are preserved across linear transformations}. That's to say, if
			\[ \u = \alpha_1 \v_1 + \cdots + \alpha_n \v_n \eqand \w = \alpha_1 f(\v_1) + \cdots + \alpha_n f(\v_n) \]
			then, if $f$ is linear we also have,
			\[ f(\u) = \w. \]
		}
		
		\boxeddefinition{A homomorphism between two vector spaces that is also compatible with scalar multiplication is called a \textbf{linear transformation} or \textbf{linear map or mapping}. That's to say, if $T$ is a linear map over a vector space defined over a field $\F{}$; each $\v_i$ is drawn from the vector space; and each $\alpha_i$ is drawn from $\F{}$ then,
			\[ T\left( \sum_i \alpha_i \v_i \right) = \sum_i \alpha_i T(\v_i). \]
		}
		
		\note{The compatibility with addition of vectors implies that a linear transformation is a homomorphism between additive groups of vectors.}
		
		\note{Linear transformations \textbf{preserve} linear combinations in their arguments but this must be distinguished carefully from \textbf{being equal} to linear combinations of their arguments. A typical linear transformation is \textbf{not} expressible as a linear combination of it's sole argument and --- in fact --- the image of a vector under a linear map only fails to be linearly independent to the original vector in the case that the original vector is an eigenvector ${ A\v = c\v. }$\\ It is, however, worth noting that a linear map between finite vector spaces may be thought of as the application of a coordinate vector to a different basis than that against which it was originally defined. Therefore, any linear combination of objects may be thought of as a linear map between the space of coefficients and the space of objects. For example, the linear combination,
			\[ \alpha_1 x_1 + \alpha_2 x_2 + \cdots + \alpha_n x_n \]
		may be thought of as a linear map from the space of coefficients ${ \alpha_i }$ to the space of objects ${ x_i }$,
			\[ T\left(\begin{bmatrix}\alpha_1\\\alpha_2\\\vdots\\\alpha_n\end{bmatrix}\right) = \alpha_1 x_1 + \alpha_2 x_2 + \cdots + \alpha_n x_n. \]
		}
		
		\begin{corollary}\label{coro:linear-maps-map-the-origin-to-itself}
			As with all homomorphisms, linear maps always map the identity to the identity. For linear maps this means mapping the zero vector to the zero vector.
		\end{corollary}
		
		\bigskip
		\labeledProposition{A linear map is homogeneous of degree 1.}{lin-map-is-homogeneous-degree-1}
		\begin{proof}
			${ L(\alpha \v) = \alpha L(\v) }$ for all ${ \alpha \in \F{},\; \v \in V }$.
		\end{proof}

		
		\medskip
		\labeledProposition{Linear Dependence is always preserved across any linear transformation.}{lin-dependence-preserved-across-any-lin-transform}
		\begin{proof}
			Let ${ \alpha_1 \v_1 + \alpha_2 \v_2 + \cdots + \alpha_n \v_n = \0 }$ be a linear relation between the vectors ${ \{\v_1,\dots,\v_n\} }$. If $L$ is a linear map then,
			\begin{align*}
				&& L(\alpha_1 \v_1 + \alpha_2 \v_2 + \cdots + \alpha_n \v_n) &= L(\0) \\
				&\iff & \alpha_1 L(\v_1) + \alpha_2 L(\v_2) + \cdots + \alpha_n L(\v_n) &= \0. \qedhere \\
			\end{align*}
		\end{proof}
		
		
		\biggerskip
		\subsubsection{The Kernel and the Image of a Linear Transformation}
		\boxeddefinition{
			Let ${ T: V \longmapsto W }$ be any linear transformation. Then the \textbf{kernel (or nullspace)} of $T$ is defined as,
		\[ ker\,T = \setc{\v}{T(\v) = \0}  \]
		and the \textbf{image} of $T$ as,
		\[ im\,T = \setc{\w \in W}{\exists \v \in V \logicsep \w = T(\v)}. \]
		}
		
		\medskip
		\labeledProposition{The kernel of ${ T: V \longmapsto W }$ is a subspace of $V$ and the image is a subspace of $W$.}{kernel_and_image_of_linear_map_are_subspaces}
		\begin{proof}
			$T$ is a homomorphism between additive groups of vectors and so the proof that the kernel and image are subspaces is the same as for general homomorphisms (see \ref{sssection:image_of_homomorphism}).
		\end{proof}
	
		\medskip
		\labeledProposition{The fibres of a linear transformation are the additive cosets of the kernel.}{lin_transform_fibres_are_additive_cosets_of_kernel}
		\begin{proof}
			Let ${ T: V \longmapsto W }$ be any linear transformation with kernel ${ K = ker\,T }$. Then, for any fixed ${ \v \in V }$,
			\[ \forall \V{k} \in K \logicsep T(\v + \V{k}) = T(\v) + T(\V{k}) = T(\v) + \0 = T(\v). \]
			So every element in the additive coset ${ \v + K }$ maps to the same value $T(\v)$ in $W$. Therefore we have,
			\[ im\,T = \setc{\w \in W}{\exists (\v + K) \subseteq V \logicsep \{\w\} = T(\v + K)}. \]
			We could also express this using the inverse image as in \ref{def:inverse_image} but the notation would be easily confused for the inverse transformation.
		\end{proof}
		\smallskip
		\begin{corollary}
			Let ${ T: V \longmapsto W }$ be any linear transformation with kernel ${ K = ker\,T }$ and let there be ${ \v \in V,\, \w \in W }$ such that 
				\[ T(\v) = \w. \] 
			Then,
				\[ T(\x) = \w \iff \x \in (\v + K). \]
		\end{corollary}
	
		\medskip
		\labeledProposition{Linear Independence is preserved across a linear transformation iff the transformation is injective.}{linear_independence_not_preserved_across_homomorphic_lin_transform}
		\begin{proof}
			Let ${ T: V \longmapsto W }$ be any linear transformation with kernel ${ K = ker\,T }$. If the kernel is nontrivial then there exists a nonempty basis of the kernel ${ B_K = \{ \V{k_1}, \dots, \V{k_n} \} }$. Being a basis $B_K$ is linearly independent so that,
			\[ \alpha_1\V{k_1} + \cdots + \alpha_n\V{k_n} = \0 \iff \alpha_1,\dots,\alpha_n = 0. \]
			However, $T(B_K)$ the image of $B_K$ under $T$ is
			\[ \{ \0, \dots, \0 \} \]
			which, by \autoref{prop:set_containing_zero_vector_not_linearly_independent}, is obviously not linearly independent.\\\\
			We can also see it more directly from the definition of a linear relation.
			\begin{align*}
			&& T(\alpha_1\V{k_1} + \cdots + \alpha_n\V{k_n}) &= \0 \\
			&\iff & \alpha_1T(\V{k_1}) + \cdots + \alpha_nT(\V{k_n}) &= \0  &\sidecomment{} \\
			&\iff & \alpha_1\0 + \cdots + \alpha_n\0 &= \0  &\sidecomment{}
			\end{align*}
			So clearly,
			\[ \alpha_1T(\V{k_1}) + \cdots + \alpha_nT(\V{k_n}) = \0 \centernot\implies \alpha_1,\dots,\alpha_n = 0 \]
			which proves that if $T$ is not injective then it does not preserve linear independence.\\\\
			Conversely, if $T$ does not preserve linear independence then, if ${ U = \{ \u_1,\dots,\u_n \} \subset V }$ is a linearly independent set in $V$, there is a nontrivial linear relation between the vectors in $T(U)$ the image of $U$ under $T$. That's to say,
			\[ \alpha_1T(\u_1) + \cdots + \alpha_nT(\u_n) = \0  \hspace{10pt}\text{ where } \prod_{i=1}^{n}\alpha_i \neq 0. \]
			But this means that,
			\begin{align*}
			&& \alpha_1T(\u_1) + \cdots + \alpha_nT(\u_n) &= \0 \\
			&\iff & T(\alpha_1\u_1 + \cdots + \alpha_n\u_n) &= \0 &\sidecomment{} \\
			&\iff & \alpha_1\u_1 + \cdots + \alpha_n\u_n \in ker\,T. &\sidecomment{}
			\end{align*}
			Since $U$ is linearly independent there is no nontrivial linear relation between its elements and since ${ \prod_{i=1}^{n}\alpha_i \neq 0 }$ we can conclude that,
			\[ \alpha_1\u_1 + \cdots + \alpha_n\u_n \neq \0 \implies ker\,T \neq \{\0\} \]
			and therefore this proves that if $T$ does not preserve linear independence then it is not injective.
		\end{proof}
	
		\note{Note that \textbf{linear dependence}, however, is preserved across any linear map (\autoref{prop:lin-dependence-preserved-across-any-lin-transform}).}
		
		\bigskip
		\subsubsection{Examples of Linear Transformations}
		\begin{exe}
			\item{As previously seen in section \ref{ssection:matrices_as_linear_transformations}, matrix multiplication on the left is a linear transformation. Let $A$ be an ${ m \times n }$ matrix with entries in $\F{}$ and consider $A$ as an operator on column vectors ${ A: \F{n} \longmapsto \F{m} }$. The kernel of $A$ is the set of vectors that are solutions to ${ A\V{x} = \0 }$ while the image (or range) is the set of vectors $\V{b}$ such that ${ A\V{x} = \V{b} }$ has a solution.\\
				The solutions ${ \setc{\V{x} \in \F{n}}{A\V{x} = \V{b}} }$ for some fixed ${ \V{b} \in \F{m} }$ are the additive coset ${ \v + K }$ where $K$ is the kernel of $A$ and ${ \v \in \F{n} }$ is such that ${ A\v = \V{b} }$. Compare with \ref{ex:coset_with_kernel}.
			}
			\item{Also previously seen in \ref{ssection:polynomials_as_vector_spaces} is that polynomials can be modeled as vectors. Let $P_n$ be the vector space of real polynomials of degree ${ \leq n }$. Then the derivative is a linear transformation ${ P_n \longmapsto P_{n-1} }$. The kernel of the derivative is the set of degree 0 polynomials (i.e. constant functions) and the additive cosets of the kernel are ${ f(x) + c }$, for ${ f(x) \in P_n }$ and constant $c$.
			}
		\end{exe}
		
		
		\bigskip\bigskip
		\subsubsection{The Dimension of a Linear Transformation}
		\boxeddefinition{The dimension of the image is called the \textbf{rank} while the dimension of the kernel is known as the \textbf{nullity}.}
		
		\medskip\note{Dimension and rank also exist for Groups (see \href{https://en.wikipedia.org/wiki/Rank_of_a_group}{wikipedia}) where it refers to the minimal generating set for the group.}
		
		\bigskip
		\labeledTheorem{\textbf{(The Dimension Formula.)} Let ${ T: V \longmapsto W }$ be a linear transformation, and assume that $V$ is finite dimensional. Then,
			\[ \dim V = \dim (\operatorname{ker} T) + \dim (\operatorname{im} T) = \operatorname{rank} + \operatorname{nullity}. \]
		}{linear_map_dimension_formula}
		\begin{proof}
			Let ${ \{\V{k_1}, \dots, \V{k_m}\} }$ be a basis of ${ \operatorname{ker} T }$. Then, by \autoref{prop:lin_ind_set_can_be_extended_to_basis}, it may be extended to a basis of $V$,
			\[ B = \{\V{k_1}, \dots, \V{k_m}, \V{u_1}, \dots, \V{u_n}\}. \]
			So, for any ${ \v \in V }$, $\v$ may be expressed as a linear combination of the vectors in $B$. Therefore, for any ${ \w \in \operatorname{im} T }$,
			\begin{align*}
			&& \w &= T(\alpha_1\V{k_1} + \cdots + \alpha_m\V{k_m} + \beta_1\V{u_1} + \cdots + \beta_n\V{u_n})  \\
			&\iff &  &= T(\alpha_1\V{k_1}) + \cdots + T(\alpha_m\V{k_m}) + T(\beta_1\V{u_1}) + \cdots + T(\beta_n\V{u_n}) &\\
			&\iff &  &= \0 + T(\beta_1\V{u_1}) + \cdots + T(\beta_n\V{u_n})   &\sidecomment{} \\
			&\iff &  &= \beta_1T(\V{u_1}) + \cdots + \beta_nT(\V{u_n})   &\sidecomment{} \\
			\end{align*}
			This shows that ${ B' = \{T(\V{u_1}), \dots, T(\V{u_n})\} }$ spans $im\,T$. Furthermore, if there were a linear relation between the elements of $B'$ then,
			\begin{align*}
			&& \beta_1T(\V{u_1}) + \cdots + \beta_nT(\V{u_n}) &= \0 \\
			&\iff & T(\beta_1\V{u_1} + \cdots + \beta_n\V{u_n}) &= \0 & \\
			&\iff & \beta_1\V{u_1} + \cdots + \beta_n\V{u_n} &\in ker\,T &\sidecomment{} \\
			&\iff & \beta_1\V{u_1} + \cdots + \beta_n\V{u_n} &= \alpha_1\V{k_1} + \cdots + \alpha_m\V{k_m} &\sidecomment{} \\
			\end{align*}
			where this last result implies a linear relation between the vectors of $B$. Since $B$ is a basis this linear relation can only be the trivial relation and so ${ \beta_1, \dots, \beta_n = 0 }$ and $B'$ is linearly independent also.
		\end{proof}
		
		\medskip\note{Notes about the Dimension Formula:
			\begin{itemize}
				\item{The Dimension Formula does not imply that the range of a linear operator and its kernel partition the space (as in a direct sum). The kernel may be in the range. For example, the operator
					\[ T(a,b) = (0,a) \]
				has equal range and nullspace as
				\[ R(T) = N(T) = \setc{(0,y)}{y \in \F{}}. \]
				In this case ${ T^2 = T_0 }$, the zero operator.
				}
				\item{This formula bears a resemblance to Lagrange's Theorem applied to homomorphisms of finite groups (\ref{coro:order_of_image_divides_both_order_of_domain_and_codomain}),
					\[ \cardinality{G} = \cardinality{ker\,\phi} \cdot \cardinality{im\,\phi}. \]
					The difference, however, is that the Dimension Formula of Linear Transformations is dealing with the generators of a group while Lagrange's Theorem is dealing with the orders of the groups. The orders of the groups are the number of elements in the group that are generated by the generators of the group. In the case of a real vector space, the vectors generated by the basis vectors are uncountably infinite due to scalar multiplication by real numbers and so cardinality doesn't apply in the same way.}
				\item{This formula \textbf{only applies to finite-dimensional vector spaces}. This should be clear as we simply cannot do this kind of arithmetic with $\infty$. For example, if the rank is infinite then the dimension of the kernel would be ${ \infty - \infty = ? }$.}
			\end{itemize}
		}
	
		
		\bigskip
		\labeledTheorem{If ${ T: V \longmapsto W }$ is a linear transformation over a finite-dimensional vector space $V$, then the quotient space of $V$ by the kernel of $T$ is a space that is in bijective correspondence to the range of $T$.}{for-lin-map-quotient-by-kernel-is-bijection-with-range}
		\begin{proof}
			Let ${ B_K = \{ \V{k}_1, \dots, \V{k}_k \} }$ be a basis of the kernel of $T$. By \autoref{prop:lin_ind_set_can_be_extended_to_basis}, it may be extended to a basis of $V$,
			\[ B = \{ \V{k}_1, \dots, \V{k}_k, \b_1, \dots, \b_{n-k} \}. \]
			Then, for any ${ \v \in V }$,
			\[\begin{aligned}
				T\v &= T(\alpha_1 \V{k}_1 + \cdots + \alpha_k \V{k}_k + \alpha_{k+1} \b_1 + \cdots + \alpha_n \b_{n-k}) \\
				&= T(\alpha_1 \V{k}_1 + \cdots + \alpha_k \V{k}_k) + T(\alpha_{k+1} \b_1 + \cdots + \alpha_n \b_{n-k}) &\sidecomment{} \\
				&= \0 + T(\alpha_{k+1} \b_1 + \cdots + \alpha_n \b_{n-k}) &\sidecomment{} \\
				&= \alpha_{k+1} T(\b_1) + \cdots + \alpha_n T(\b_{n-k}) &\sidecomment{} \\
			\end{aligned}\]
			Therefore ${ B_R = \{ T(\b_1), \dots, T(\b_{n-k}) \} }$ spans the range of $T$. A linear relation between the elements of $B_R$ would imply,
			\[\begin{aligned}
				&& \alpha_1 T(\b_1) + \cdots + \alpha_{n-k} T(\b_{n-k}) &= \0 \\
				&\iff & T(\alpha_1 \b_1 + \cdots + \alpha_{n-k} \b_{n-k}) &= \0,
			\end{aligned}\]
			which means that the vector,
			\[ \alpha_1 \b_1 + \cdots + \alpha_{n-k} \b_{n-k} \in \operatorname{ker} T \]
			which is impossible by construction of the basis $B$ as it would imply a linear relation with the elements of $B_K$.\\
			Therefore, $B_R$ is linearly independent and a basis of the range (i.e. the image) of $T$. Furthermore, ${ B_{K'} = \{  \b_1, \dots, \b_{n-k} \} }$  is a basis of the quotient space of $V$ by the kernel of $T$ and the map
			\[ \phi: \operatorname{span} B_{K'} \longmapsto \operatorname{span} B_R = R(T) \; \suchthat \phi(\v) = T\v \]
			is a bijection because:
			\begin{itemize}
				\item{For any ${ \v \in R(T) }$, there exists some ${ \beta_1, \dots, \beta_{n-k} }$ such that,
					\[ \v = \beta_1 T(\b_1) + \cdots + \beta_{n-k} T(\b_{n-k})) = T(\beta_1 \b_1 + \cdots + \beta_{n-k} \b_{n-k}) \]
					and, clearly, ${ \beta_1 \b_1 + \cdots + \beta_{n-k} \b_{n-k} \in \operatorname{span} B_{K'} }$. Therefore $\phi$ is surjective.
				}
				\item{For any ${ \v_1, \v_2 \in V }$ such that ${ \phi(\v_1) = \phi(\v_2) }$ we have,
					\[\begin{aligned}
						&& T\v_1 &= T\v_2 \\
						&\iff & T(\alpha_1 \b_1 + \cdots + \alpha_{n-k} \b_{n-k}) &= T(\beta_1 \b_1 + \cdots + \beta_{n-k} \b_{n-k}) &\sidecomment{} \\
						&\iff & \alpha_1 T(\b_1) + \cdots + \alpha_{n-k} T(\b_{n-k}) &= \beta_1 T(\b_1) + \cdots + \beta_{n-k} T(\b_{n-k})
					\end{aligned}\]
					which, by the linear independence of $B_R$ implies that ${ \alpha_i = \beta_i, \; 1 \leq i \leq n-k }$. This in turn implies that,
					\[\begin{aligned}
						&& \alpha_1 \b_1 + \cdots + \alpha_{n-k} \b_{n-k} &= \beta_1 \b_1 + \cdots + \beta_{n-k} \b_{n-k} \\
						&\iff & \v_1 &= \v_2. &\sidecomment{} \\
					\end{aligned}\]
					Therefore $\phi$ is injective. \qedhere
				}
			\end{itemize}
		\end{proof}
		\note{Note that this is the linear algebra version of \autoref{theo:first_isomorphism_theorem}.}

	
		\biggerskip
		\subsubsection{The Algebra of Linear Transformations}
		\bigskip
		\notation{If $T_1$ and $T_2$ are being used to denote linear maps then ${ T_1 T_2 }$ will denote their function composition and any other common operations performed on them (e.g. addition, subtraction, scalar multiplication or division, etc.) will denote a pointwise function definition. That's to say, for example,
			\[ (T_1 + T_2)\v = T_1\v + T_2\v. \]
		}
	
		\bigskip
		\labeledProposition{Any linear combination of linear maps is a linear map.}{linear-combination-of-linear-maps-is-linear-map}
		\begin{proof}
			Let ${ M = \sum_i \alpha_i T_i }$ be a map formed as an arbitrary linear combination of linear maps $T_i$. Then, following the pointwise definition,
			\[\begin{aligned}
				M(\beta_1\v_1 + \beta_2\v_2) &= \sum_i \alpha_i T_i (\beta_1\v_1 + \beta_2\v2) &\sidecomment{pointwise defn.}\nn
				&= \sum_i \alpha_i T_i \beta_1\v_1 + \alpha_i T_i \beta_2\v2 &\sidecomment{linearity of $T_i$}\nn
				&= \sum_i \alpha_i T_i \beta_1\v_1 + \sum_i \alpha_i T_i \beta_2\v2 &\sidecomment{'+' associative, commutative}\nn
				&= \beta_1 \sum_i \alpha_i T_i \v_1 + \beta_2 \sum_i \alpha_i T_i \v2 &\sidecomment{linearity of $T_i$}\nn
				&= \beta_1 M(\v_1) + \beta_2 M(\v2) &\sidecomment{pointwise defn.}.\nn
			\end{aligned}\]
			In fact, we can extend this to any arbitrary linear combination of vectors,
			\[\begin{aligned}
				M\left( \sum_j \beta_j \v_j \right) &= \sum_i \alpha_i T_i \left( \sum_j \beta_j \v_j \right) &\sidecomment{pointwise defn.}\nn
				&= \sum_i \sum_j \alpha_i T_i \beta_j \v_j &\sidecomment{linearity of $T_i$}\nn
				&= \sum_j \sum_i \alpha_i T_i \beta_j \v_j &\sidecomment{'+' associative, commutative}\nn
				&= \sum_j \beta_j \sum_i \alpha_i T_i \v_j &\sidecomment{linearity of $T_i$}\nn
				&= \sum_j \beta_j M(\v_j) &\sidecomment{pointwise defn.}.\nn
			\end{aligned}\]
		\end{proof}
	
	}




% ----------------------



	\pagebreak
	\searchableSubsection{Linear Transformations as Matrices}{linear algebra}{
		\bigskip
		\labeledProposition{Left multiplication by a ${ m \times n }$ matrix is a linear transformation ${ \F{n} \longmapsto \F{m} }$.}{left_multiplication_by_matrix_are_lin_transforms}
		\begin{proof}
			Let ${ T: \F{n} \longmapsto \F{m} }$ be a linear transformation. Then $T$ is a map from $n$-vectors to $m$-vectors that is compatible with the vector space operations. Let $A$ be an ${ m \times n }$ matrix then ${  A\V{x} = \V{b} }$ where ${ \V{x} \in \F{n} }$ and ${ \V{b} \in \F{m} }$ showing that ${ T(\V{x}) = A\V{x} = \V{b} }$ is a map of the form ${ \F{n} \longmapsto \F{m} }$. Furthermore,
			\[ T(\V{x_1} + \V{x_2}) = A(\V{x_1} + \V{x_2}) = A\V{x_1} + A\V{x_2} = T(\V{x_1}) + T(\V{x_2})\]
			and
			\[ T(c\V{x}) = A(c\V{x}) = cA\V{x} = cT(\V{x}) \]
			which shows that left multiplication preserves vector addition and scalar multiplication so that ${ T(\V{x}) = A\V{x} = \V{b} }$ is a linear map as required.
		\end{proof}
	
		\medskip
		\labeledTheorem{Every linear transformation ${ \F{n} \longmapsto \F{m} }$ is left multiplication by a particular ${ m \times n }$ matrix.}{every_lin_transform_is_left_multiplication_by_a_particular_matrix}
		\begin{proof}
			For any ${ \V{x} = \langle x_1,\dots,x_n \rangle \in \F{n} }$ we can write it as,
			\[ x_1\e{1} + \cdots + x_n\e{n}. \]
			Therefore if ${ T: \F{n} \longmapsto \F{m} }$ then,
			\[ T(\x) = T(x_1\e{1} + \cdots + x_n\e{n}) = T(\e{1})x_1 + \cdots + T(\e{n})x_n \in \F{m} \]
			and so letting ${ A \in \F{m \times n} }$ be,
			\[ 
				A =
				\begin{bmatrix}
				T(\e{1}) & \cdots & T(\e{n})
				\end{bmatrix}
			\]
			we have,
			\[ T(\V{x}) = A\V{x}. \qedhere \]
		\end{proof}
		\begin{corollary}
			Any linear transformation between spaces isomorphic to $\F{n}$ and $\F{m}$ (refer to \autoref{prop:vector_space_isomorphic_to_coordinate_space_of_same_dimension} and \ref{ex:isomorphism_with_Fn}) is left multiplication by a particular ${ m \times n }$ matrix.
		\end{corollary}
	
		\medskip\note{This is why linear transformations from a space to itself can be wholly characterized by what they do to the axes and also why every such linear transformation can be considered a change of basis and vice-versa.\\
			Conceptually, a linear transformation changes the coordinates of a selection of transformed vectors. The confusion comes about because we implement the matrix of the linear transformation $A$ by transforming the basis against which the coordinates are applied,
			\[ 
				A =
				\begin{bmatrix}
				T(\e{1}) & \cdots & T(\e{n})
				\end{bmatrix}.
			\]
			Whereas a change of basis transforms the basis against which coordinates are applied \textbf{and then updates the coordinates to balance out the change}.\\
			This can be seen if we deconstruct the change of basis formula:
			\[ B\x_B = B'\x_{B'} \iff \x_{B'} = \inv{(B')}B\x_B \]
			\[ \x_{B'} = P\x_B = \inv{(B')}B\x_B \]
			This can be thought of as first obtaining the coordinates against the standard basis ${ B\x_B }$ and then applying the inverse of the target basis so as to obtain the equivalent coordinates against the target basis. But, note, we could also consider the whole thing as a linear transformation represented by $P$.\\\\		
			The biggest difference, however, is that a linear transformation can also be between different spaces --- say from $n$-dimensional space to $m$-dimensional space --- in which case it cannot be thought of as a change of basis as a vector ${ \v \in \F{n} }$ cannot be equivalently expressed using a basis of $\F{m}$ because the two spaces are not isomorphic.
		}
		
		\bigskip
		\subsubsection{Examples of Linear Transformations as Matrices}
		\begin{exe}
			\ex{Let ${ T: \R{2} \longmapsto \R{2} }$ be a linear transformation such that,
				\[
				T(\e{1}) = \begin{bmatrix}
				1 \\
				2
				\end{bmatrix} \eqand
				T(\e{2}) = \begin{bmatrix}
				-1 \\
				0
				\end{bmatrix}.
				\]
				The transformation $T$ has been completely described in this way because, for any ${ \x = \langle x_1, x_2 \rangle \in \R{2} }$ we have,
				\begin{align*}
				&& T(\x) &= T(x_1\e{1} + x_2\e{2}) \\
				&\iff & T(\x) &= x_1T(\e{1}) + x_2T(\e{2}) \\ 
				&\iff & T(\x) &= x_1\begin{bmatrix}
				1 \\
				2
				\end{bmatrix} +
				x_2\begin{bmatrix}
				-1 \\
				0
				\end{bmatrix}
				=
				\begin{bmatrix}
				x_1 - x_2 \\
				2x_1
				\end{bmatrix}. &
				\end{align*}
				So, $T$ is also left multiplication by the matrix,
				\[ A = \begin{bmatrix}
				1 & -1 \\
				2 & 0
				\end{bmatrix}. \]
				\bigskip
			}
			\ex{Consider a linear map ${ T: \R{n} \longmapsto \R{m} }$ and the matrix representing it ${ A \in \R{mn} }$. Suppose the equation ${ A\x = \b }$ has the known solution
				\[ \x = \begin{bmatrix}1\\2\\0\\-1\\0\end{bmatrix} +
						s \begin{bmatrix}2\\1\\1\\0\\0\end{bmatrix} +
						t \begin{bmatrix}1\\1\\0\\-1\\1\end{bmatrix}
						\hspace{30pt} s,t \in \R{}.
				\]
				What can be said about the linear transformation $T$ from looking at the solution $\x$?
				\begin{itemize}
					\item{The dimension of the domain, ${ n = 5 }$, is clear since the solution $\x$ is a vector in the domain space.}
					\item{The nullity - dimension of the kernel - is 2, since there are two free variables ${ s,t }$. The basis of the 2-dimensional nullspace is the two vectors being multiplied by these variables.}
					\item{Using the dimension formula (\autoref{theo:linear_map_dimension_formula}) we can deduce that the rank of $T$ is $n$ - nullity. So the rank is ${ 5 - 2 = 3 }$. This is also supported by the fact that the particular solution has 3 non-zero components.}
				\end{itemize}
				What can \textbf{not} be said?
				\begin{itemize}
					\item{The dimension of the codomain $m$ cannot be derived from looking at the solution $\x$. The dimension of the image of $T$ is given by its range because the kernel maps to the origin in the codomain and so the image of the kernel has dimension zero. Since we are not told whether or not the linear map is surjective we cannot know the dimension of the codomain space - only the image of $T$ in the codomain space.}
				\end{itemize}
				\bigskip
			}
			\ex{The minimal linear transformation is multiplication by a minimal matrix --- a one-by-one matrix, i.e. a scalar. So
				\[ T(\v) = A\v = a\v. \]
				Since the real number line, for example, may be considered a one-dimensional vector space, multiplication of two real numbers, for example, may be considered as a linear transformation.
				\bigskip
			}\label{ex:minimal-linear-transformation}
		\end{exe}
	
		\bigskip\bigskip
		\subsubsection{Linear Transformations and Change of Basis}\label{sssection:linear-trans-and-change-of-basis}
		\medskip
		It is often possible to achieve powerful simplifications of problems by selecting appropriate bases. In this section we will look at linear transformations represented by matrices between arbitrary bases of spaces. So here we are looking at the relationship between linear transformations and change of basis.\\

		\medskip\label{def:matrix_of_lin_transform_wrt_bases}
		\boxeddefinition{If the matrix $A$ of a linear transformation ${ T: V \longmapsto W }$ is defined as, for ${ \x \in V }$,
			\[ T(\x) = A\x = \b \in W \]
			then \textbf{the matrix of $T$ with respect to the bases ${ B \subset V }$ and ${ B' \subset W }$} is defined as the matrix $A$ that satisfies,
			\[ A\x_{B} = \b_{B'} \in W \]
			and also
			\[  T(\x) = [B']A\x_{B} = \b \]
		}
	
		\medskip
		\subsubsection{Intuition of the Matrix of $T$ with respect to Bases}
		\medskip
		Let ${ T: V \longmapsto W }$ be a linear transformation and let ${ B_V = \{\V{v_1},\dots,\V{v_n}\} }$ be a basis of $V$ and ${ B_W = \{\V{w_1},\dots,\V{w_m}\} }$ be a basis of $W$. Then ${ T(\V{v_i}) \in W }$ and so there is some ${ m \times n }$ matrix ${ A = (a_{ij}) }$ such that,
		\[
			\begin{bmatrix}
			\V{w_1} & \cdots & \V{w_m}
			\end{bmatrix}
			A
			=
			\begin{bmatrix}
			T(\V{v_1}) & \cdots & T(\V{v_n})
			\end{bmatrix}.		
		\]
			where,
		\[ T(\V{v_j}) = 
			\begin{bmatrix}
				\V{w_1} & \cdots & \V{w_m}
			\end{bmatrix}
			\begin{bmatrix}
				a_{1j}\\
				\vdots \\
				a_{mj}
			\end{bmatrix}
					= a_{1j}\V{w_1} + \cdots + a_{mj}\V{w_m} = \sum_i a_{ij}\V{w_i} 
		\]
		so that the $j$th column of the matrix $A$ is the coordinate vector of $T(\V{v_j})$ with respect to the basis $B_W$.\\\\		
		Substituting ${ B_W = \{\V{w_1},\dots,\V{w_m}\} }$ we can obtain an expression for $A$,
		\begin{align*}
		&& \begin{bmatrix}
			\V{w_1} & \cdots & \V{w_m}
			\end{bmatrix}
			A &=
			\begin{bmatrix}
			T(\V{v_1}) & \cdots & T(\V{v_n})
			\end{bmatrix} \\
		&\iff & [B_W]A &= \begin{bmatrix}
					T(\V{v_1}) & \cdots & T(\V{v_n})
					\end{bmatrix} \\
		&\iff & A &= \inv{[B_W]}\begin{bmatrix}
					T(\V{v_1}) & \cdots & T(\V{v_n})
					\end{bmatrix}. &\sidecomment{} \\
		\end{align*}		
		The matrix $A$ is referred to as the \textbf{matrix of $T$ with respect to the bases $B_V$ and $B_W$} and conforms to,
		\[ A\x_{B_V} = \b_{B_W}. \]
		This can be seen as,
		\begin{align*}
		&& A\x_{B_V} &= \inv{[B_W]}\begin{bmatrix}
						T(\V{v_1}) & \cdots & T(\V{v_n})
						\end{bmatrix}\x_{B_V} \\
		&\iff & A\x_{B_V} &= \inv{[B_W]}\b_{B_V} &\sidecomment{} \\
		&\iff & A\x_{B_V} &= \b_{B_W} &\sidecomment{} \\
		\end{align*}
		so that ${ \x_{B_V} }$ --- the coordinate vector with respect to $B_V$ --- is first transformed by applying the coordinates to the transformed version of the basis $B_V$ and then these coordinates are converted to $B_W$ coordinates by left multiplication by $\inv{[B_W]}$.\\\\		
		\note{If we chose different bases for the spaces we would get a different matrix. If the bases are the standard bases then the matrix is the standard matrix for the transformation.}
		
		\medskip
		\subsubsection{Examples of Linear Transform Matrices \wrt Bases}
		\begin{exe}
			\ex{Let ${ T: \R{2} \longmapsto \R{2} }$ be a linear transform defined (against the standard basis) by,
				\[ T\left(\begin{bmatrix}1 \\ 0\end{bmatrix}\right) = \begin{bmatrix}2 \\ 3\end{bmatrix} \eqand
					T\left(\begin{bmatrix}0 \\ 1\end{bmatrix}\right) = \begin{bmatrix}3 \\ 2\end{bmatrix}.
				\]
				Then, if we define the matrix of $T$ with respect to the standard basis only then we have,
				\[ A = \begin{bmatrix}
						T\left(\begin{bmatrix}1 \\ 0\end{bmatrix}\right) & T\left(\begin{bmatrix}0 \\ 1\end{bmatrix}\right)
						\end{bmatrix} =
						\begin{bmatrix}
						2 & 3\\
						3 & 2
						\end{bmatrix}
				\]
				and, if we define a vector ${ \x = \langle 1,1 \rangle }$ against the standard basis we can see that,
				\[ T(\x) = 1 \cdot T\left(\begin{bmatrix}1 \\ 0\end{bmatrix}\right) + 1 \cdot T\left(\begin{bmatrix}0 \\ 1\end{bmatrix}\right) =
				   A\x =
				   \begin{bmatrix}
				   2 & 3\\
				   3 & 2
				   \end{bmatrix}
				   \begin{bmatrix}
				   1\\
				   1
				   \end{bmatrix} =
				   \begin{bmatrix}
				   5\\
				   5
				   \end{bmatrix}.
				\]
				If we now define $B$, an alternative basis of $\R{2}$, as
				\[ B = \left\{
							\begin{bmatrix}3\\0\end{bmatrix},
							\begin{bmatrix}0\\2\end{bmatrix}
					   \right\}
				\]
				then we have,
				\[
					[B] = 	\begin{bmatrix}
							3 & 0\\
							0 & 2
							\end{bmatrix}
					\eqand
					\inv{[B]} = \frac{1}{6}
								\begin{bmatrix}
								2 & 0\\
								0 & 3
								\end{bmatrix}
				 \]
				and the linear transform matrix \textbf{of coordinate vectors \wrt the basis $B$} is defined as,
				\[ A = \begin{bmatrix}
						T\left(\begin{bmatrix}3 \\ 0\end{bmatrix}\right) & T\left(\begin{bmatrix}0 \\ 2\end{bmatrix}\right)
						\end{bmatrix} =
						\begin{bmatrix}
						6 & 6\\
						9 & 4
						\end{bmatrix}.
				\]
				Now \textit{this} matrix $A$ expects coordinate vectors \wrt $B$ and so if we convert $\x$ to basis $B$ as follows,
				\[ \x_B = \inv{[B]}\x = \frac{1}{6}\begin{bmatrix}
													2 & 0\\
													0 & 3
													\end{bmatrix} 
													\begin{bmatrix}
													1\\
													1
													\end{bmatrix} =
													\begin{bmatrix}
													1/3\\
													1/2
													\end{bmatrix}									
				\]
				then we find that,
				\[ T(\x) = A\x_B = \begin{bmatrix}
									6 & 6\\
									9 & 4
									\end{bmatrix}
									\begin{bmatrix}
									1/3\\
									1/2
									\end{bmatrix} =
									\begin{bmatrix}
									6/3 + 6/2\\
									9/3 + 4/2
									\end{bmatrix} =
									\begin{bmatrix}
									5\\
									5
									\end{bmatrix}.
				\]
				\\If we were to define another basis of $\R{2}$ called $B'$ and construct the matrix of $T$ with respect to $B$ and $B'$ then the matrix $A$ would become,
				\[ A = \inv{[B']}\begin{bmatrix}
						T\left(\begin{bmatrix}3 \\ 0\end{bmatrix}\right) & T\left(\begin{bmatrix}0 \\ 2\end{bmatrix}\right)
						\end{bmatrix}
				\]
				and to get the result in standard coordinates we would need to apply the result to the basis vectors of $B'$,
				\[ T(\x) = [B']A\x_B. \]
				
				In the case where we want the result to be in the same basis as the argument vector $\x_B$ we still need to modify the matrix $A$. In this case $A$ becomes,
				\[ A = \inv{[B]}\begin{bmatrix}
						T\left(\begin{bmatrix}3 \\ 0\end{bmatrix}\right) & T\left(\begin{bmatrix}0 \\ 2\end{bmatrix}\right)
						\end{bmatrix}
				\]
				This $A$ still expects $\x$ to be in $B$ coordinates but outputs a result defined in $B$ coordinates rather than standard coordinates.
				\[ A\x_B = \frac{1}{6}\begin{bmatrix}
							2 & 0\\
							0 & 3
							\end{bmatrix}
							\begin{bmatrix}
							6 & 6\\
							9 & 4
							\end{bmatrix}
							\x_B =
							\begin{bmatrix}
							2 & 2\\
							9/2 & 2
							\end{bmatrix}
							\begin{bmatrix}
							1/3\\
							1/2
							\end{bmatrix} =
							\begin{bmatrix}
							5/3\\
							5/2
							\end{bmatrix}.
				\]
				
				So, to get the result in standard coordinates we need to apply the result to the basis vectors of $B'$,
				\[ T(\x) = [B](A\x_B) =
							\begin{bmatrix}
							3 & 0\\
							0 & 2
							\end{bmatrix}
				 			\begin{bmatrix}
				 			5/3\\
				 			5/2
				 			\end{bmatrix} =
				 			\begin{bmatrix}
				 			5\\
				 			5
				 			\end{bmatrix}.
				\]
			}
		\end{exe}
	
		\bigskip
		\labeledProposition{Let $A$ be the matrix of a linear transformation ${ T: V \longmapsto W }$ with respect to the bases ${ B_V, B_W }$ of dimension $n$ and $m$ respectively. The matrices $A'$ which represent $T$ with respect to other bases are those of the form,
			\[ A' = QA\inv{P} \]
			where ${ Q \in GL_m(\F{}), P \in GL_n(\F{}) }$.
		}{matrix_wrt_to_2_bases_wrt_to_other_bases}
		\begin{proof}
			Let ${ B_V' = \{\v_1',\dots,\v_n'\}, B_W' = \{\w_1',\dots,\w_n'\} }$ be alternative bases with respect to which we want to find $A'$, the matrix of $T$. Also let,
			\[ B_V = B_V'P \eqand B_W = B_W'Q. \]
			Then for ${ \v_i' \in B_V',\, T(\v') \in span\,B_W' }$ so there exists a matrix $A'$ such that, using the notation $T(B_V)$ to indicate the image of the set $B_V$ under $T$ and $[B_V]$ for the matrix whose columns are the elements of $B_V$,
			\begin{align*}
			&& \begin{bmatrix}
				\w_1' & \cdots & \w_m'
				\end{bmatrix}
				A'&=
				\begin{bmatrix}
				T(\v_1') & \cdots & T(\v_n')
				\end{bmatrix}  \\
			&\iff & [B_W']A' &= [T(B_V')] &\sidecomment{} \\
			&\iff & A' &= \inv{[B_W']}[T(B_V')] &\sidecomment{} \\
			&\iff & A' &= \inv{[B_W']}[T(B_V\inv{P})] &\sidecomment{} \\
			&\iff & A' &= Q\inv{[B_W]}[T(B_V)]\inv{P} &\sidecomment{P is coefficient matrix} \\
			&\iff & A' &= QA\inv{P}. &\qedhere \\
			\end{align*}
		\end{proof}
	
		\bigskip
		\subsubsection{Simplification of the Matrix of a Transformation}
		\medskip
		\labeledProposition{Let ${ T: V \longmapsto W }$ be a linear transformation of rank $r$. Bases ${ B_V,B_W }$ may be chosen so that the matrix of $T$ takes the form,
			\[ A = \begin{bmatrix}
					I_r & \vdots \\
					\cdots & 0 \\
					\end{bmatrix}.
			\]
		}{simplification_of_linear_transform_matrix}
		\begin{proof}
			Let ${ U = \{\u_1,\dots,\u_k\} }$ be a basis of the kernel of $T$ where ${ k = dim(ker\,T) }$. Then $U$ may be extended to a basis of $V$ (\autoref{prop:lin_ind_set_can_be_extended_to_basis}),
			\[ B_V = \{\v_1,\dots,\v_r,\u_1,\dots,\u_k\}. \]
			Then, let ${ T(\v_i) = \w_i }$ so that,
			\[ [T(B_V)] = \begin{bmatrix}
						\w_1 & \cdots & \w_r & \0 & \cdots & \0
						\end{bmatrix}. 
			\]
			As shown in \autoref{theo:linear_map_dimension_formula}, ${ \{ \w_1,\dots,\w_r \} }$ is a basis of the image of $T$ and can also be extended to a basis of $W$,
			\[ B_W = \{ \w_1,\dots,\w_r, \x_1,\dots, \x_{m-r} \}. \]
			So, the matrix $A$ of $T$ with respect to the bases ${ B_V,B_W }$ satisfies,
			\begin{align*}
			&& A &= \inv{[B_W]}[T(B_V)] \\
			&\iff & [B_W]A &= [T(B_V)] \\
			&\iff & \begin{bmatrix}\w_1 & \dots & \w_r & \x_1 & \dots & \x_{m-r}\end{bmatrix}A &= 
					\begin{bmatrix}\w_1 & \dots & \w_r & \0 & \dots & \0\end{bmatrix}. &\sidecomment{}
			\end{align*}
			If we look at the components of the matrices we see,
			\[  
				\begin{bmatrix}
				w_{11} \dots & w_{1r} & x_{11} \dots & x_{1(m-r)} \\
				\vdots &  &  &  \\
				w_{r1} \dots & w_{rr} & x_{r1} \dots & x_{r(m-r)} \\
				\vdots &  &  &  \\
				w_{m1} \dots & w_{mr} & x_{m1} \dots & x_{m(m-r)} \\
				\end{bmatrix}
				\begin{bmatrix}
				a_{11} & \dots & a_{1n} \\
				\vdots &  &  \\
				a_{r1} & \dots & a_{rn} \\
				\vdots &  &  \\
				a_{m1} & \dots & a_{mn} \\
				\end{bmatrix}
				= 
				\begin{bmatrix}
				w_{11} \dots & w_{1r} & 0 \dots & 0 \\
				\vdots &  &  &  \\
				w_{r1} \dots & w_{rr} & 0 \dots & 0 \\
				\vdots &  &  &  \\
				w_{m1} \dots & w_{mr} & 0 \dots & 0 \\
				\end{bmatrix}
			\]
			which makes it clear that $A$ has the form,
			\[ A =
				\begin{bmatrix}
				1 & \dots & 0 & 0 & \dots & 0 \\
				\vdots & \ddots & \vdots & &  \\
				0 & \dots & 1 & 0 & \dots & 0 \\
				\vdots &  & & &  \\
				0 & \dots & 0 & 0 & \dots & 0 \\
				\end{bmatrix}
			\]
			as required.
		\end{proof}
		\begin{corollary}
			By \autoref{prop:left_multiplication_by_matrix_are_lin_transforms}, left multiplication by any matrix is a linear transformation and so is equivalent to left multiplication by a matrix of the form
			\[
			\begin{bmatrix}
				I_r & \vdots \\
				\cdots & 0 \\
				\end{bmatrix}
			\]
			but with reference to different coordinate systems.
		\end{corollary}
	
		\bigskip
		\subsubsection{Example of Simplification by Selecting Bases}
		\begin{exe}
			\ex{Continuing the example \ref{ex:gaussian-elimination} of Gaussian Elimination for row reducing a matrix we have a matrix
			\[ A = \begin{bmatrix}
						3 & 1 & 1  \\
						0 & 2 & -4 \\
						3 & 2 & -1
					\end{bmatrix}
			\]
			whose rref form is
			\[ A' = \begin{bmatrix}
						1 & 0 & 1  \\
						0 & 1 & -2 \\
						0 & 0 & 0
					\end{bmatrix}.
			\]
			The rref tells us that the first two columns of $A$ are a basis of the image and the kernel is 
			\[ 	c \begin{bmatrix}-1\\2\\1\end{bmatrix} \hspace{20pt} c \in \R{}. \]	
			
			We can use this information to form a matrix ${ A_{PQ} }$ --- which is the matrix $A$ expressed with respect to the bases $P$ and $Q$ --- such that $A_{PQ}$ is maximally simplified. Following the procedure used in the proof of \autoref{prop:simplification_of_linear_transform_matrix}, we begin by finding a basis of the domain space by extending a basis of the kernel.\\
			
			Since the kernel is one-dimensional and the domain is three-dimensional, we can extend the basis of the kernel given above to a basis of the domain by adding two of the three standard basis vectors. So, the chosen basis of the domain is
			\[ P = \left\{ \begin{bmatrix}1\\0\\0\end{bmatrix}, \begin{bmatrix}0\\1\\0\end{bmatrix}, \begin{bmatrix}-1\\2\\1\end{bmatrix} \right\}. \]
			
			Again following the procedure from the same proof, we next form a basis of the codomain space that extends a basis of the image which, in this case, is the first two columns of the matrix $A$. So, we can choose the basis,
			\[ Q = \left\{ \begin{bmatrix}3\\0\\3\end{bmatrix}, \begin{bmatrix}1\\2\\2\end{bmatrix}, \begin{bmatrix}0\\0\\1\end{bmatrix} \right\}. \]
			\note{We could have chosen anything for the final column just so long as it is linearly independent of the first two columns.}
			
			Then,
			\begin{align*}
			A_{PQ} &= \inv{Q} A P \\
			&= 	\inv{\begin{bmatrix}
					3 & 1 & 0\\
					0 & 2 & 0\\
					3 & 2 & 1
				\end{bmatrix}}
				\begin{bmatrix}
					3 & 1 & 1  \\
					0 & 2 & -4 \\
					3 & 2 & -1
				\end{bmatrix}
				\begin{bmatrix}
					1 & 0 & -1  \\
					0 & 1 &  2 \\
					0 & 0 &  1
				\end{bmatrix} &\sidecomment{} \\[8pt]
			&= 	\frac{1}{6}
				\begin{bmatrix}
					2  & -1 & 0\\
					0  &  3 & 0\\
					-6 & -3 & 6
				\end{bmatrix}
				\begin{bmatrix}
					3 & 1 & 1  \\
					0 & 2 & -4 \\
					3 & 2 & -1
				\end{bmatrix}
				\begin{bmatrix}
					1 & 0 & -1  \\
					0 & 1 &  2 \\
					0 & 0 &  1
				\end{bmatrix} &\sidecomment{} \\[8pt]
			&= 	\frac{1}{6}
				\begin{bmatrix}
				2  & -1 & 0\\
				0  &  3 & 0\\
				-6 & -3 & 6
				\end{bmatrix}
				\begin{bmatrix}
				3 & 1 & 0 \\
				0 & 2 & 0 \\
				3 & 2 & 0
				\end{bmatrix} &\sidecomment{} \\[8pt]
			&= 	\begin{bmatrix}
					1 & 0 & 0\\
					0 & 1 & 0\\
					0 & 0 & 0
				\end{bmatrix}.
			\end{align*}
			}\label{ex:matrix-simplification-by-selecting-bases}
		\end{exe}
		
	}



% ---------------------


	\pagebreak
	\searchableSubsection{Linear Operators and Eigenvectors}{linear algebra}{
		
		\bigskip
		\boxeddefinition{A \textbf{linear operator} is a linear transformation from a space to itself. That's to say, the domain and codomain of the transformation are the same space and considered with respect to the same basis.}
		
		\medskip
		\boxeddefinition{A linear operator is called \textbf{singular} if it does not have an inverse and \textbf{nonsingular} if it has an inverse.}
		
		\bigskip
		\boxeddefinition{If $T$ is a linear operator on a vector space (finite or infinite) over a field $\F{}$ and $p(x)$ is a polynomial over $\F{}$, 
			\[ p(x) = a_0 + a_1 x + \cdots + a_n x^n \]
		then we can define the \textbf{polynomial of the linear operator} $T$ as
			\[ p(T) = a_0 I + a_1 T + \cdots + a_n T^n. \]
		Since any power of the operator $T^n$ is also a linear operator and the polynomial is a linear combination of these, by \autoref{prop:linear-combination-of-linear-maps-is-linear-map}, \textbf{the polynomial $p(T)$ is also a linear operator}.\\
		
		Similarly for a square matrix $A$,
			\[ p(A) = a_0 I + a_1 A + \cdots + a_n A^n. \]
		}\label{defn:polynomials-of-linear-operators}
		
		\medskip
		\note{To show the validity of polynomials of linear operators:\\\\ Let $T$ be an arbitrary linear map over a vector space $V$ defined over a field $\F{}$. Define ${ T + a }$ as the linear map ${ T + aI }$ and similarly ${ T - b = T - bI }$. Lastly, define ${ (T + a)(T - b) }$ as the composition of the linear maps,
			\[ (T + aI) \circ (T - bI). \]
		Then,
		\[\begin{aligned}
			(T + a)(T - b) &= T(T - b) + a(T - b) &\sidecomment{pointwise definition} \\
			&= TT - Tb + aT - ab &\sidecomment{linearity} \\
			&= T^2 + (a-b)T - ab. \\
		\end{aligned}\]
		In fact, polynomials over a field such as the reals can be viewed as linear combinations of a linear map -- the simplest linear map: scalar multiplication.\\\\ If we regard the real variable $x$ as the function "multiply by $x$ for any $x$ in the field" ${ f_1(t) = xt }$, then ${ x + a }$ is the linear combination of this function with the function "multiply by the constant $a$" (defined pointwise) which results in: ${ f_2(t) = (x + a)t = xt + at }$. Then 
		\[\begin{aligned}
			(x + a)^2 &= (x + a)(x + a) \\
			&= (x + a)f_2(t) \\
			&= (x + a)(xt + at) \\
			&= x^2 t + axt + axt + a^2 t \\
			&= x^2 t + 2axt + a^2 t \\
			&= (x^2 + 2ax + a^2)t.
		\end{aligned}\]
		A full treatment of this topic requires Modules (\href{https://en.wikipedia.org/wiki/Module_(mathematics)}{wikipedia}).
		}
		
		\bigskip
		\labeledProposition{A linear operator ${ T: V \longmapsto V }$ is bijective iff it has a trivial kernel.}{injective_lin_operator_is_bijective}
		\begin{proof}
			By \autoref{theo:homomorphism_injective_iff_kernel_is_trivial} of homomorphisms, if $T$ has a trivial kernel then it is injective which implies that ${ \cardinality{im\,T} = \cardinality{V} }$ so that it is also surjective (because the domain is equal to the codomain). Therefore, it is bijective.\\
			Conversely, if it is bijective then it is injective and so it has a trivial kernel.
		\end{proof}
		\begin{corollary}
			A linear operator ${ T: V \longmapsto V }$ is isomorphic iff it has a trivial kernel.
		\end{corollary}
		\begin{corollary}
			A linear operator is nonsingular iff it has a trivial kernel.
		\end{corollary}
	
		\medskip\note{Note that these are not true of linear transformations in general because, for a transformation between different vector spaces of different dimensions, injectivity does not imply bijectivity and invertibility.
		}
	
		\medskip
		\labeledProposition{The following conditions on a linear operator ${ T: V \longmapsto V }$ on a finite-dimensional vector space are equivalent:
			\begin{enumerate}[label=(\roman*)]
				\item{${ ker\,T > 0 }$.}
				\item{${ im\,T < V }$.}
				\item{If $A$ is the matrix of the operator with respect to an arbitrary basis, then ${ det\,A = 0 }$.}
				\item{$T$ is singular}
			\end{enumerate}
		}{properties_of_non_bijective_lin_operator}
		\begin{proof}
			The first two of these properties follow from the dimension formula for finite-dimensional vector spaces. The third follows since an operator with a non-trivial kernel is noninvertible (\autoref{prop:injective_lin_operator_is_bijective}) and so its matrix will also be noninvertible; noninvertible matrices have determinant 0. The last property follows from the definition of singular and the fact that $T$ is noninvertible.
		\end{proof}
	
		\medskip\note{Again it's worth noting the contrast with transformations in general: for a transformation whose domain vector space is lower dimension than its codomain, the transformation may be injective but not surjective (kernel is trivial but ${ im\,T < V }$), and conversely, if the domain is higher dimension than the codomain then the transformation may be surjective while not injective (${ ker\,T > 0 }$ but image covers codomain). Furthermore, the third condition relating to the determinant, only applies to operators because the determinant is a property of square matrices.
		}
		
		\note{Note that the first two properties do not hold for infinite-dimensional vector spaces. For example, let ${ V = \R{\infty} }$ be the space of sequences ${ a_1,a_2,\dots }$. Then the "shift operator" is defined as,
			\[ T(a_1,a_2,\dots) = (0,a_1,a_2,\dots). \]
			This is a linear operator because,
			\begin{align*}
			\alpha T(a_1,a_2,\dots) + \beta T(b_1,b_2,\dots) &= \alpha (0,a_1,a_2,\dots) + \beta (0,b_1,b_2,\dots) \\
			&= (0, \alpha a_1, \alpha a_2,\dots) + (0, \beta b_1, \beta b_2,\dots) &\sidecomment{} \\
			&= (0, \alpha a_1 + \beta b_1, \alpha a_2 + \beta b_2,\dots) &\sidecomment{} \\
			&= T(\alpha a_1 + \beta b_1, \alpha a_2 + \beta b_2,\dots). &\sidecomment{} \\
			\end{align*}
			However, while clearly for this operator we have ${ im\,T < V }$, nevertheless the kernel of this operator is the trivial kernel ${ \{\0\} }$. This just shows further that the dimension formula (\autoref{theo:linear_map_dimension_formula}) for finite-dimensional vector spaces does not apply for infinite-dimensional spaces.\\
			The explanation of this is that infinite sets can have proper subsets that have equal cardinality. So we can have an image whose dimensions are a proper subset of the dimensions of the space but the image, nevertheless, has the same dimensionality as the space. This can only happen with infinite-dimensional spaces.
		}
	
		
		\bigskip
		\labeledProposition{Let $A$ be the matrix of a linear operator $T$ with respect to the basis $B$ of dimension $n$. The matrices $A'$ which represent $T$ with respect to other bases are those of the form,
			\[ A' = PA\inv{P} \]
			where ${ P \in GL_n(\F{}) }$.
		}{operator_matrix_wrt_to_1_base_wrt_to_other_bases}
		\begin{proof}
			A linear operator is a specialization of a linear transformation where the domain and codomain are the same set so we can use the definition (\ref{def:matrix_of_lin_transform_wrt_bases}) of the matrix of a linear transformation \wrt to two different bases and simply set the two bases to the same set $B$. This produces,
			\[ T(\x) = A\x_B = \b_B \]
			so that,
			\[ [B]A = [T(\b_1) \cdots T(\b_n)] \iff A = \inv{[B]}[T(\b_1) \cdots T(\b_n)]. \]		
			Now let there be another basis $B'$ related to $B$ by,
			\[ [B] = [B']P \iff [B]\inv{P} = [B'], \]
			\[ P\x_B = \x_{B'}. \]
			To define a matrix $A'$ that performs the same linear transformation as $A$ \wrt to the basis $B'$ we need,
			\[ T(\x) = A'\x_{B'} = \b_{B'} \]
			and
			\[ [B']A' = [T(\b_1') \cdots T(\b_n')] \iff A' = \inv{[B']}[T(\b_1') \cdots T(\b_n')]. \]
			If we have the vectors of $B'$ encoded in $B$-coordinates then we can use the transformed version of the basis $B$ to produce the transformed version of the basis $B'$,
			\[ [T(\b_1') \cdots T(\b_n')] = [T(\b_1) \cdots T(\b_n)]\inv{[B]}[B'] = [T(\b_1) \cdots T(\b_n)]\inv{P}. \]
			If we now note that,
			\[ A = \inv{[B]}[T(\b_1) \cdots T(\b_n)] \eqand A' = \inv{[B']}[T(\b_1') \cdots T(\b_n')]  \]
			then,
			\begin{align*}
			&& A' &= \inv{[B']}[T(\b_1) \cdots T(\b_n)]\inv{P} \\
			&\iff & A' &= \inv{[B']}[B]A\inv{P} &\sidecomment{} \\
			&\iff & A' &= PA\inv{P}. &\sidecomment{} \qedhere
			\end{align*}
		\end{proof}
	
		\bigskip
		\subsubsection{Similar Matrices}\label{sssection:similar-matrices}
		\bigskip
		\boxeddefinition{If two matrices ${ A,A' }$ are related by,
			\[ A' = PA\inv{P} \]
			for some ${ P \in GL_n(\F{}) }$ then they are known as \textbf{similar} matrices or \textbf{conjugates}.
		}
		\medskip
		\note{Note:
			\begin{itemize}
				\item{Similar matrices represent the same linear transformation expressed with respect to different bases.}
				\item{In general, linear maps are not invertible so the matrix $A$ is not necessarily a member of a group. So, this use of the term "conjugation" only equates to the term in group theory within the general linear group $GL_n(\F{})$.}
				\item{Similar matrices have the same determinant which means that they expand or contract the space by the same amount. This is to be expected as they represent the same linear transformation defined against different bases.}
			\end{itemize}		
		}
		
		\medskip
		\subsubsection{Intuition of Similar Matrices}
		\[ A' = PA\inv{P} \iff A'P = PA \]
		$P$ is the change of basis matrix such that ${ \x_{B'} = P\x_B }$. So $P$ is $B_{B'}$ the basis vectors of $B$ \wrt to $B'$. Another way of looking at it: ${ P = \inv{[B']}[B] }$ so it decodes coordinates in $B$-coords to standard coordinates and then encodes them into $B'$-coords. Meanwhile $A$ transforms the basis vectors of $B$ and then encodes the result in $B$-coords so the schematic of $A$ is ${ A = \inv{[B]}[T(B)] }$.\\
		So 
		\[ PA = P\inv{[B]}[T(B)] = \inv{[B']}[B]\inv{[B]}[T(B)] = \inv{[B']}[T(B)]. \]
		By similar reasoning the schematic of $A'$ is ${ A' = \inv{[B']}[T(B')] }$. But also we have,
		\[ A' = PA\inv{P} = (\inv{[B']}[B])(\inv{[B]}[T(B)])(\inv{[B]}[B']) = \inv{[B']}[T(B)]\inv{[B]}[B'] \]
		so that,
		\begin{align*}
		&& \inv{[B']}[T(B')] &= \inv{[B']}[T(B)]\inv{[B]}[B'] \\
		&\iff & [T(B')] &= [T(B)]\inv{[B]}[B']. &\sidecomment{} \\
		\end{align*}
		What this last result is saying is that if we take the basis vectors of $B'$ and encode them in $B$-coordinates and then apply the result to the transformed basis of $B$ then the result is the transformed basis of $B'$ defined in standard coordinates.\\\\
		So, we have,
		\[ A'P = PA = \inv{[B']}[T(B)] \]
		\[ \inv{P}A' = A\inv{P} = \inv{[B]}[T(B)]\inv{[B]}[B'] = \inv{[B]}[T(B')]. \]
		
		
		\bigskip
		\begin{tcolorbox}[breakable,enhanced jigsaw,colframe=white,colback=white,boxrule=0pt,arc=0pt,left=0pt,right=0pt,top=0pt,bottom=0pt]
			\labeledTheorem{If $A$ is similar to a matrix ${ \tilde{A} = \inv{P}AP }$ then 
				\[ A^n = (P \tilde{A} \inv{P})^n = P \tilde{A}^n \inv{P}. \]
			}{exponentiation-of-similar-matrices}
			\begin{proof}
				Remembering that matrix multiplication is not, in general, commutative,
				\begin{align*}
					&& A^n &= (P \tilde{A} \inv{P})(P \tilde{A} \inv{P})\cdots(P \tilde{A} \inv{P}) \\
					&&  &= P \tilde{A} (\inv{P})(P) \tilde{A} (\inv{P}P) \cdots (\inv{P}P) \tilde{A} \inv{P} \\
					&&  &= P \tilde{A}^n \inv{P}. \qedhere
				\end{align*}
			\end{proof}
		\end{tcolorbox}
		
		\bigskip
		\labeledProposition{Similarity of matrices is an equivalence relation.}{matrix_similarity_is_equiv_relation}
		\begin{proof}
			For any ${ M,N \in GL_n(\F{}) }$, similarity is an equivalence relation because of the following properties.
			
			\paragraph{\small{Reflexivity:}}
			\begin{align*}
			& N = I^{-1}NI \\
			\therefore \; & N \sim N
			\end{align*}
			
			\paragraph{\small{Symmetry:}}
			\begin{align*}
			& & N &= P^{-1}MP \\
			&\iff & NP^{-1} &= P^{-1}M(PP^{-1}) \\
			&\iff & NP^{-1} &= P^{-1}M \\
			&\iff & PNP^{-1} &= (PP^{-1})M \\
			&\iff & PNP^{-1} &= M \\
			&\iff & R^{-1}NR &= M,\;\; R \in X\\
			&\;\;\therefore & N \sim M &\iff M \sim N
			\end{align*}
			
			\paragraph{\small{Transitivity:}}
			\begin{align*}
			& & N &= P^{-1}MP,\;\; M = Q^{-1}AQ \\
			&\implies & N &= P^{-1}(Q^{-1}AQ)P \\
			&\iff & N &= (P^{-1}Q^{-1})A(QP) \\
			&\iff & N &= R^{-1}AR,\;\; R \in X\\
			&\;\;\therefore & (N \sim M) & \wedge (M \sim Q) \iff (N \sim Q)
			\end{align*}
		\end{proof}
	
		\bigskip
		\labeledProposition{Similar matrices have the same determinant.}{similar-matrices-have-same-determinant}
		\begin{proof}
			Let ${ A' = PA\inv{P} }$ where $P$ is a change of basis matrix. Then,
			\begin{align*}
			\det{A'} &= \det{PA\inv{P}}\\
			 &= (det{P}) \cdot (\det{A}) \cdot (\det{\inv{P}})\\
			 &= (\det{P}) \cdot (\det{A}) \cdot (1/\det{P})\\
			 &= \det{A}. \qedhere
			\end{align*}
		\end{proof}
		
		
		
		\bigskip
		\subsubsection{Invariant Subspaces}
		\medskip
		\boxeddefinition{Let ${ T: V \longmapsto V }$ be a linear operator on a vector space. A subspace $W$ of $V$ is called an \textbf{invariant subspace} or a \textbf{$T$-invariant subspace} if it is carried to itself by the operator,
			\[ TW \subseteq W. \]
			In other words, $W$ is $T$-invariant if ${ T(\w) \in W }$ for all ${ \w \in W }$. In this case, $T$ may be referred to as defining an operator on $W$ that is the \textbf{restriction of $T$ to $W$}.
		}
		\notation{Denote the restriction of $T$ to $W$ by $T_w$.}
		
		\bigskip
		\begin{tcolorbox}[breakable,enhanced jigsaw,colframe=white,colback=white,boxrule=0pt,arc=0pt,left=0pt,right=0pt,top=0pt,bottom=0pt]
			\labeledProposition{If $T$ is a linear operator over a vector space $V$ with range $R(T)$ and kernel/nullspace $N(T)$ then the following subspaces of $V$ are all $T$-invariant:
				\begin{enumerate}[label=(\roman*)]
					\item{$V$}
					\item{$ \{ \0 \} $}
					\item{$R(T)$}
					\item{$N(T)$}
				\end{enumerate}
			}{range-nullspace-origin-and-whole-space-are-all-T-invariant}
			\begin{proof}$ $
				\begin{enumerate}[label=(\roman*)]
					\item{$V$}\\
						Since $T$ is a linear operator, ${ T: V \longmapsto V }$, we have, for all ${ \v \in V }$, ${ T\v \in V }$.
					\item{$ \{ \0 \} $}\\
						Any linear transformation, by homogeneity (\autoref{coro:linear-maps-map-the-origin-to-itself}), maps the origin to itself.
					\item{$R(T)$}\\
						By the definition of the range of $T$ we must have for all ${ \v \in V }$, ${ T\v \in R(T) }$ and since $T$ is a linear operator, ${ T: V \longmapsto V }$, 
						\[ R(T) \subseteq V \implies \forall \V{r} \in R(T), \; T\V{r} \in R(T). \]
					\item{$N(T)$}\\
						By the definition of the nullspace/kernel of $T$ we have: for all ${ \v \in N(T) }$, ${ T\v = \0 }$. Since the origin is a member of all vector spaces, this implies that also for all ${ \v \in N(T) }$, ${ T\v \in N(T) }$.
				\end{enumerate}
			\end{proof}
		\end{tcolorbox}
	
		\bigskip		
		\labeledProposition{Let $T$ be an operator over a vector space $V$ and $W$ a subspace of $V$ such that ${ V = R(T) \oplus W }$ where $R(T)$ denotes the range of $T$. If $W$ is $T$-invariant then ${ W \subseteq N(T) }$ where $N(T)$ denotes the nullspace of $T$. Furthermore, if $V$ is finite-dimensional then ${ W = N(T) }$.
		}{invariant-subspace-linearly-independent-from-range-is-in-nullspace}
		\begin{proof}
			Let ${ U = R(T) }$ so that ${ V = U \oplus W }$ and for all ${ \v \in V }$, there exists ${ \u \in U, \w \in W }$ such that
			\[ \v = \u + \w \eqand \u + \w = \0 \implies \u = \w = \0. \]
			Since $W$ is $T$-invariant, for any ${ \w \in W }$
			\[ T\w \in W. \]
			But ${ U = R(T) }$ is the range of $T$, so it must also be that
			\[ T\w \in U. \]
			Therefore, there exists some ${ \w_1 \in W, \, \u_1 \in U }$ such that
			\[ T\w = \w_1 = \u_1. \]
			Since,
			\[ \w_1 = \u_1 \iff \u_1 - \w_1 = \0 \implies \w_1 = \u_1 = \0 \]
			we obtain the result that, for all ${ \w \in W }$,
			\[ T\w = \0. \]
			This implies that ${ W \subseteq N(T) }$.\\
			
			Furthermore, if $V$ is finite-dimensional, the dimension formula for linear operators over finite-dimensional vector spaces (\autoref{theo:linear_map_dimension_formula}) tells us that,
			\[ \dim R(T) + \dim N(T) = \dim V \iff \dim N(T) = \dim V - \dim R(T).  \]
			So we have,
			\[ \dim N(T) = \dim V - \dim U \]
			but also
			\[ V = U \oplus W \implies \dim V = \dim U + \dim W \iff \dim V - \dim U = \dim W \]
			which gives us
			\[ \dim W = \dim N(T). \]
			We can therefore deduce,
			\[ [\, W \subseteq N(T) \,] \land [\, \dim W = \dim N(T) \,] \implies W = N(T). \]
		\end{proof}
	
		\biggerskip
		\subsubsubsection{Examples of Invariant Subspaces}
		\begin{exe}
			\ex{Let ${ T: \R{3} \longmapsto \R{3} }$ be defined by,
				\[ T(a, b, c) = (a + b, \, b + c, \, 0). \]
				Then the $xy$-plane ${ \setc{(x,y,0)}{x,y \in \R{}} }$ and the $x$-axis ${ \setc{(x,0,0)}{x \in \R{}} }$ are $T$-invariant subspaces of $\R{3}$.
			}
		\end{exe}
		
		\biggerskip\biggerskip
		\subsubsubsection{Invariant Subspaces as Matrix Blocks}
		Let ${ W \subseteq V }$ be a $T$-invariant subspace of $V$ with basis ${ B_W = \w_1,\dots,\w_k }$. Then, by \autoref{prop:lin_ind_set_can_be_extended_to_basis}, $B_W$ can be extended to a basis of $V$,
		\[ B_V = \{ \w_1,\dots,\w_k, \v_1,\dots,\v_{n-k} \}. \]
		If we look at the matrix $A$ of $T$ with respect to this basis $B_V$ we find a characteristic pattern.
		\[ A = \inv{[B_V]}[T(B_V)] = \inv{[B_V]}[T(\w_1) \cdots T(\w_k) \; T(\v_1) \cdots T(\v_{n-k})] \]
		with ${ T(\w_1), \dots, T(\w_k) \in W }$ so that,
		\[ T(\w_i) = \alpha_1\w_1 + \cdots + \alpha_k\w_k. \] 
		This means that when we express $T(\w_i)$ with respect to the basis $B_V$ the coordinate vectors will take the form,
		\[ (\alpha_1,\dots,\alpha_k,0,\dots,0)^T. \]
		As a result, the matrix $A$ will take the form,
		\[ A = \begin{bmatrix}
				C & D\\
				0 & E
				\end{bmatrix}\label{eq:invariant_space_block_decomposition}
		\]
		where $C$ is a ${ k \times k }$ matrix that represents the restriction of $T$ to $W$.\\
		
		\note{A description of a matrix of the form of the description of $A$ here is known as a \textbf{block decomposition}.} 
		
		On the other hand, if ${ V = W_1 \oplus W_2 }$ where \textit{both} $W_1$ \textit{and} $W_2$ are $T$-invariant subspaces then $A$ takes the form,
		\[ A = \begin{bmatrix}
				C & 0\\
				0 & E
				\end{bmatrix} 
		\]
		where, as before, $C$ is a ${ k \times k }$ matrix that represents the restriction of $T$ to $W_1$ but, this time, also $E$ is a ${ (n-k) \times (n-k) }$ matrix that represents the restriction of $T$ to $W_2$.
		
		\smallskip\note{Matrices with the form of the matrix,
			\[ \begin{bmatrix}
				C & 0\\
				0 & E
				\end{bmatrix} \]
				are known as \textbf{block diagonal matrices} or \textbf{diagonal block matrices}.
		}
	
		\bigskip
		\subsubsubsection{Cyclic Subspaces}
		\boxeddefinition{Let $T$ be a linear operator on a vector space $V$ and let ${ \v \neq \0 \in V }$. Then the subspace,
			\[ W = \operatorname{span} \{ \v, \, T\v, \, T^2\,\v, \dots \} \]
			is called the \textbf{$T$-cyclic subspace of $V$ generated by $\v$}.\\
			The $T$-cyclic subspace of $V$ generated by any element of $V$, is $T$-invariant by construction.
			\note{refer: cyclic subgroups in Group Theory \ref{sssection:cyclic-subgroups}}
		}
	
		\bigskip
		\labeledProposition{The $T$-cyclic subspace of $V$ generated by ${ \v \in V }$ is the smallest $T$-invariant subspace of $V$ containing $\v$.}{cyclic-subspace-generated-by-v-is-smallest-invariant-space-containing-v}
		\begin{proof}
			Let $W$ be the $T$-cyclic subspace of $V$ generated by ${ \v \in V }$.\\
			Clearly, from the definition, $W$ contains $\v$ so $W$ is a subspace containing $\v$.\\
			Furthermore, if $W$ is to be $T$-invariant then $T\v$ must also be in $W$. But then for $W$ to be $T$-invariant we also need that 
			\[ T(T\v) = T^2\,\v \]
			is in $W$ too. In fact, we need the closure of the composition of $T$ applied to $\v$.\\
			Therefore, $W$ is the smallest $T$-cyclic subspace of $V$ containing $\v$.
		\end{proof}
	
		\paragraph{Examples}
		\begin{exe}
			\ex{Let ${ T: \R{3} \longmapsto \R{3} }$ be defined by,
				\[ T(a, b, c) = (-b + c, \, a + c, \, 3c). \]
				To obtain the $T$-cyclic subspace generated by ${ \e{1} = (1,0,0) }$ we compose $T$ repeatedly:
				\[\begin{aligned}
					T\e{1} &= (0,1,0) = \e{2} \\
					T^2\,\e{1} &= T(0,1,0) = (-1,0,0) = -\e{1} \\
					T^3\,\e{1} &= T(-1,0,0) = (0,-1,0) = -\e{2} \\
					T^4\,\e{1} &= T(0,-1,0) = (1,0,0) = \e{1}
				\end{aligned}\]
				which produces the cycle of elements:
				\[ \e{1} \to \e{2} \to -\e{1} \to -\e{2} \to \e{1}. \]
				Therefore the subspace generated is
				\[ \operatorname{span} \{ \e{1}, \e{2} \} = \setc{(x,y,0)}{x,y \in \R{}}. \]
			}\label{ex:simple-T-cyclic-subspace}
			\ex{Let $T$ be the linear operator on $P(\R{})$, the polynomials with real coefficients, defined by differentiation,
				\[ T(p(x)) = \Dif_x p. \]
				Then the $T$-cyclic subspace generated by $x^2$ is ${ \operatorname{span} \{ x^2, 2x, 2 \} }$.
			}
		\end{exe}
		
		\biggerskip
		\subsubsection{Eigenvectors}
		\boxeddefinition{An \textbf{eigenvector} of a linear operator $T$ is a nonzero vector $\v$ with the property under $T$ that,
			\[ T(\v) = c\v \]
			for some constant ${ c \in \F{} }$. The constant $c$ is called an \textbf{eigenvalue}.\\\\
			Eigenvectors may also be referred to as \textbf{characteristic vectors} and eigenvalues as \textbf{characteristic values}.\\\\
			The \textbf{eigenspace} of an eigenvalue is the subspace formed by the eigenvectors associated with the eigenvalue and the zero vector.
		}
		\medskip\note{Note:
			\begin{itemize}
				\item{To have eigenvectors, $T$ must be an operator as the image of the eigenvector ${ \v \in V }$ under $T$,
					\[ T(\v) = c\v \in V \]
					 is, by definition, in the same space as the eigenvector itself (if there were a change of basis then we would not consider it to be the same vector). In fact, this is the key of the relationship between eigenvectors and invariant subspaces: The space spanned by eigenvectors of $T$ is $T$-invariant. The bases of $T$-invariant subspaces, however, need not be eigenvectors. For example, a rotation of a plane in a 3d space has the plane as a T-invariant subspace but the only eigenvector is the axis of rotation, perpendicular to the plane.
				 }
				\item{An eigenvector may not be $\0$ but an eigenvalue may be 0. As a consequence of this, every nonzero vector in the kernel is an eigenvector (with eigenvalue 0). As a consequence of this, if $W$ is a 2-dimensional $T$-invariant subspace, for example, and its image under $T$ is one-dimensional (a line inside the plane of $W$) then vectors in $W$ that are not in the line of the image will be in the kernel of $T$ and so also eigenvectors but with the eigenvalue 0. It is also worth noting that they are still considered to be parallel to their image under $T$ because, by convention, the zero vector $\0$ is considered to be parallel to all vectors. 
				}
				\item{If we speak of an eigenvector of a square matrix then we are referring to an eigenvector of \textbf{left multiplication} by the matrix, i.e. a nonzero vector $\x$ such that,
					\[ A\x = c\x. \]
					Clearly if $\x_B$ is the coordinate vector of ${ \v \in V }$ with respect to $B$ --- a basis of $V$ --- and $A$ is the matrix of $T$ with respect to the basis $B$, then
					\[ A\x_B = c\x_B \iff T(\v) = c\v. \]
					As a result, all similar matrices have the same eigenvalues.	
				}
			\end{itemize}
		 }
	 
	 	\bigskip
	 	\labeledTheorem{The eigenspace of an eigenvalue is a vector subspace.}{eigenspaces-are-vector-subspaces}
	 	\begin{proof}
	 		Let $S$ be the set of all eigenvectors of a linear operator $T$ corresponding to a particular eigenvalue $c$. Then the eigenspace is defined as,
	 		\[ E = S \cup \{\0\}. \]
	 		Then $E$ is a subspace because it contains the zero vector and
	 		\[ \v,\w \in E \implies T(\alpha\v + \beta\w) = \alpha(c\v) + \beta(c\w) = c(\alpha\v + \beta\w) \implies \alpha\v + \beta\w \in E. \qedhere \]
	 	\end{proof}
 		\begin{corollary}
 			Any linear combination of eigenvectors with eigenvalue $c$ is also an eigenvector with eigenvalue $c$.
 		\end{corollary}
 	

 		\bigskip
 		\labeledProposition{Eigenvectors corresponding to different eigenvalues are linearly independent. That's to say: Let ${ \v_1,\dots,\v_r \in V }$ be eigenvectors for a linear operator $T$, with distinct eigenvalues ${ c_1,\dots,c_r }$. Then the set ${ \{\v_1,\dots,\v_r\} }$ is linearly independent.}{distinct-eigenvals-means-independent-eigenvecs}
 		\begin{proof}
 			Assume for contradiction that there exists a linear relation between the set of eigenvectors,
 			\[ \alpha_1\v_1 + \cdots + \alpha_r\v_r = \0. \]
 			Linearity of $T$ gives us,
 			\[ T(\alpha_1\v_1 + \cdots + \alpha_r\v_r) = \alpha_1T(\v_1) + \cdots + \alpha_rT(\v_r) = T(\0) = \0 \]
 			while the eigenvector property gives us,
 			\[ \alpha_1T(\v_1) + \cdots + \alpha_rT(\v_r) = \alpha_1c_1\v_1 + \cdots + \alpha_rc_r\v_r \]
 			so we have the simultaneous equations,
 			\begin{align*}
 				&& \alpha_1\v_1 + \cdots + \alpha_r\v_r &= \0  \\
 				&& \alpha_1c_1\v_1 + \cdots + \alpha_rc_r\v_r &= \0.
 			\end{align*}
 			If we multiply the first equation by $c_r$ and subtract the second equation from it we get,
 			\[ \alpha_1(c_r - c_1)\v_1 + \cdots \alpha_{r-1}(c_r - c_{r-1})\v_{r-1} = \0. \]
 			Since all the eigenvalues are distinct, for ${ i \neq j,\; c_i - c_j \neq 0 }$ and the eigenvectors $\v_i$, by definition, are nonzero. So, this equation implies that either ${ \alpha_1,\dots,\alpha_{r-1} = 0 }$ or there is a linear relation between the vectors ${ \v_1,\dots,\v_{r-1} }$.\\
 			This dependence of the properties of the $r$-length list on the properties of the ${ (r-1) }$-length list signals that we can set up a proof by induction using the hypothesis that a $k$-length list is linearly independent.\\
 			If we use ${ k = 2 }$ as the base case, set up the linear relation and use the eigenvector property as before then this results in,
 			\[ \alpha_1(c_2 - c_1)\v_1 = \0. \]
 			As before, both ${ c_2 - c_1 }$ and $\v_1$ are nonzero so this implies that ${ \alpha_1 = 0 }$. This, in turn, implies that ${ \alpha_2 = 0 }$ also and so, the list of length ${ k = 2 }$ is linearly independent.\\
 			Then the induction step is to assume that the list of length ${ k = r-1 }$ is linearly independent and show that this implies that the list of length ${ k = r }$ is linearly independent. We have already shown that if we set up a linear relation on a list of eigenvectors of length ${ k = r }$ then the eigenvector property implies that,
 			\[ \alpha_1(c_r - c_1)\v_1 + \cdots \alpha_{r-1}(c_r - c_{r-1})\v_{r-1} = \0. \]
 			Now we can use the induction hypothesis to assert that ${ \v_1,\dots,\v_{r-1} }$ are linearly independent implying that ${ \alpha_1,\dots,\alpha_{r-1} = 0 }$. This, in turn, implies that ${ \alpha_r = 0 }$ meaning that ${ \v_1,\dots,\v_r }$ is linearly independent.
 		\end{proof}
 	
 	
 		\bigskip
 		\labeledProposition{If we consider an $n \times n$ matrix $A$ with real entries as a matrix in $\C{n \times n}$ and it has a complex eigenvalue $\lambda$ with a corresponding eigenvector $\v$, then the complex conjugate $\conj{\lambda}$ is also an eigenvalue and has a corresponding eigenvector $\conj{\v}$, the complex conjugate of $\v$.}{complex-eigenvalues-and-eigenvectors-come-in-pairs-with-their-conjugates}
 		\begin{proof}
 			\[\begin{aligned}
 				&& A\v &= \lambda\v \\
 				&\iff & \conj{A\v} &= \conj{\lambda\v} \\
 				&\iff & \conj{A} \, \conj{\v} &= \conj{\lambda} \, \conj{\v} &\sidecomment{by \ref{prop:conjugate-of-matrix-times-vector-is-conjugate-matrix-times-conjugate-vector}}.
 			\end{aligned}\]
 			But $A$ is a matrix with only real entries and so ${ \conj{A} = A }$. So the previous result implies that
 			\[ A \conj{\v} = \conj{\lambda} \, \conj{\v} \]
 			which means that $\conj{\lambda}$ is an eigenvalue of $A$ with a corresponding eigenvector $\conj{\v}$. 
 		\end{proof}
 	
 		\bigskip
 		\labeledProposition{A hermitian matrix has only real-valued eigenvalues.}{hermitian-matrix-has-only-real-eigenvalues}
 		\begin{proof}
 			Let $A$ be a hermitian matrix (\ref{def:hermitian-matrix}) and let $\lambda$ be an arbitrary eigenvalue of $A$ with $\v$ as an eigenvector corresponding to $\lambda$. Then,
 			\[\begin{aligned}
 				&& \v^* A \v &= \v^* A^* \v   &\sidecomment{${ \because A }$ is hermitian}\\
 				&\iff & \v^* (A \v) &= (A \v)^* \v   &\sidecomment{by props. of hermitian conjugate \ref{def:hermitian-conjugate}} \\
 				&\iff & \v^* (\lambda \v) &= (\lambda \v)^* \v   &\sidecomment{by eigenvalue property}\\
 				&\iff & \v^* (\lambda \v) &= \conj\lambda \v^* \v   &\sidecomment{by \ref{prop:hermitian-conj-of-scalar-x-matrix-is-conj-of-scalar-x-hermitian-conj-of-matrix}}\\
 				&\iff & \lambda \norm{\v}^2 &= \conj\lambda \norm{\v}^2   &\sidecomment{by defn. of induced norm \ref{def:vector-norm}} \\
 				&\iff & \lambda &= \conj\lambda   &\sidecomment{${ \because \v \neq \0 \implies \norm{\v} > 0 }$ so has inverse} \\
 				&\therefore & \lambda &\in \R{}.  \qedhere
 			\end{aligned}\]
 		\end{proof}
 	
	 
	 	\bigskip\medskip
	 	\subsubsection{Examples of Eigenvectors and Eigenvalues}
	 	\begin{exe}
	 		\item{If we take the matrix,
	 			\[ 
		 			\begin{bmatrix}
		 			3 & 1\\
		 			0 & 2
		 			\end{bmatrix}
	 			\]
	 			we can determine its eigenvectors by finding the solutions to the equation,
	 			\[
	 			 	\begin{bmatrix}3 & 1\\0 & 2\end{bmatrix}\begin{bmatrix}x_1\\x_2\end{bmatrix} = c\begin{bmatrix}x_1\\x_2\end{bmatrix}.	 			 	
	 			\]
	 			This gives us two simultaneous equations in three unknowns: the two vector dimensions ${ x_1,x_2 }$ and the eigenvalue $c$,
	 			\begin{align*}
	 			&& 3x_1 + x_2 &= cx_1 \\
	 			&& 2x_2 &= cx_2 &\sidecomment{} \\
	 			\end{align*}
	 			which imply that,
	 			\[ 6x_1 = 2cx_1 - cx_2 \iff (6/c - 2)x_1 = -x_2 \iff x_2 = (2 - 6/c)x_1. \]
	 			So, if ${ x_1 = 1 }$ then ${ x_2 = 2 - (6/c) }$ and we have the follwing eigenvector/eigenvalue pairs.
	 			\[
	 				c=3: \begin{bmatrix}1\\0\end{bmatrix}, \; c=1: \begin{bmatrix}1\\-4\end{bmatrix} \wrong
	 			\]
	 			\subparagraph{Why does this not work?} There is an additional constraint not expressed in the linear system here: eigenvectors are nonzero by definition. This means that we have the additional constraint,
	 			\[ x_1x_2 \neq 0 \]
	 			which cannot be expressed in a linear system of equations. When ${ c \not\in \{2,3\} }$ both $x_1$ and $x_2$ are zero and the vector is not an eigenvector.\\
	 			So, how can we systematically restrict the values of $c$ to only those that produce valid eigenvectors? If we follow the alternative logic:
	 			\begin{align*}
	 			&& \begin{bmatrix}3 & 1\\0 & 2\end{bmatrix}\begin{bmatrix}x_1\\x_2\end{bmatrix} &= c\begin{bmatrix}x_1\\x_2\end{bmatrix}  \\
	 			&\iff & \begin{bmatrix}3 & 1\\0 & 2\end{bmatrix}\begin{bmatrix}x_1\\x_2\end{bmatrix} - c\begin{bmatrix}x_1\\x_2\end{bmatrix} &= \0 &\sidecomment{} \\
	 			&\iff & \left( \begin{bmatrix}3 & 1\\0 & 2\end{bmatrix}I - cI \right)\begin{bmatrix}x_1\\x_2\end{bmatrix} &= \0 &\sidecomment{} \\
	 			&\iff & \begin{bmatrix}3 - c & 1\\0 & 2 - c\end{bmatrix}\begin{bmatrix}x_1\\x_2\end{bmatrix} &= \0 &\sidecomment{} \\
	 			\end{align*}
	 			Now if we let
	 			\[ A = \begin{bmatrix}3 - c & 1\\0 & 2 - c\end{bmatrix} \]
	 			then the nullspace of $A$ is the space of vectors ${ (x_1,x_2)^T }$ that satisfy this equation. If $A$ is invertible and nonsingular then this space is the trivial space ${ \{\0\} }$ but, if $A$ is singular however, then there exist nonzero vectors that satisfy this equation. Therefore, it is precisely the nonzero vectors that are in the nullspace of this matrix $A$ when it is singular that are the eigenvectors we are looking for. So, if we first determine the values of $c$ for which $A$ is singular, we can then determine the eigenvectors. Since, by \autoref{prop:properties_of_non_bijective_lin_operator}, $A$ is singular if and only if ${ det\,A = 0 }$, we are looking for \textit{precisely the values of $c$ which make ${ det\,A = 0 }$}.
	 		}
 			\item{The minimal linear transformation seen in \ref{ex:minimal-linear-transformation} --- which is just scalar multiplication --- has every vector as its eigenvectors because
 				\[ T(\v) = A\v = a\v \]
 				for all ${ \v \in V }$. So every vector in $V$ is an eigenvector with eigenvalue $a$.
 			}\label{ex:eigenvectors-of-minimal-linear-transformations}
	 	\end{exe}
 	
 		\bigskip
 		\subsubsection{Matrices of Eigenvectors}
 		\bigskip
 		If ${ \v_1 \in V }$ is a eigenvector of a linear transformation $T$ and we extend the set ${ \{\v_1\} }$ (by \autoref{prop:lin_ind_set_can_be_extended_to_basis}) to a basis of $V$, say ${ \{\v_1,\dots,\v_n\} }$, then the matrix of $T$ will have the block form,
 		\[
 			\begin{bmatrix}
 			c & B\\
 			0 & D
 			\end{bmatrix} =
 			\begin{bmatrix}
 			c & \dots & \dots\\
 			0 & \dots & \dots\\
 			\vdots & \dots & \dots\\
 			0 & \dots & \dots
 			\end{bmatrix}
 		\]
 		where $c$ is the eigenvalue of $\v_1$. This is the same block decomposition as that shown for $T$-invariant spaces in \ref{eq:invariant_space_block_decomposition} with the case of a 1-dimensional invariant subspace.
 		
 		\medskip
 		\labeledProposition{If $A$ is the matrix of a linear operator $T$ with respect to a basis $B$ then the matrix $A$ is diagonal iff every basis vector in $B$ is an eigenvector of $T$.}{diagonal_iff_every_basis_vector_eigenvector}
 		\begin{proof}
 			The defining property of the matrix $A$ is that the $j$-th column is the coordinates of the image of the $j$-th basis vector in $B$ under $T$,
 			\[ A(:j) = T(\v_j) = a_{1j}\v_1 + \cdots + a_{nj}\v_n. \]
 			For an eigenvector $\v_j$, ${ T(\v_j) = c\v_j = a_{jj}\v_j }$ so that ${ a_{jj} = c }$ the eigenvalue and for all ${ a_{ij} }$ such that ${ i \neq j }$, ${ a_{ij} = 0 }$. 
 		\end{proof}
 		\begin{corollary}
 			\label{coro:diagonal-similar-matrix-iff-exists-basis-of-eigenvectors}
 			The matrix of a linear operator $T$ over a vector space $V$ is similar to a diagonal matrix iff there exists some basis of $V$ solely comprised of eigenvectors of $T$. 
 		\end{corollary}
 	
 		\bigskip
 		\labeledProposition{Similar matrices have the same eigenvalues.}{similar_matrices_same_eigenvalues}
 		\begin{proof}
 			Similar matrices represent the same transformation with respect to different bases and for a matrix $A$ representing a transformation $T$ with respect to an arbitrary basis $B$,
 			\[ T(\v) = c\v \iff A\v_B = c\v_B. \]
 			That's to say, the eigenvalues are not dependent on the basis with respect to which a coordinate vector is defined.
 		\end{proof}
	}


% --------------------


	\pagebreak
	\searchableSubsection{Diagonalisation}{linear algebra}{
		\bigskip\bigskip
		\boxeddefinition{The process of determining a diagonal matrix that is similar to a given matrix of a linear operator is known as \textbf{diagonalisation}.}
		
		\bigskip
		\subsubsection{Existence of Eigenvectors}
		\begin{itemize}
			\item{Every linear operator on a complex vector space has at least one eigenvector and, in most cases, these form a basis.}
			\item{Linear operators over real vector spaces need not have eigenvectors (e.g. rotation of the plane $\R{2}$ by an angle $\theta$ has no eigenvector unless ${ \theta = 0 \text{ or } \pi }$).}
			\item{Real matrices that are \textit{positive} (having only positive components) are guaranteed to have at least one positive eigenvector.}
		\end{itemize}
	
		\bigskip
		\subsubsection{The Effect of Multiplication by a Positive Matrix}
		\TODO{Artin[134]}
		
		\bigskip
		\subsubsection{Determining the Eigenvectors}
		The process of finding eigenvectors is to first determine the eigenvalues and then calculate the eigenvectors that correspond to those eigenvalues. Let $I$ be the identity operator. Then,
		\[ T(\v) = c\v \iff T(\v) - c\v = \0 \iff [T - cI](\v) = \0 \]
		where the expression ${ T - cI }$ is a linear combination of linear transformations and so is also a linear transformation. Furthermore, it is an operator as both its terms are operators (in fact they need to have the same dimensions in order for the expression to make sense).\\
		Two things are clear from this expression:
		\begin{enumerate}[label=(\roman*)]
			\item{The matrix of the linear operator ${ T - cI }$ is ${ A - cI }$ where $I$ is the identity matrix.}
			\item{The eigenvector $\v$ is in the kernel of ${ T - cI }$ and so is in the nullspace of ${ A - cI }$.}
		\end{enumerate}
	
		\medskip
		\labeledProposition{A linear operator $T$ has a nontrivial kernel iff 0 is an eigenvalue of $T$.}{lin_op_singular_iff_0_eigenvalue}
		\begin{proof}
			This follows from the fact that, if we let ${ \v \neq \0 \in ker\,T }$, then
			\[ T(\v) = \0 = 0\v = c\v \]
			for ${ c = 0 }$ so that a nontrivial kernel implies that 0 is an eigenvalue. Conversely, if 0 is an eigenvalue we must have ${ 0\v = \0 = T(\v) }$ and, since if a vector $\v$ is an eigenvector, by definition, ${ \v \neq \0 }$, this therefore implies that the kernel contains a nonzero vector.
		\end{proof}
		\begin{corollary}
			A linear operator $T$ has all the properties in \autoref{prop:properties_of_non_bijective_lin_operator} iff 0 is an eigenvalue of $T$.
		\end{corollary}
	
		\medskip
		\labeledProposition{The eigenvalues of a linear operator $T$ are the scalars ${ c \in \F{} }$ such that the linear operator ${ [T - cI] }$ is singular.}{eigenvals_are_c_such_that_T-cI_singular}
		\begin{proof}
			The eigenvalues of a linear operator $T$ are the scalars ${ c \in \F{} }$ such that there exists a nonzero vector $\v$ with ${ [T - cI](\v) = \0 }$. If such a vector exists then the kernel of ${ T - cI }$ is nontrivial and so, by \autoref{prop:properties_of_non_bijective_lin_operator}, ${ T - cI }$ is singular.
		\end{proof}
		\begin{corollary}
			If ${ A - cI }$ is the matrix of ${ T - cI }$, \textbf{the eigenvalues of $\bm{T}$ are the scalars ${\bm{ c \in \F{} }}$ such that ${\bm{ det(A - cI) = 0 }}$}.
		\end{corollary}
		\begin{corollary}
			The eigenvalues of ${ A - cI }$ are the same as the eigenvalues of ${ cI - A }$.
		\end{corollary}
		\begin{proof}
			If $A$ is a ${ n \times n }$ matrix representing the operator $T$ then,
			\[ det(-A) = (-1)^n det(A). \]
			So, if ${ det(A) = 0 }$ then ${ det(-A) = 0 }$ also. Therefore,
			\[ det(A - cI) = 0 \iff det(cI - A) = 0. \qedhere \]
		\end{proof}
	
		\bigskip
		\subsubsection{The Characteristic Polynomial}
		\notation{It is customary to use either the variable $t$ or $\lambda$ to denote the eigenvalue in the characteristic polynomial.}
		\boxeddefinition{The \textbf{characteristic polynomial} of a linear operator $T$ is the polynomial,
			\[ p(t) = det(tI - A) = \sum s(j_1,\cdots,j_n)a_{1j_1} \cdots a_{nj_n} \]
			where the sum is defined over all permutations ${ j_1, \cdots, j_n }$ of ${ \{1,\dots,n\} }$  and ${ s(j_1,\cdots,j_n) }$ is the sign of the permutation.
		}\label{def:characteristic-polynomial}
		\note{The determinant is an expression in which every term is the product of a component in every column and row of the matrix with no column or row appearing more than once in each term,
			\[
				tI - A = \begin{bmatrix}
						(t - a_{11}) & -a_{12} & \cdots & -a_{1n}\\
						-a_{21}	& (t - a_{22}) & \cdots & -a_{2n}\\
						\vdots &   &  & \vdots\\
						-a_{n1} & \cdots & \cdots & (t - a_{nn})
						\end{bmatrix}.
			\]
			It can be seen in the matrix of ${ tI - A }$ that the highest power of $t$ will be obtained in the term of the determinant that forms the product of all the diagonal terms ${ a_{11} \cdots a_{nn} }$ which occurs when ${ j_1, \cdots, j_n = 1,\dots,n }$. This term of the determinant will be the product of precisely $n$ terms containing the eigenvalue $t$. Therefore the result is a polynomial of degree $n$ in the eigenvalue $t$.
		}
	
		\medskip
		\labeledTheorem{The eigenvalues of a linear operator are the roots of its characteristic polynomial.}{eigenvalues_are_roots_of_characteristic_poly}
		\begin{proof}
			If $p(t)$ is the characteristic polynomial of a linear operator $T$ then the values of $t$ for which ${ p(t) = 0 }$ are the values of $t$ such that ${ det(tI - A) = 0 }$ and these are precisely the eigenvalues.
		\end{proof}
	
		\bigskip
		\labeledProposition{The eigenvalues of an upper or lower triangular matrix are its diagonal entries.}{triangular_matrix_eigenvalues_are_on_diagonal}
		\begin{proof}
			The determinant of a triangular matrix is equal to the product of its diagonal entries and if $A$ is a triangular matrix then ${ tI - A }$ is also triangular. Therefore, the characteristic polynomial is simply,
			\[ p(t) = (t - a_{11})\cdots(t - a_{nn}) \]
			and so the eigenvalues are the diagonal entries ${ a_{11},\dots,a_{nn} }$.
		\end{proof}
	
		\bigskip
		\labeledProposition{A positive matrix (a matrix whose entries are all positive) has at least one eigenvector with positive coordinates.}{positive_matrix_has_at_least_1_positive_eigenvector}
		\note{An abstract vector does not have coordinates so when we refer to a "positive" vector with positive-valued coordinates, this is with respect to a particular basis. In this context, the basis in question is the basis with respect to which the matrix outputs the transformed vectors.}
		\begin{proof}
			\TODO{review: this "proof" is not really a proof, more an example using a 2x2 matrix.}
		\end{proof}
	
		\bigskip
		\labeledProposition{The characteristic polynomial of a linear operator does not depend on the basis with respect to which the matrix of the operator is defined.}{characterstic_poly_independent_of_basis_of_matrix}
		\begin{proof}
			For two similar matrices representing the same linear operator $T$ we have,
			\[ A' = PA\inv{P} \]
			where $P$ is the matrix of change of basis between the bases of $A$ and $A'$. If we form the characteristic polynomial of $A'$,
			\begin{align*}
			&& tI - A' &= tI - PA\inv{P} \\
			&\iff &  &= PtI\inv{P} - PA\inv{P} &\sidecomment{\small{${ PtI\inv{P} = tP\inv{P} = tI }$}} \\
			&\iff &  &= P(tI - A)\inv{P} &\sidecomment{by distributivity of matrix multiplication}. \\
			\end{align*}
			Then,
			\begin{align*}
			&& det(tI - A') &= det(P(tI - A)\inv{P}) \\
			&\iff &  &= det\,P \cdot det(tI - A) \cdot det\,\inv{P} & \\
			&\iff &  &= det(tI - A). & \\
			\end{align*}
			This result, ${ det(tI - A') = det(tI - A) }$, must hold for all $t$ and therefore implies that, for $p,p'$ characteristic polynomials of $A$ and $A'$ respectively,
			\[ \forall t \in \F{} \logicsep p(t) = p'(t). \]
			This implies that the characteristic polynomials are equal.
		\end{proof}
	
		\bigskip
		\labeledProposition{The characteristic polynomial ${ p(t) }$ of a matrix $A$ has the form
			\[ p(t) = t^n - (tr\,A)t^{n-1} + \cdots + (-1)^n(det\,A), \]
			where ${ tr\,A }$ is the trace of $A$ (see: \ref{sssection:trace}):
			\[ tr\,A = a_{11} + \cdots + a_{nn}. \]
		}{characteristic_poly_coefficients}
		\begin{proof}
			Calculation of the characteristic polynomial of a matrix $A$ is calculation of ${ p(t) = det(tI - A) }$ the determinant of the matrix ${ tI - A }$ which takes the form,
			\[
				tI - A = \begin{bmatrix}
						(t - a_{11}) & \cdots & \cdots 			   & \vdots\\
						\vdots & (t - a_{22}) & \cdots 			   & \vdots\\
						\vdots &   			&  					   & \vdots\\
						\vdots & \cdots 	& (t - a_{(n-1)(n-1)}) & -a_{(n-1)n}\\
						\vdots & \cdots 	& -a_{n(n-1)} 		   & (t - a_{nn})
						\end{bmatrix}.
			\]
			This calculation proceeds with terms of products of elements from each row and a permutation of the column indices (see: \ref{sssection:determinant_formula}) of which we examine the first two terms.\\
			
			The first term is for the identity permutation ${ j_1,\dots,j_n = 1,\dots,n }$ along the diagonal:
			\begin{align*}
			&& a_{11} \cdots a_{nn} &= (t - a_{11})\cdots(t - a_{nn})  \\
			&& &= t^n - (a_{11} + \cdots + a_{nn})t^{n-1} \\ 
			&&&\hspace{20pt} + (a_{11}a_{22} + a_{11}a_{33} + \cdots + a_{(n-1)(n-1)}a_{nn})t^{n-2} \\
			&&&\hspace{20pt} \dots + (-1)^n(a_{11} \cdots a_{nn}) &\sidecomment{}
			\end{align*}
			The second term is the permutation one swap away from identity 
			\[ j_1,\dots,j_n = 1,\dots,n,(n-1) \]
			which is an odd permutation, so the sign is $-1$:
			\begin{align*}
			& \hspace{15pt} (-1)a_{11} \cdots a_{(n-2)(n-2)}a_{(n-1)n}a_{n(n-1)} &\\
			& =(-1)(t - a_{11}) \cdots (t - a_{(n-2)(n-2)})(-a_{(n-1)n})(-a_{n(n-1)}) & \\
			& = - a_{(n-1)n}a_{n(n-1)}t^{n-2} + a_{(n-1)n}a_{n(n-1)}(a_{11} + \cdots +  a_{(n-2)(n-2)})t^{n-3} & \\ 
			& \hspace{20pt} \dots - (-1)^n(a_{11} \cdots a_{(n-2)(n-2)}a_{(n-1)n}a_{n(n-1)}) &
			\end{align*}
			From these first two terms we can discern enough about the general pattern of the characteristic polynomial to see that the first two terms in $t^n$ and $t^{n-1}$ are produced by the first permutation and take the form
			\[ t^n - (a_{11} + \cdots + a_{nn})t^{n-1} = t^n - (tr\,A)t^{n-1} \]
			as required. We can also see that the final terms of each permutation --- those that involve no powers of $t$ --- are going to sum up to the value of ${ (-1)^n(det\,A) }$. Therefore the characteristic polynomial takes the form,
			\[ p(t) = t^n - (tr\,A)t^{n-1} + \cdots + (-1)^n(det\,A) \]
			as claimed.
		\end{proof}
	
		\bigskip
		\begin{corollary}
			\label{coro:matrix-trace-independent-of-basis}
			The trace of a matrix of a linear operator is independent of the basis with respect to which the matrix is defined.
		\end{corollary}
		\begin{proof}
			Let $T$ be a linear operator and $A$ be the matrix of $T$ with respect to a basis $B$. Then \autoref{prop:characterstic_poly_independent_of_basis_of_matrix} tells us that any matrix of the same linear operator defined with respect to some other basis (i.e. a similar matrix to $A$) has the same characteristic polynomial. \autoref{prop:characteristic_poly_coefficients} tells us that the coefficients of this characteristic polynomial include the trace of the matrix $A$. Therefore, if $A'$ is a matrix of $T$ defined against the basis $B'$ and $P$ is the change of basis matrix such that ${ B'P = B }$ then,
			\[ A' = PA\inv{P} \iff p'(t) = p(t) \iff tr\,A' = tr\,A.  \qedhere \]
		\end{proof}
	
		\medskip\note{As a result of this we can refer to the characteristic polynomial, determinant and trace of a linear operator $T$ without reference to a particular matrix or basis.}
		
		\bigskip
		\labeledProposition{Let $T$ be a linear operator on a finite-dimensional vector space $V$.%
			\begin{enumerate}[label=(\roman*)]
				\item{If $V$ has dimension $n$, then $T$ has at most $n$ eigenvalues.}
				\item{If $\F{}$ is the field of complex numbers and ${ V \neq 0 }$, then $T$ has at least one eigenvalue, and hence it has an eigenvector.}
			\end{enumerate}
		}{num-eigenvalues}
		\begin{proof}\nl
			\begin{enumerate}[label=(\roman*)]
				\item{For any field $\F{}$, a polynomial of degree $n$ can have at most $n$ different roots (see Artin[373]). Since $T$ is defined over a vector space of dimension $n$, the degree of the characteristic polynomial of $T$ is $n$. Then, by \autoref{theo:eigenvalues_are_roots_of_characteristic_poly} we can have a maximum of $n$ eigenvalues.}
				\item{Every polynomial of positive degree with complex coefficients has at least one complex root. This fact is called the Fundamental Theorem of Algebra (\href{https://en.wikipedia.org/wiki/Fundamental_theorem_of_algebra}{wikipedia}).}
			\end{enumerate}
		\end{proof}
	
		\bigskip
		\labeledProposition{Let $T$ be a linear operator on a finite-dimensional complex vector space $V$. There is a basis $B$ of $V$ such that the matrix $A$ of $T$ is upper triangular.}{complex_space_exists_basis_st_matrix_is_upper_triang}
		\begin{proof}
			By \autoref{prop:num-eigenvalues}, $T$ has at least one eigenvector. We can extend this eigenvector to a basis of $V$, say,
			\[ B' = \{\v_1',\dots,\v_n'\}. \]
			Then the first column of the matrix $A'$ of $T$ with respect to $B'$ will be
			\[ (c_1,0,\dots,0)^T \]
			where $c_1$ is the eigenvalue of $\v_1'$. Therefore $A'$ has the form
			\[
				A' = \begin{bmatrix}
					c_1 & \cdots \\
					0   &   D \\
					\end{bmatrix} 
			\]
			where $D$ is a ${ (n-1) \times (n-1) }$ matrix and, if ${ P = \inv{[B']}I = \inv{[B']} }$ is the change of basis matrix, then
			\[ A' = PA\inv{P}. \]
			Now we can use induction on the dimension of the matrix $n$ and the induction hypothesis will be that there exists some upper triangular
			\[ Q' = QD\inv{Q}. \]
			Define
			\[ Q_1 = \begin{bmatrix}
					1 & 0\\
					0 & Q
					\end{bmatrix}. 
			\]
			Then
			\[ (Q_1P)A\inv{(Q_1P)} = Q_1PA\inv{P}\inv{Q_1} = Q_1A'\inv{Q_1} \]
			takes the form
			\[
				\begin{bmatrix}
				c_1 & \cdots\\
				0   & QD\inv{Q}
				\end{bmatrix} 
			\]
			which is upper triangular. This proves the induction step.
		\end{proof}
	
		\medskip\note{Note that this proof is over the complex number field but the same proof would work over any field that contains all the roots of the characteristic polynomial.}

	
		\bigskip
		\labeledTheorem{Let $T$ be a linear operator on a vector space $V$ of dimension $n$ over a field $F$. Assume that its characteristic polynomial has $n$ \textbf{distinct} roots in $F$. Then there is a basis for $V$ with respect to which the matrix of $T$ is diagonal.}{maximum-distinct-roots-of-char-poly-implies-diagonal}
		\begin{proof}
			If the characteristic polynomial has $n$ distinct roots then there are $n$ distinct eigenvalues along with their associated eigenvectors. By \autoref{prop:distinct-eigenvals-means-independent-eigenvecs}, these eigenvectors form a linearly independent set. Since the dimension of the space $V$ is $n$, by \autoref{theo:lin_ind_set_of_dim_cardinality_is_basis}, these eigenvectors form a basis of $V$. Then, by \autoref{prop:diagonal_iff_every_basis_vector_eigenvector}, the matrix of $T$ with respect to this basis is diagonal.
		\end{proof}
	
		\bigskip\note{Note that:
			\begin{itemize}
				\item{The diagonal entries of the matrix of a linear operator with respect to a basis of eigenvectors are the eigenvalues. For this reason, the set of values is wholly determined by the linear operator although the order is determined on the order of the vectors in the basis set (which is not significant);
				}
				\item{If a matrix $A$ is found to be similar to a diagonal matrix $B$ via a change of basis expressed in the matrix $P$ then, by \autoref{theo:exponentiation-of-similar-matrices},
					\[ A^m = (\inv{P}BP)^m = \inv{P}B^mP. \]
				}
			\end{itemize}
		}
		\begin{corollary}\label{coro:matrix-with-eigenbasis-has-trace-equal-to-sum-of-eigenvalues-and-determinant-equal-to-product}
			If a linear operator on a vector space of dimension $n$ over a field $F$ has a characteristic polynomial with $n$ distinct roots then its determinant is equal to the product of its eigenvalues and its trace is equal to the sum of its eigenvalues.
		\end{corollary}
		\begin{proof}
			By \autoref{prop:similar-matrices-have-same-determinant} and \autoref{coro:matrix-trace-independent-of-basis}, the determinant and trace are independent of the basis with respect to which the matrix is defined and so the existence of a basis with respect to which the matrix is diagonal means that we can look at the determinant and trace of the diagonal matrix.
		\end{proof}
	
	
		\bigskip
		\labeledProposition{Let $T$ be a linear operator on a finite-dimensional vector space $V$ over a field $F$, $q(t)$ an arbitrary polynomial over $F$, and $\v$ an eigenvector of $T$ with eigenvalue $c$. Then
			\[ q(T)\v = q(c)\v. \]
		}{polynomial-of-linear-map-times-eigenvec-is-polynomial-of-eigenval-times-eigenvec}
		\begin{proof}
			The polynomial $q(t)$ has the general form,
			\[ q(t) = a_n t^n + \cdots + a_1 t + a_0. \]
			Since $T$ is an operator we can use \ref{defn:polynomials-of-linear-operators} to define,
			\[ q(T) = a_n T^n + \cdots + a_1 T + a_0 I. \]
			Then,
			\[\begin{aligned}
				q(T)\v &= a_n T^n \, \v + \cdots + a_1 T \v + a_0 \v \\
				&= a^n c^n \, \v + \cdots + a_1 c \v + a_0 \v \\
				&= (a^n c^n + \cdots + a_1 c + a_0) \v \\
				&= q(c)\v. \qedhere
			\end{aligned}\]
		\end{proof}
		\begin{corollary}
			If $p(t)$ is the characteristic polynomial of a \textbf{diagonalizable} linear operator $T$ over a finite-dimensional vector space, then $p(T)$ is 0, the zero operator $T_0$.
		\end{corollary}
		\begin{proof}
			As above, for an eigenvector $\v$ with eigenvalue $c$,
			\[ p(T)\v = p(c)\v. \]
			But now, $p(t)$ is the characteristic polynomial of $T$ and $c$, as an eigenvalue of $T$, \textit{is a root of the characteristic polynomial}. So, for any eigenvector of $T$,
			\[  p(T)\v = p(c)\v = 0\v = 0. \]
			Since $T$ is diagonalizable, there exists a basis of the space $V$ comprised solely of eigenvectors. Therefore, for any vector ${ \v \in V }$, we can express it as a linear combination of eigenvectors each of which will be mapped to the zero vector by $p(T)$.
		\end{proof}
	
		\bigskip
		\labeledProposition{Let $T$ be a linear operator on a finite-dimensional vector space $V$, and let $W$ be a $T$-invariant subspace of $V$. Then the characteristic polynomial of the restriction of $T$ to $W$, $T_w$, divides the characteristic polynomial of $T$.}{char-poly-of-restriction-to-invariant-space-divides-char-poly-of-T}
		\begin{proof}
			Let ${ B_w = \{ \w_1, \dots, \w_k \} }$ be a basis of $W$ and extend it to a basis of $V$, ${ B = \{ \w_1, \dots, \w_k, \v_1, \dots, \v_{n-k} \} }$. Then the matrix of $T$ with respect to the basis $B$ has the form,
			\[ A = 	\begin{bmatrix}
						A_w & C  \\
						0   & D
					\end{bmatrix}
			\]
			where ${ A_w = T_w(B_w) }$ is the matrix of $T_w$ \wrt to the basis $B_w$.\\
			So, the characteristic polynomial $p(t)$ of $A$ is
			\[\begin{aligned}
				p(t) = \det (A - tI_n) &=
									\begin{vmatrix}
										A_w - tI_k & C \\
										0               & D - tI_{n-k}
									\end{vmatrix} \nn
				&= \det (A_w - tI_k) \cdot \det (D - tI_{n-k}) \nn
				&= q(t) \cdot \det (D - tI_{n-k})
			\end{aligned}\]
			where $q(t)$ is the characterstic polynomial of $T_w$, the restriction of $T$ to $W$. 		
		\end{proof}
	
		\bigskip
		\labeledProposition{Let $T$ be a linear operator on a finite-dimensional vector space $V$, and let $W$ denote the $T$-cyclic subspace of $V$ generated by ${ \v \in V }$. Suppose that ${ \dim W = k \geq 1 }$ (and hence ${ \v \neq \0 }$). Then,
			\begin{enumerate}[label=(\roman*)]
				\item{${ \{ \v, T\v, T^2\,\v, \dots, T^{k-1}\,\v \} }$ is a basis for $W$.}
				\item{If 
					\[ T^k\,\v = -a_0\v - a_1 T\v - \cdots - a_{k-1} T^{k-1}\,\v, \]
					then the characteristic polynomial of $T_w$ is
					\[ p(t) = (-1)^k (t^k + a_{k-1} t^{k-1} + \cdots + a_1 t + a_0). \]
				}
			\end{enumerate}
		}{characteristic-polynomial-of-T-cyclic-subspace}
		\begin{proof}
			Let ${ B_x = \setc{ T^i\,\v }{ i \in \N{}, \; 0 \leq i \leq x } }$. Since $V$ is finite-dimensional, there exists some ${ m \in \N{} }$ such that
			\[ T^m\,\v \in \operatorname{span} B_{m-1}. \]
			Let ${ j \in \N{} }$ be the lowest such value so that ${ B_{j-1} = \setc{ T^i\,\v }{ i \in \N{}, \; 0 \leq i \leq j-1 } }$ is linearly independent. If we can show that $B_{j-1}$ spans $W$ then, by \autoref{theo:lin_ind_set_of_dim_cardinality_is_basis}, $B_{j-1}$ is a basis of $W$.\\
			We can show inductively that for any ${ m \in \N{} }$, ${ T^m\,\v \in \operatorname{span} B_{j-1} }$. Begin by observing that, clearly, ${ T^m\,\v }$ for any ${ 0 \leq m \leq j-1 }$ is a member of $B_{j-1}$ and is, therefore, trivially in the span. Then, for some ${ m > j-1 }$ we have that, by the induction hypothesis, $T^{m-1}\,\v$ is in the span of $B_{j-1}$ so,
			\[ T^{m-1}\,\v = a_0 \v + a_1 T\v + \cdots + a_{j-1} T^{j-1}\,\v \]
			for some ${ a_0, a_1, \dots, a_{j-1} \in \F{} }$. Then,
			\[\begin{aligned}
				T^m\,\v &= T(a_0 \v + a_1 T\v + \cdots + a_{j-1} T^{j-1}\,\v) \\
				&= a_0 T\v + a_1 T^2\,\v + \cdots + a_{j-1} T^j\,\v \\
			\end{aligned}\]
			which is in the span of $B_{j-1}$ because $T^j\,\v$ is in the span by construction. Therefore, $B_{j-1}$ is a basis of $W$ and it follows then that it must have length $k$ so that ${ j = k }$.
			
			\note{Note that we could also have attempted to prove this by invoking the division theorem (\autoref{theo:division-theorem}) to observe that, if ${ j \in \N{} }$ is the lowest natural number such that ${ T^j\,\v \in \operatorname{span} B_{j-1} }$, then, for any ${ m > j \in \N{} }$, there exist ${ q \in \Z{}, r \in \N{} }$ with ${ 0 \leq r < j }$, such that
				\[ m = qj + r \implies T^m\,\v = (T^j\,\v)^q \, (T^r\,\v). \]
				Here, $T^j\,\v$ is in the span of $B_{j-1}$ by hypothesis and $T^r\,\v$ is an element of $B_{j-1}$ and so, trivially, in the span. However, this still leaves the question of whether $(T^j\,\v)^q$ is in the span of $B_{j-1}$ -- which is precisely what we're trying to prove!
			}
			
			To build the characteristic polynomial of $T_w$, observe that the regular structure of the basis $B_{k-1}$ results in a regular structure of the matrix of the transformation \wrt this basis (remember that, by \autoref{prop:characterstic_poly_independent_of_basis_of_matrix}, the characterstic polynomial is the same regardless of the basis \wrt which we specify the matrix). If $A_w$ is the matrix of $T_w$ \wrt to the basis $B_{k-1}$ then (see: \ref{sssection:linear-trans-and-change-of-basis}),
			\[ A_w = \inv{[B_{k-1}]} [T(B_{k-1})] \]
			where $[B_{k-1}]$ denotes the matrix whose columns are the elements of $B_{k-1}$ and $[T(B_{k-1})]$ denotes the matrix whose columns are the elements of $B_{k-1}$ transformed by $T$. So, the columns of $[T(B_{k-1})]$ are ${ T\v, T^2\,\v, \dots, T^k\,\v }$ which, when expressed \wrt the basis $B_{k-1}$ are going to be,
			\[ (0,1,0,\dots,0), \, (0,0,1,0\dots,0), \dots, (-a_0, -a_1, \dots, -a_{k-1}). \]
			
			So, the matrix $A_w$ is going to take the form,
			\[ A_w = 	\begin{bmatrix}
							0 & 0 & \cdots & 0 & -a_0 \\
							1 & 0 & \cdots & 0 & -a_1 \\
							0 & 1 & \cdots & 0 & -a_2 \\
							\vdots & \vdots & &  & \vdots \\
							0 & 0 & \cdots & 0 & -a_{k-2} \\
							0 & 0 & \cdots & 1 & -a_{k-1} \\
						\end{bmatrix}.
			\]
			Note that $A_w$ is square ${ k \times k }$ which is as expected because $T_w$ is ${ W \longmapsto W }$.\\
			
			To show that this results in the desired characteristic polynomial we use induction on $k$, the dimension of $A_w$.\\
			
			For ${ k = 2 }$,
			\[\begin{aligned}
				A_w &= 	\begin{bmatrix}
							0 & -a_{k-2} \\
							1 & -a_{k-1}
						\end{bmatrix} \leadsto
						\begin{vmatrix}
							-t & -a_{k-2} \\
							1 & -a_{k-1} - t
						\end{vmatrix} \nn
				&= (-t)^k + (-t)^{k-1} (-a_{k-1}) + (-1)^{k-1} (-a_{k-2}) (1) \\
				&= (-1)^k (t^k + a_{k-1} t^{k-1} + a_0).
			\end{aligned}\]
			which has the desired form ${ (-1)^k (t^k + a_{k-1} t^{k-1} + \cdots + a_1 t + a_0) }$.\\
			
			For ${ k = n > 2 \in \N{} }$, we assume that matrices of size ${ (n-1) \times (n-1) }$ have the desired form of characteristic polynomial,
			\[ p_{n-1}(t) = (-1)^{n-1} (t^{n-1} + a_{n-2} t^{n-2} + \cdots + a_1 t + a_0). \]
			Then,
			\[\begin{aligned}
				\det A_w &= \begin{vmatrix}
								-t & 0 & \cdots & 0 & -a_0 \\
								1 & -t & \cdots & 0 & -a_1 \\
								0 & 1 & \cdots & 0 & -a_2 \\
								\vdots & \vdots & &  & \vdots \\
								0 & 0 & \cdots & -t & -a_{k-2} \\
								0 & 0 & \cdots & 1 & -a_{k-1} - t \\
							\end{vmatrix} \nn
				&= (-t) p_{n-1}(t) + (-1)^{k-1} (-a_0) (1) &\sidecomment{} \\
				&= (-t) [(-1)^{k-1} (t^{k-1} + a_{k-1} t^{k-1} + \cdots + a_2 t + a_1)] + (-1)^{k-1} (-a_0) &\sidecomment{} \\
				&= (-1)^k (t^k + a_{k-1} t^k + \cdots + a_2 t^2 + a_1 t) + (-1)^k a_0 &\sidecomment{} \\
				&= (-1)^k (t^k + a_{k-1} t^k + \cdots + a_2 t^2 + a_1 t + a_0). \qedhere
			\end{aligned}\]
		\end{proof}
	
	
		\biggerskip
		\labeledTheorem{\textbf{(Cayley-Hamilton Theorem.)} If $p(t)$ is the characteristic polynomial of \textbf{any} linear operator $T$ over a finite-dimensional vector space, then $p(T)$ is 0, the zero operator $T_0$.}{any-linear-operator-on-finite-vector-space-satisfies-its-characteristic-polynomial}
		\begin{proof}
			Let ${ T: V \longmapsto V }$ be a linear operator over a finite-dimensional vector space. Then for an arbitrary ${ \v \neq \0 \in V }$ there exists an associated $T$-cyclic subspace $W$ with basis ${ \{ \v, T\v, \dots, T^{k-1}\,\v \} }$ and we have
			\[ T^k\,\v = -a_0\v - a_1 T\v - \cdots - a_{k-1} T^{k-1}\,\v \]
			for some scalars ${ a_0,a_1,\dots,a_{k-1} }$.\\
			
			By \autoref{prop:characteristic-polynomial-of-T-cyclic-subspace} then, the characteristic polynomial of $W$ is
			\[ p_w(t) = (-1)^k (t^k + a_{k-1} t^{k-1} + \cdots + a_1 t + a_0) \]
			and then,
			\[ p_w(T)\v = (-1)^k (T^k\,\v + a_{k-1} T^{k-1}\,\v + \cdots + a_1 T\v + a_0\v) = \0. \]
			But also, \autoref{prop:char-poly-of-restriction-to-invariant-space-divides-char-poly-of-T} tells us that $p_w(t)$ divides $p(t)$, the characteristic polynomial of $T$. Therefore,
			\[ p(T)\v = (p_w(T) \cdot q(T))\v = (q(T) \cdot p_w(T))\v = q(T)(p_w(T)\v) = q(T)(\0) = \0. \]
			\note{Note the commutativity of the polynomial factors for polynomials in a single linear operator (see: \ref{defn:polynomials-of-linear-operators}).}
			In this way, $p(T)$ has been shown to map any arbitrary non-zero vector in the space to the zero vector and, since it is a linear operator (by the definition of a polynomial of a linear operator \ref{defn:polynomials-of-linear-operators}), it must also map the zero vector to the zero vector. It therefore follows that ${ p(T) = 0 = T_0 }$, the zero operator.
		\end{proof}
	
	
		\bigskip
		\labeledProposition{Let $T$ be a linear operator over a finite-dimensional vector space $V$ and let
			\[ V = W_1 \oplus W_2 \oplus \cdots \oplus W_n \]
			where each $W_i$, for ${ 1 \leq i \leq n }$, is a $T$-invariant subspace of $V$. Then, if $p(t)$ is the characteristic polynomial of $T$ while $p_i(t)$ is the characteristic polynomial of $T_{W_i}$ we have,
			\[ p(t) = p_1(t) p_2(t) \cdots p_n(t). \]
		}{}
		\begin{proof}
			This proof proceeds by induction on the number of invariant subspaces $n$ with base case ${ n = 2 }$.\\
			
			For ${ n = 2 }$, we have ${ W_1 \oplus W_2 = V }$ and if $B_1$ is a basis for $W_1$ and $B_2$ is a basis of $W_2$ then
			\[ B = B_1 \cup B_2 \]
			is a basis of $V$. If we form the matrix of $T$ \wrt $B$ then we get
			\[ 
				A = \begin{bmatrix}
						A_{W_1} & 0\\
						0 & A_{W_2}
					\end{bmatrix}.
			\]
			Let ${ k_1 = \dim W_1, k_2 = \dim W_2 }$, then
			\[\begin{aligned}
				&& \det (A - tI) &= \det (A - tI_{k_1}) \cdot \det (A - tI_{k_2}) \\
				&\iff & p(t) &= p_1(t) p_2(t).
			\end{aligned}\]
			So the proposition is proven for the base case ${ n = 2 }$.\\
			
			For the induction step, assume that the proposition holds for some ${ n - 1 \geq 2 \in \N{} }$. Then
			\[ V = W_1 \oplus W_2 \oplus \cdots \oplus W_n = (W_1 \oplus W_2 \oplus \cdots \oplus W_{n-1}) \oplus W_n. \]
			If we let
			\[ W' = W_1 \oplus W_2 \oplus \cdots \oplus W_{n-1} \]
			then $W'$ is a $T$-invariant subspace of $V$ whose characteristic polynomial, by the induction hypothesis, takes the form
			\[ p_{W'}(t) = p_1(t) p_2(t) \cdots p_{n-1}(t) \]
			but we also have 
			\[ W' \oplus W_n = V \]
			which, by the base case proof means that the characteristic polynomial of $T$ can be expressed as
			\[ p(t) = p_{W'}(t) p_n(t) = p_1(t) p_2(t) \cdots p_{n-1}(t) p_n(t). \]
		\end{proof}
	
			
		
		\sep
		\subsubsection{Examples of Diagonalization using the Characteristic Polynomial}
		\begin{exe}
			\ex{Let the real-valued matrix $A$ be,
				\[ A = \begin{bmatrix}
						4 & 0 & 4\\
						0 & 4 & 4\\
						4 & 4 & 8
						\end{bmatrix}.
				\]
				Constructing the characteristic polynomial,
				\begin{align*}
				\lvert{A - \lambda I}\rvert &= \begin{vmatrix}
													4 - \lambda & 0 & 4\\
													0 & 4 - \lambda & 4\\
													4 & 4 & 8 - \lambda
													\end{vmatrix} \\
				&= (4 - \lambda)\begin{vmatrix}4 - \lambda & 4\\4 & 8 - \lambda\end{vmatrix} + 
										4\begin{vmatrix}0 & 4 - \lambda\\4 & 4\end{vmatrix} \\
				&= (4 - \lambda)((4 - \lambda)(8 - \lambda) - 16 - 16)\\
				&= (4 - \lambda)(\lambda^2 - 12\lambda)\\
				&= (4 - \lambda)\lambda(\lambda - 12).
				\end{align*}
				So the eigenvalues are 4,0 and 12. To find an eigenvector for 4 we need to solve the equation ${ (A - 4I)\x = \0 }$ so we construct the matrix,
				\[ A - 4I = \begin{bmatrix}
							0 & 0 & 4\\
							0 & 0 & 4\\
							4 & 4 & 4
							\end{bmatrix}
				\]
				and then find the nullspace of this matrix using row reduction,
				
				\begin{align*}
				\begin{bmatrix}
				0 & 0 & 4\\
				0 & 0 & 4\\
				4 & 4 & 4
				\end{bmatrix} &=
				\begin{bmatrix}
				1 & 1 & 1\\
				0 & 0 & 1\\
				0 & 0 & 1
				\end{bmatrix} \\
				&= \begin{bmatrix}
				1 & 1 & 0\\
				0 & 0 & 1\\
				0 & 0 & 0
				\end{bmatrix}
				\end{align*}
				which gives ${ x_3 = 0 }$ and one free variable $x_2$. So
				\[ \x = \begin{bmatrix}-t \\ t \\ 0\end{bmatrix} = t\begin{bmatrix}-1 \\ 1 \\ 0\end{bmatrix} \hspace{30pt} t \neq 0 \in \R{}. \]
				\note{Note that ${ t \neq 0 }$ because eigenvectors are nonzero by definition.}
			}
			\biggerskip
			\ex{Let ${ T : \R{4} \longmapsto \R{4} }$ be defined as,
				\[ T(a,b,c,d) = (a + b + 2c - d, \, b + d, \, 2c - d, \, c + d). \]
				The subspace ${ W = \setc{(x,y,0,0)}{x,y \in \R{}} }$ is a $T$-invariant subspace of $\R{4}$ as,
				\[ T(x,y,0,0) = (x + y, \, y, \, 0, \, 0). \]
				Clearly, the standard basis of the $xy$-plane, ${ \{\e{1}, \e{2}\} }$ is a basis of $W$. This basis of $W$ can be extended to the standard basis of $\R{4}$ and the matrix of $T$ \wrt this basis is
				\[ A = 	\begin{bmatrix}
							1 & 1 & 2 & -1\\
							0 & 1 & 0 & 1 \\
							0 & 0 & 2 & -1\\
							0 & 0 & 1 & 1
						\end{bmatrix} 
				\]
				which contains the matrix for the restriction of $T$ to $W$ \wrt to the standard basis of the $xy$-plane in the top left corner,
				\[ 
					A_w =
					\begin{bmatrix}
						1 & 1 \\
						0 & 1
					\end{bmatrix}.
				\]
				So, we can see that the characteristic polynomial of $A_w$ divides that of $A$ as,
				\[ \det (A - tI_4) = \det (A_w - tI_2) \, \cdot \, 
														\begin{vmatrix}
															2 & -1\\
															1 & 1
														\end{vmatrix}.
				\]
			}
			\biggerskip
			\ex{Continuing the example \ref{ex:simple-T-cyclic-subspace}, we have ${ T: \R{3} \longmapsto \R{3} }$ defined by,
				\[ T(a, b, c) = (-b + c, \, a + c, \, 3c) \]
				and the $T$-cyclic subspace generated by $\e{1}$,
				\[ W = \operatorname{span} \{ \e{1}, \e{2} \} = \setc{(x,y,0)}{x,y \in \R{}}. \]
				The  restriction of $T$ to $W$ is
				\[ T_w = T(a, b) = (-b, a). \]
				If we form the matrix of $T_w$
				\[ A_w = [T_w(\{ \e{1}, \e{2} \})] = 
							\begin{bmatrix}
								0 & -1\\
								1 & 0
							\end{bmatrix},
				\]
				then the characteristic polynomial generated by the matrix method is
				\[
					\begin{vmatrix}
						-t & -1\\
						1 & -t
					\end{vmatrix} = t^2 + 1.
				\]
				Alternatively, we could apply \autoref{prop:characteristic-polynomial-of-T-cyclic-subspace} which tells us that, since in $W$ we have
				\[ T^2\,\e{1} = T(T\e{1}) = T\e{2} = -\e{1} \]
				then the characteristic polynomial of $T_w$ is
				\[ p(t) = (-1)^2 (t^2 + 1) = t^2 + 1. \]
			}
		\end{exe}
	
		
		\sep
		\subsubsection{Repeated Eigenvalues}
		\bigskip
		It's not every matrix that is diagonalizable because we can only form a diagonal matrix when there is a complete set of eigenvectors of the matrix that form a basis of the domain of the matrix. However, if we refer to the Fundamental Theorem of Algebra \autoref{theo:fundamental-theorem-of-algebra} we see that there will always be (when counted with multiplicity) $n$ roots of a degree-$n$ non-constant polynomial with complex coefficients. So, if we consider our matrix to be defined over the complex field, then we will always have, counted with multiplicity, $n$ roots of a characteristic polynomial of a ${ n \times n }$ matrix. The reason is that, when we don't have a complete eigenbasis, we have one or more roots of the characteristic polynomial with a multiplicity greater than one -- or, in other words, repeated roots.\\
		
		Consider, for example, the matrix
		\[
			A = \begin{bmatrix}
					a & 1\\
					0 & a
				\end{bmatrix} 
		\]
		that results in the characteristic polynomial
		\[ t^2 - 2at + a^2 = 0 \implies (t - a)(t - a) = (t - a)^2 = 0. \]
		
		This implies that we have a single eigenvalue ${ t = a }$, repeated twice. In this case, we cannot form an eigenbasis of the space and a diagonalized matrix of $A$. However, we can still find two eigenvectors, both of which will be in the eigenspace of the eigenvalue ${ t = a }$. The reasoning is as follows: If we find the eigenvector $\v_1$ for the eigenvalue in the usual way,
		\[ (A - aI)\v_1 = 
			\begin{bmatrix}
				0 & 1\\
				0 & 0
			\end{bmatrix}\v_1 = \0 \implies \v_1 = \begin{bmatrix}1\\ 0\end{bmatrix}. 
		\]
		But we also have a second eigenvector $\v_2$ such that,
		\[ (A - aI)^2 \,\v_2 = (A - aI)(A - aI)\v_2 = \0. \]
		We can leverage the first eigenvector to solve this:
		\[ (A - aI)\v_2 = \v_1 \implies (A - aI)[(A - aI)\v_2] = (A - aI)\v_1 = 0. \]
		So, if we can find a vector $\v_2$ such that,
		\[ (A - aI)\v_2 = \v_1 \iff A\v_2 = a\v_2 + \v_1 \]
		then $\v_2$ is also a kind of eigenvector with eigenvalue $a$. This kind of eigenvector is known as a \textit{generalized eigenvector}.\\
		
		In the example here it is easy to see that,
		\[
			\begin{bmatrix}
				0 & 1\\
				0 & 0
			\end{bmatrix}\v_2 = \begin{bmatrix}1\\ 0\end{bmatrix} \implies \v_2 = \begin{bmatrix}0\\ 1\end{bmatrix}
		\]
		and together, $\v_1$ and $\v_2$ form a basis of the two-dimensional domain of the matrix $A$. In fact,
		\[
			A = \begin{bmatrix}
					a & 1\\
					0 & a
				\end{bmatrix} = 
				\begin{bmatrix}
					0 & 1\\
					1 & 0
				\end{bmatrix} 
				\begin{bmatrix}
					a & 0\\
					1 & a
				\end{bmatrix}
				\begin{bmatrix}
					0 & 1\\
					1 & 0
				\end{bmatrix} 
		\]
		or, alternatively,
		\[
			A = \begin{bmatrix}
					a & 1\\
					0 & a
				\end{bmatrix} = 
				\begin{bmatrix}
					1 & 0\\
					0 & 1
				\end{bmatrix} 
				\begin{bmatrix}
					a & 1\\
					0 & a
				\end{bmatrix}
				\begin{bmatrix}
					1 & 0\\
					0 & 1
				\end{bmatrix}.
		\]
		
		\biggerskip
		\labeledProposition{The multiplicities of the eigenvalues of a linear operator sum to the dimension of the vector space over which it is defined.}{sum-of-eigenvalue-multiplicities-is-dimension-of-space}
		\begin{proof}
			It is easy to see that, if the characteristic polynomial of a linear operator factorizes to
			\[ p(t) = (t - \lambda_1)^{m_1}(t - \lambda_2)^{m_2}\cdots(t - \lambda_k)^{m_k} \]
			then the highest power of $t$ is ${ t^{m_1 + m_2 + \cdots + m_k} }$ and so the degree of the polynomial is ${ \sum_i m_i }$, the sum of the multiplicities of the eigenvalues.\\
			Furthermore, it is also clear from the definition of the characteristic polynomial (\ref{def:characteristic-polynomial}) that -- since the variable $t$ only appears in the diagonal elements of the matrix of the operator -- the degree of the characteristic polynomial is equal to the number of diagonal elements in the matrix; that's to say, the dimension of the space.
		\end{proof}
		
		\bigskip
		\labeledProposition{The dimension of an eigenspace is less than or equal to the multiplicity of the corresponding eigenvalue.}{eigenspace-dimension-leq-multiplicity-of-eigenvalue}
		\begin{proof}
			Let $T$ be a linear operator over a finite-dimensional vector space $V$ of dimension $n$ and let $\lambda$ be an eigenvalue of $T$. Further, let $E_\lambda$ be the eigenspace corresponding to the eigenvalue $\lambda$ and let ${ d = \dim E_\lambda }$ and $m$ be the multiplicity of $\lambda$.\\
			Then, there exists a $d$-length basis for $E_\lambda$. Let this basis be ${ \{ \x_1, \dots, \x_d \} }$ and extend it to a basis of $V$,
			\[ B = \{ \x_1, \dots, \x_d, \x_{d + 1}, \dots, \x_n \}. \]
			If we construct the matrix of $T$ \wrt to this basis of $V$, we see that the block form of the matrix comes out,
			\[
				[T]_B = [T(B)] = 	\begin{bmatrix}
										\lambda I_d & A\\
										0 & C
									\end{bmatrix}
			\]
			so that the characteristic polynomial of $T$ (which is the same against any basis by \autoref{prop:characterstic_poly_independent_of_basis_of_matrix}),
			\[\begin{aligned}
				p(t) &= \det ((\lambda - t)I_d) \, \cdot \, \det (C - t I_{n-d}) \\
				&= (t - \lambda)^d \, \cdot \, \det (C - t I_{n-d}).
			\end{aligned}\]
			Therefore, the multiplicity of $\lambda$ is at least $d$ and so we have ${ m \geq d }$ as required.
		\end{proof}
	
		\bigskip
		\labeledProposition{A linear operator over a finite-dimensional vector space such that the characteristic polynomial factorizes, is diagonalizable iff for each eigenvalue, the dimension of the eigenspace is equal to the multiplicity of the eigenvalue.}{lin-operator-diagonalizable-iff-all-eigenspace-dimensions-equal-to-multiplicity-of-eigenvalue}
		\begin{proof}
			Let $T$ be a linear operator over a finite-dimensional vector space $V$ of dimension $n$ such that the characteristic polynomial of $T$ contains the eigenvalue $\lambda$ with multiplicity $m$.\\
			
			\subheading{Matrix-based proof}
			Assume $T$ is diagonalizable so that there exists a basis such that the matrix of $T$ \wrt to this basis, is diagonal. Since the eigenvalue $\lambda$ in the characteristic polynomial has multiplicity $m$ then there must $m$ columns of the matrix that have $\lambda$ as the diagonal element. Therefore the basis vectors that are eigenvectors of $\lambda$ are the vectors corresponding to these $m$ columns. The eigenspace of $\lambda$ is therefore $m$-dimensional.\\
			
			Conversely, assume that, for every eigenvalue $\lambda$ with multiplicity $m$, the dimension of the eigenspace of $\lambda$ is $m$. Then the number of columns with the sole non-zero element being $\lambda$ in the diagonal position is $m$. Since this applies to every eigenvalue, the total number of columns of the matrix with the only non-zero element being the eigenvalue in the diagonal position is the sum of the multiplicities which, by the definition of the characteristic polynomial, is the number of columns in the matrix and hence, the dimension of $V$.\\
			
			\subheading{Non matrix-based proof}
			Suppose that $T$ is diagonalizable and let $B$ be a basis of $V$ consisting of eigenvectors of $T$. Let the distinct eigenvalues of $T$ be ${ \setc{\lambda_i}{1 \leq i \leq k} }$ and let ${ B_i = B \cap E_{\lambda_i} }$ be the subset of the basis $B$ consisting of eigenvectors of the eigenvalue $\lambda_i$.\\
			If we further let ${ n_i = \cardinality{B_i} }$ be the number of eigenvectors of $\lambda_i$ in the basis $B$ then, because $B_i$ is a linearly independent set in $E_{\lambda_i}$, it follows that
			\[ \dim E_{\lambda_i} \geq n_i. \]
			Also, by \autoref{prop:eigenspace-dimension-leq-multiplicity-of-eigenvalue}, if $m_i$ is the multiplicity of $\lambda_i$, then
			\[ \dim E_{\lambda_i} \leq m_i. \]
			Since $B$ is a basis for $V$,
			\[ \dim V = n \implies \cardinality{B} = n = \sum_i n_i \]
			and also, by \autoref{prop:sum-of-eigenvalue-multiplicities-is-dimension-of-space},
			\[ \sum_i m_i = \dim V = n. \]
			Thus we have,
			\[ n = \sum_i n_i \leq \sum_i \dim E_{\lambda_i} \leq \sum_i m_i = n. \]
			We can therefore conclude that
			\[ \sum_i \dim E_{\lambda_i} = n. \]
			Furthermore, if there were any eigenspace $E_{\lambda_i}$ such that ${ \dim E_{\lambda_i} < m_i }$ then there would have to be another such that the dimension is greater than its multiplicity so that the sum of all the dimensions of the eigenspaces equals the sum of the multiplicities. But, by \autoref{prop:eigenspace-dimension-leq-multiplicity-of-eigenvalue}, there cannot be an eigenspace with dimension greater than its multiplicity. That's to say,
			\[\begin{aligned}
				&& \left( \forall i \logicsep (m_i - \dim E_{\lambda_i}) \geq 0 \right) \, &\land \, \left( \sum_i (m_i - \dim E_{\lambda_i}) = 0 \right) \\
				&\implies & \forall i \logicsep (m_i - \dim E_{\lambda_i}) &= 0. &\sidecomment{} \\
			\end{aligned}\]			
			Therefore, we conclude that,
			\[ \forall i \logicsep \dim E_{\lambda_i} = m_i. \]
			
			\nl
			Conversely, suppose that ${ \forall i \logicsep \dim E_{\lambda_i} = m_i }$. Then, for each eigenspace $E_{\lambda_i}$, there exists a basis $B_i$ such that ${ \cardinality{B_i} = m_i }$. Furthermore, since the eigenspaces are linearly independent (\autoref{prop:distinct-eigenvals-means-independent-eigenvecs}), they form a direct sum
			\[ W = E_{\lambda_1} \oplus E_{\lambda_2} \oplus \cdots \oplus E_{\lambda_k} \]
			where ${ \dim W = \sum_i m_i = n }$.\\
			Therefore, the union of the bases
			\[ B = \bigcup_i B_i \]
			is a linearly independent set of length $n$ and is, therefore, a basis for $V$ consisting of eigenvectors of $T$. It follows, by the definition, that $T$ is diagonalizable.
		\end{proof}

		

% -------------------- break -------------------


		
		\pagebreak
		\subsubsection{Generalized Eigenvectors}\label{sssection:generalized-eigenvectors}
		\bigskip
		\boxeddefinition{The \textbf{algebraic multiplicity} of an eigenvalue is the number of times that it appears as a root of the characteristic polynomial.\\
		
			The \textbf{geometric multiplicity} of an eigenvalue is the dimension of its associated eigenspace.\\
			
			An eigenvalue is said to be \textbf{defective} if its geometric multiplicity is less than its algebraic multiplicity.
		}
		\boxeddefinition{A \textbf{generalized eigenvector of rank (or index) $\bm{r}$} of a matrix $A$ corresponding to an eigenvalue $\lambda$ is defined as a vector $\v$ such that,
			\[ (A - \lambda I)^r \, \v = \0 \eqand (A - \lambda I)^{r-1} \, \v \neq \0. \]
			
			\nl[3]
			An eigenvector is a particular case of a generalized eigenvector with rank 1 as,
			\[ (A - \lambda I)^1 \, \v = \0 \iff (A - \lambda I) \, \v = \0 \]
			and we have the restriction that an eigenvector cannot be $\0$ so also,
			\[ (A - \lambda I)^0 \, \v = \v \neq \0. \]
		}	
		\boxeddefinition{The generalized eigenspace of an eigenvalue $\lambda$ of a linear operator $T$ over a vector space $V$ is the set
			\[ K_\lambda = \setc{\v \in V}{\exists k \in \N{} \suchthat (T- \lambda I)^k \, \v = \0}. \]
		}\label{def:generalized-eigenspace}
	
		\bigskip
		\labeledProposition{The generalized eigenspace of an eigenvalue $\lambda$ of a linear operator $T$ over a vector space $V$ is a $T$-invariant subspace of $V$ that contains the eigenspace of $\lambda$.}{generalized-eigenspace-is-T-invariant-subspace}
		\begin{proof}
			Let $K_\lambda$ be the generalized eigenspace of an eigenvalue $\lambda$. $K_\lambda$ is a subspace of the vector space $V$ because:
			\begin{itemize}
				\item{It contains the zero vector because ${ (T- \lambda I)^k \, \0 = \0 }$ for any ${ k \in \N{} }$;}
				\item{If $\v_1,\v_2$ are members of $K_\lambda$ such that
					\[ (T- \lambda I)^p \, \v_1 = \0 = (T- \lambda I)^q \, \v_2 \]
					for some ${ p,q \in \N{} }$, then
					\[ (T- \lambda I)^{p+q} \, (\v_1 + \v_2) = (T- \lambda I)^q \, (\0) + (T- \lambda I)^p \, (\0) = \0. \]
				}
			\end{itemize}
			$K_\lambda$ is $T$-invariant because if $\v$ is a member of $K_\lambda$ with index $p$ then,
			\[ (T- \lambda I)^p \, \v = \0 \iff (T- \lambda I)^p \, T\v = T[(T- \lambda I)^p \, \v] = T\0 = \0. \]
			Therefore ${ \v \in K_\lambda \implies T\v \in K_\lambda }$.\\
			
			Finally, $K_\lambda$ clearly contains the eigenspace of $\lambda$ as this is just
			\[ \setc{\v \in V}{(T- \lambda I)^k \, \v = \0} \]
			where ${ k = 1 }$.
		\end{proof}
	
		\bigskip
		\labeledProposition{If $T$ is a linear operator on a vector space $V$ with eigenvalue $\lambda$ then $T$-invariance is equivalent to ${ (T - \lambda I) }$-invariance.}{T-invariance-is-equiv-to-T-minus-lambda-invariance}
		\begin{proof}
			Let $W$ be a $T$-invariant subspace of $V$. Then, for any ${ \w \in W }$,
			\[ T\w \in W, \; \w \in W \implies T\w - \lambda \w = (T - \lambda I)\w \in W.  \]
			Conversely, if $W$ is a $(T - \lambda I)$-invariant subspace of $V$, then, for any ${ \w \in W }$,
			\[ (T - \lambda I)\w \in W, \; \w \in W \implies T\w - \lambda \w + \lambda \w = T\w \in W. \]
		\end{proof}
	
		\bigskip
		\labeledProposition{If $T$ is a linear operator on a vector space $V$ and $p(t)$ is a polynomial with coefficients in the same field as $V$, then $T$-invariance is equivalent to $p(T)$-invariance.}{T-invariance-is-equiv-to-any-polynomial-invariance}
		\begin{proof}
			\TODO{This is easily shown by expanding the polynomial to monomial terms that are a linear combination of the basis vectors of a $T$-cyclic space.}
		\end{proof}
	
		\bigskip
		\labeledProposition{If a vector $\v_r$ is a generalized eigenvector of rank $r$ corresponding to the eigenvalue $\lambda$ of the linear transformation $T$ then,
			\[ \v = (T - \lambda I)^{r-1} \, \v_r \]
			is an eigenvector of $T$ with eigenvalue $\lambda$.
		}{gen-eigenvec-chain-initial-vector-is-eigenvector}
		\begin{proof}
			By the definition of a generalized eigenvector of rank $r$ we have,
			\[ (T - \lambda I)^r \, \v_r = \0 \iff (T - \lambda I)(T - \lambda I)^{r-1} \, \v_r = \0 \]
			but also
			\[ (T - \lambda I)^{r-1} \, \v_r \neq \0 \]
			which also implies that
			\[ (T - \lambda I)^m \, \v_r \neq \0 \]
			for any ${ m < r-1 }$. It therefore follows that also,
			\[ (T - \lambda I) \, \v_r \neq \0. \]
			So we must have,
			\[ (T - \lambda I)^{r-1} \, \v_r = \v \suchthat (T - \lambda I)\v = \0. \]
			Therefore ${ \v = (T - \lambda I)^{r-1} \, \v_r }$ is an eigenvector of the transformation $T$.
		\end{proof}
	
		\bigskip
		\labeledProposition{The generalized eigenspaces of distinct eigenvalues are linearly independent spaces.}{distinct-eigenvalue-gen-eigenspaces-are-lin-independent}
		\begin{proof}
			Let $T$ be a linear operator on a (not necessarily finite) vector space defined over a field $\F{}$, and let ${ \setc{\lambda_i}{1 \leq i \leq k} }$ be a (not necessarily exhaustive) set of distinct eigenvalues of $T$. Let $K_{\lambda_i}$ be the generalized eigenspace corresponding to the eigenvalue $\lambda_i$ and let ${ \v_i \in K_{\lambda_i} }$ be an arbitrary vector in the generalized eigenspace of $\lambda_i$. The proposition is proven if it is shown that
			\[ \v_1 + \v_2 + \cdots + \v_k = \0 \implies \v_1, \v_2, \dots, \v_k = \0. \]
			
			Following a proof by induction on the number of distinct eigenvalues $k$, if we take ${ k = 1 }$ as the base case, the proposition holds trivially as
			\[ \v_1 = \0 \implies \v_1 = \0. \]
			
			\nl[2]
			For the induction step, assume that the proposition holds for some ${ k - 1 \geq 1 \in \N{} }$. Now identify two possible cases: ${ \v_k = \0 }$ and ${ \v_k \neq \0 }$.\\
			
			\nl[2]
			Assume ${ \v_k = \0 }$. Then we have,
			\[ \v_1 + \v_2 + \cdots + \v_{k-1} + \v_k = \v_1 + \v_2 + \cdots + \v_{k-1} = \0 \]
			which satisfies the proposition by the induction hypothesis so that
			\[ \v_1 + \v_2 + \cdots + \v_{k-1} = \0 \implies \v_1, \v_2, \dots, \v_{k-1} = \0. \]
			Since, also, ${ \v_k = \0 }$ by assumption, the proposition holds.\\
			
			\nl[3]
			Conversely, assume that ${ \v_k \neq \0 }$ and that we have a linear relation so that,
			\[ \v_1 + \v_2 + \cdots + \v_k = \0 \implies \v_k = -(\v_1 + \v_2 + \cdots + \v_{k-1}). \]
			Let ${ r_i \in \N{} }$ be the rank of each $\v_i$ so that,
			\[ (T - \lambda_i I)^{r_i} \, \v_i = \0 \]
			and, using \autoref{prop:gen-eigenvec-chain-initial-vector-is-eigenvector}, define the eigenvectors $\x_i$,
			\[ \x_i = (T - \lambda_i I)^{r_i - 1} \, \v_i. \]

			The linear relation implies,
			\[\begin{aligned}
				&& \v_1 + \v_2 + \cdots + \v_k &= \0 \\
				&\implies & (T - \lambda_1 I)^{r_1} \, \v_1 + (T - \lambda_1 I)^{r_1} \, \v_2 + \cdots + (T - \lambda_1 I)^{r_1} \, \v_k &= \0 \\
				&\implies & \0 + (T - \lambda_1 I)^{r_1} \, \v_2 + \cdots + (T - \lambda_1 I)^{r_1} \, \v_k &= \0 \\
				&\implies & (T - \lambda_1 I)^{r_1} \, (T - \lambda_2 I)^{r_2} \, \v_2 + \cdots + (T - \lambda_1 I)^{r_1} \, (T - \lambda_2 I)^{r_2} \, \v_k &= \0 \\
				&\implies & (T - \lambda_1 I)^{r_1} \, (T - \lambda_2 I)^{r_2} \, \cdots (T - \lambda_k I)^{r_k} \, \v_k &= \0. \\
			\end{aligned}\]
		
			If we define the polynomial with coefficients in $\F{}$,
			\[ p(t) = (t - \lambda_1)^{r_1}(t - \lambda_2)^{r_2}\cdots(t - \lambda_{k-1})^{r_{k-1}} \]
			then, using \ref{defn:polynomials-of-linear-operators}, the linear relation implies that
			\[ (T - \lambda_1)^{r_1} \, (T - \lambda_2)^{r_2} \, \cdots (T - \lambda_k)^{r_k} \, \v_k = \0 \implies p(T)\v_k = \0. \]
			
			However, we also have the eigenvector $\x_k$,
			\[ \x_k = (T - \lambda_k I)^{r_k - 1} \v_k \]
			and by \autoref{prop:polynomial-of-linear-map-times-eigenvec-is-polynomial-of-eigenval-times-eigenvec}, 
			\[ p(T)\x_k = (\lambda_k - \lambda_1)(\lambda_k - \lambda_2)\cdots(\lambda_k - \lambda_{k-1}) \neq \0. \]
			And so, for the eigenvector $\x_k$,
			\[\begin{aligned}
				&& p(T)\x_k = p(T)(T - \lambda_k I)^{r_k - 1} \, \v_k &\neq \0 \\
				&\iff & (T - \lambda_k I)^{r_k - 1} \, p(T)\v_k &\neq \0 &\sidecomment{commutativity by \ref{defn:polynomials-of-linear-operators}} \\
				&\implies & p(T)\v_k &\neq \0 \\
			\end{aligned}\]
			which contradicts the implications of the linear relation.
		\end{proof}
	
	
		\bigskip
		\labeledProposition{Let $T$ be a linear operator on a finite-dimensional vector space $V$ with distinct eigenvalues ${ \setc{\lambda_i}{1 \leq i \leq k} }$. For each $i$, let $S_i$ be a linearly independent subset of the generalized eigenspace $K_{\lambda_i}$. Then,
			\[ S_i \cap S_j = \emptyset \eqword{for} i \neq j \]
			and ${ S = S_1 \cup S_2 \cup \cdots \cup S_k }$ is a linearly independent subset of $V$.
		}{union-of-lin-ind-subsets-of-gen-eigenspaces-is-lin-ind}
		\begin{proof}
			Suppose that ${ \x \in S_{i} \cap S_{j} }$ for some ${ i \neq j }$. Let ${ \y = -\x }$. Then ${ \x \in K_{\lambda_i}, \, \y \in K_{\lambda_j} }$, and ${ \x + \y = \0 }$. By \autoref{prop:distinct-eigenvalue-gen-eigenspaces-are-lin-independent}, ${ \x = \0 }$, contrary to the fact that $\x$ lies in a linearly independent set. Thus ${ S_i \cap S_j = \emptyset }$.\\
			
			Now suppose that for each $i$
			\[ S_i = \{ \x_{i 1}, \x_{i 2}, \dots, \x_{i n_i} \}. \]
			Then 
			\[ S = \setc{ \x_{i j} }{ 1 \leq j \leq n_i, 1 \leq i \leq k }. \] 
			Consider any scalars $a_{i j}$ such that
			\[ \sum_{i=1}^k \sum_{j=1}^{n_i} a_{i j} \x_{i j} = \0. \]
			For each $i$ let
			\[ \y_i = \sum_{j=1}^{n_i} a_{i j} \x_{i j}. \]
			Then ${ \y_i \in K_{\lambda_i} }$ for each $i$ and ${ \y_1 + \y_2 + \cdots + \y_k = \0 }$. Therefore, by \autoref{prop:distinct-eigenvalue-gen-eigenspaces-are-lin-independent}, ${ \y_i = \0 }$ for all $i$. But for all $i$, $S_i$ is linearly independent by hypothesis. Thus, for each $i$, it follows that ${ a_{i j} = 0 }$ for all $j$. We conclude that $S$ is linearly independent.
		\end{proof}
	
		
	
		\biggerskip
		\group{
		\subsubsubsection{Chains/Cycles of Generalized Eigenvectors}
		\boxeddefinition{Let $T$ be a linear transformation of which $\lambda$ is an eigenvalue and $\v$ is a generalized eigenvector of rank $r$. Then the set
			\[ \{\, (T - \lambda I)^{r-1} \, \v, \, (T - \lambda I)^{r-2} \, \v, \, \dots, \, \v \,\} \]
		is known as a \textbf{chain} or \textbf{cycle} of generalized eigenvectors of length $r$. The elements $(T - \lambda I)^{r-1} \, \v$ and $\v$ are known as the \textbf{initial~vector} and \textbf{end~vector} respectively of the chain or cycle.
		}}
	
		\note{Note that a chain or cycle of generalized eigenvectors is a subset of a generalized eigenspace. There may be more than one chain/cycle in a given generalized eigenspace.}
	
		\biggerskip
		\labeledTheorem{The generalized eigenvectors in a chain are linearly independent.}{gen-eigenvecs-in-a-chain-are-linearly-independent}
		\begin{proof}
			Prove by induction on the length of the chain. 
			
			\subheading{Base case: length ${ \bm{= 2} }$}
			A chain of length 2 corresponding to the eigenvector $\lambda$ of a linear transformation $T$, has the form
			\[ C_2 = \{ (T - \lambda I) \v, \, \v \} \]
			where $\v$ is a generalized eigenvector of rank 2. Then also, by the definition of the generalized eigenvector of rank 2,
			\[ (T - \lambda I)^2 \, \v = \0 \eqand (T - \lambda I) \v \neq \0. \]
			
			Assume, for contradiction, that the chain is not linearly independent. Then there exists a linear relation between the elements of the chain,
			\[ \alpha_1 (T - \lambda I) \, \v + \alpha_2 \v = \0 \]
			where ${ \alpha_1, \alpha_2 \neq 0 }$ are constant scalars. This further implies, defining ${ \alpha = -\frac{\alpha_1}{\alpha_2} }$, that
			\[ \v = \alpha (T - \lambda I) \, \v \implies (T - \lambda I) \v = \alpha (T - \lambda I)^2 \, \v = \0. \]
			But ${ (T - \lambda I) \v = \0 }$ contradicts the definition of $\v$ as a generalized eigenvector of rank 2. It follows then, that a chain of length 2 is linearly independent.
			
			\subheading{Induction step: length ${ \bm{= k} }$}
			A chain of length $k$ corresponding to the eigenvector $\lambda$ of a linear transformation $T$, has the form
			\[ C_k = \{ (T - \lambda I)^{k-1} \, \v, \, (T - \lambda I)^{k-2} \, \v, \, \dots, \, (T - \lambda I) \v, \, \v \} \]
			where $\v$ is a generalized eigenvector of rank $k$. Then also, by the definition of the generalized eigenvector of rank $k$,
			\[ (T - \lambda I)^k \, \v = \0 \eqand (T - \lambda I)^{k-1} \, \v \neq \0. \]
			It follows from the fact that $\v$ is a generalized eigenvector of rank $k$ that ${ \w = (T - \lambda I) \v }$ is a generalized eigenvector of rank ${ k - 1 }$ because,
			\[ (T - \lambda I)^{k-1} \, \w = (T - \lambda I)^k \, \v = \0 \eqand (T - \lambda I)^{k-2} \, \w = (T - \lambda I)^{k-1} \, \v \neq \0. \]
			Therefore
			\[ C_{k-1} = \{ (T - \lambda I)^{k-1} \, \v, \, (T - \lambda I)^{k-2} \, \v, \, \dots, \, (T - \lambda I) \v \} \]
			is a $(k-1)$-length chain and is, by the induction hypothesis, linearly independent.\\
			
			It follows that $C_k$ will be linearly independent iff $\v$ is not in the span of $C_{k-1}$. Assume for contradiction, that $\v$ is indeed in the span of $C_{k-1}$. Then $\v$ can be expressed as a linear combination of the elements of $C_{k-1}$,
			\[ \v = \alpha_1 (T - \lambda I)^{k-1} \, \v + \alpha_2 (T - \lambda I)^{k-2} \, \v + \cdots + \alpha_{k-1} (T - \lambda I) \v. \]
			But this implies that,
			\[\begin{aligned}
				(T - \lambda I)^{k-1} \, \v &= \alpha_1 (T - \lambda I)^{k-2} ((T - \lambda I)^k \, \v) \\
				&\hspace{14pt} + \alpha_2 (T - \lambda I)^{k-3} ((T - \lambda I)^k \, \v) + \cdots + \alpha_{k-1} ((T - \lambda I)^k \, \v) \nn
				&= \alpha_1 (T - \lambda I)^{k-2} \, (\0) + \alpha_2 (T - \lambda I)^{k-3} (\0) + \cdots + \alpha_{k-1} (\0) \nn
				&= \0.
			\end{aligned}\]
			Since ${ (T - \lambda I)^{k-1} \, \v = \0 }$ contradicts the definition of $\v$ as a generalized eigenvector of rank $k$, we can deduce that $\v$ cannot be in the span of $C_{k-1}$.
		\end{proof}
	
		\bigskip
		\labeledTheorem{The generalized eigenvectors in a chain form the basis of a $T$-invariant space.}{chain-of-gen-eigenvecs-is-basis-of-invariant-space}
		\begin{proof}
			Let $C_k$ be a chain/cycle of length $k$ corresponding to the eigenvector $\lambda$ of a linear transformation $T$ so that,
			\[ C_k = \{ (T - \lambda I)^{k-1} \, \v, \, (T - \lambda I)^{k-2} \, \v, \, \dots, \, (T - \lambda I) \v, \, \v \}. \]
			By \autoref{theo:gen-eigenvecs-in-a-chain-are-linearly-independent} we have that $C_k$ is linearly independent so the proposition will be proven if we can show that the space spanned by $C_k$ is $T$-invariant.\\
			
			\note{Note that it may be tempting to use the proof used in \autoref{prop:generalized-eigenspace-is-T-invariant-subspace}:
				For any ${ \x \in C_k }$ we have some ${ m \leq k \in \N{} }$ such that
				\[ (T - \lambda I)^m\,\x = \0. \]
				Therefore, for any ${ n \in \N{} }$,
				\[ (T - \lambda I)^m \, T^n \, \x = T^n \, (T - \lambda I)^m \, \x = T^n \, (\0) = \0. \]
				But this only proves that $T^n \,\x$ is a generalized eigenvector with eigenvalue $\lambda$, it does \textbf{not} prove that it is in the span of the chain/cycle $C_k$. Remember, there may be more than one chain/cycle in a generalized eigenspace.
			}
			
			\[\begin{aligned}
				T\v &= (T - \lambda I) \v + \lambda \v, \\
				T(T - \lambda I) \v &= (T - \lambda I)^2 \, \v + \lambda T\v - \lambda^2 \v \\
									&= (T - \lambda I)^2 \, \v + \lambda (T - \lambda I) \v. \\
			\end{aligned}\]
			For any ${ m \in \N{} }$ such that ${ 1 \leq m \leq k-1 }$,
			\[\begin{aligned}
				T(T - \lambda I)^m\,\v &= (T - \lambda I)(T - \lambda I)^m\,\v + \lambda (T - \lambda I)^m\,\v \\
				&= (T - \lambda I)^{m+1}\,\v + \lambda (T - \lambda I)^m\,\v.\\
			\end{aligned}\]
			When ${ m = k-1 }$ we have the special case that,
			\[\begin{aligned}
				T(T - \lambda I)^{k-1}\,\v &= (T - \lambda I)^k\,\v + \lambda (T - \lambda I)^{k-1}\,\v \\
				&= \0 + \lambda (T - \lambda I)^{k-1}\,\v \\
			\end{aligned}\]
			which reflects the fact that ${ (T - \lambda I)^{k-1}\,\v }$ is an eigenvector.\\
			
			So, we have shown that $T\x$ for any ${ \x \in C_k }$ is in the span of $C_k$. For any ${ n > 1 \in \N{} }$,
			\[\begin{aligned}
				&& T^n\,(T - \lambda I)^m\,\v &= (T - \lambda I)T^{n-1}\,(T - \lambda I)^m\,\v + \lambda (T - \lambda I)^m\,\v \\
				&\iff &  &= T^{n-1}\,(T - \lambda I)^{m+1}\,\v + \lambda (T - \lambda I)^m\,\v &\sidecomment{} \\
			\end{aligned}\]
			In the case that ${ m + 1 = k }$,
			\[ T^{n-1}\,(T - \lambda I)^{m+1}\,\v + \lambda (T - \lambda I)^m\,\v = \0 + \lambda (T - \lambda I)^m\,\v \]
			which is clearly in the span of $C_k$. Otherwise, 
			\[\begin{aligned}
				&& T^{n-1}\,(T - \lambda I)^{m+1}\,\v + \lambda (T - \lambda I)^m\,\v &\in \operatorname{span} C_k \\
				&\iff &  T^{n-1}\,(T - \lambda I)^{m+1}\,\v &\in \operatorname{span} C_k. &\sidecomment{} \\
			\end{aligned}\]
			Therefore, by induction, the space spanned by $C_k$ is shown to be $T$-invariant.
		\end{proof}

		
		\bigskip
		\labeledProposition{Let $T$ be a linear operator on a finite-dimensional vector space $V$ and let $Z_i$ for ${ 1 \leq i \leq q }$ be cycles of generalized eigenvectors of $T$ corresponding to a single common eigenvalue $\lambda$.\\
		If $\y_i$ is the initial vector in cycle $i$ then, if the set ${ \setc{ \y_i }{ 1 \leq i \leq q } }$ is linearly independent then the sets $Z_i$ are disjoint and their union
			\[ Z = \bigcup_{i=1}^q Z_i \]
		is linearly independent.
		}{for-given-eigenvalue-the-union-of-cycles-of-linearly-independent-eigenvectors-is-a-linearly-independent-set}
		\begin{proof}
			To show that the cycles $Z_i$ are disjoint, assume for contradiction that, for ${ i \neq j }$,
			\[\begin{aligned}
				&& &\exists \x \in Z_i \cap Z_j \\
				&\implies & &\exists r_i,r_j \in \N{} \logicsep [ \y_i = (T - \lambda I)^{r_i - 1}\,\x ] \land [ \y_j = (T - \lambda I)^{r_j - 1}\,\x ].
			\end{aligned}\]
			If we now assume \WLOG that ${ r_j \geq r_i }$ so that ${ r = r_j - r_i \geq 0 }$, we have
			\[ \y_j = (T - \lambda I)^r\,\y_i = (T - \lambda I)^{r-1}\,(T - \lambda I) \y_i = \0 \]
			where ${ (T - \lambda I) \y_i = \0 }$ because $\y_i$ is an eigenvector by hypothesis. But $\y_j$ is also an eigenvector by hypothesis and therefore, by definition, cannot be $\0$. It therefore follows that
			\[ \centernot\exists  \x \in Z_i \cap Z_j \implies Z_i \cap Z_j = \emptyset \]
			and the cycles $Z_i$ are disjoint.\\
			
			To show that $Z$ is linearly independent, we will use induction on ${ n = \cardinality{Z} }$, the cardinality of the set $Z$. If ${ n = 1 }$ then the proposition holds trivially because the cycles, by definition, contain only non-zero vectors.\\
			
			Assume that, for some ${ n > 1 }$, the proposition holds for any number less than $n$. We will show that $Z$ is a basis of $W$, the space that it generates. Since $W$ is clearly $(T - \lambda I)$-invariant, we can define the restriction of $(T - \lambda I)$ to $W$ and denote it $(T - \lambda I)_W$.\\
			
			Firstly, note that the image under $(T - \lambda I)$ of $Z_i$,
			is equal to $Z_i$ but with the end vector swapped for $\0$. So, if we define,
			\[ Z_i' = (T - \lambda I) Z_i \setminus \{\0\} \]
			then their union
			\[ Z' = \bigcup_{i=1}^q Z_i' \]
			contains all the non-zero images under $(T - \lambda I)$ of the vectors in $Z$ and therefore spans the range of $(T - \lambda I)_W$. Furthermore, this is also a disjoint union because, for each $i$, ${ Z_i' \subset Z_i }$ and the sets $Z_i$ are disjoint. So we can deduce that
			\[\begin{aligned}
				\cardinality{Z'} &= \sum_{i=1}^q \cardinality{Z_i'} \nn
				&= \sum_{i=1}^q (\cardinality{Z_i} - 1) &\sidecomment{} \nn
				&= \left( \sum_{i=1}^q \cardinality{Z_i} \right) - q \nn
				&= \cardinality{Z} - q \\
				&= n - q.
			\end{aligned}\]
			Since the cardinality of $Z'$ is less than $n$ and it consists of cycles of generalized eigenvectors with the same set of linearly independent eigenvector initial vectors as $Z$, we can use the induction hypothesis to deduce that $Z'$ is linearly independent. Since $Z'$ spans the range of $(T - \lambda I)_W$ and is linearly independent, it is therefore a basis of the range of $(T - \lambda I)_W$ which, in turn, must therefore be of dimension ${ n - q }$. Meanwhile, the kernel of $(T - \lambda I)_W$ is the set of eigenvectors ${ \setc{ \y_i }{ 1 \leq i \leq q } }$ and so has dimension $q$.\\
			
			The space $W$ is generated by $Z$ -- a finite set of vectors -- and is thus finite-dimensional. So we can employ the dimension formula (\autoref{theo:linear_map_dimension_formula}) to deduce that,
			\[ \dim W = \operatorname{rank} (T - \lambda I)_W + \operatorname{nullity} (T - \lambda I)_W = (n - q) + q = n. \]
			
			Since, by the induction hypothesis, ${ \cardinality{Z} = n }$ and this is equal to the dimension of $W$, the space that $Z$ generates, it follows that $Z$ is a basis of the space $W$. Therefore $Z$ is linearly independent.
		\end{proof}	
		
		
		
		
		
		\biggerskip
		\subsubsection{Jordan Canonical Form}
		\bigskip
		\boxeddefinition{A \textbf{Jordan block} is a matrix (appearing as a block within a larger matrix) with the form,
			\[
				\begin{bmatrix}
					\lambda & 1 &  &  &  &  & \\
					& \lambda & 1 &  &  &  &  \\
					& & \ddots & \ddots &  & &  \\
					& & & \ddots & \ddots & & \\
					& & & & \ddots & \ddots & \\
					& & & & &  \lambda & 1
				\end{bmatrix} 
			\]
			or its transpose (depending on the ordering of the basis vectors).
		}
		\boxeddefinition{A \textbf{Jordan} matrix or \textbf{Jordan form} or \textbf{Jordan Canonical form} matrix, is a matrix consisting solely of jordan blocks.}
		\boxeddefinition{Let $T$ be a linear operator on a vector space $V$ such that the characteristic polynomial of $T$ factorizes. A \textbf{Jordan Canonical Basis} is a basis for $V$ that is a disjoint union of cycles of generalized eigenvectors of $T$.}\label{def:jordan-canonical-basis}
		
		\biggerskip
		\note{\textbf{Preliminary to Proof of Existence Jordan Canonical Basis}\\\\
			If ${ T(x, y) = (ax + y, ay) }$ then, \wrt the standard basis, $T$ is represented in matrix form as,
			\[
			\begin{bmatrix}
				a & 1\\
				0 & a
			\end{bmatrix}.
			\]
			The $x$-axis is $T$-invariant because ${ T(x, 0) = (ax, 0) }$. So, the characteristic polynomial of the restriction of $T$ to the $x$-axis divides the characteristic polynomial of $T$. The characteristic polynomial of $T$ is ${ p(t) = (a - t)^2 }$ so that ${ (T - a I)^2\,\v = \0 }$ for any $\v$ in the space.\\\\
			Let ${ W = R(T - a I) }$ and define the restriction of $T$ to $W$ as $T_W$ so that, for ${ \w \in W }$,
			\[ (T - a I)\w = \0. \]
			What this looks like in matrix form is
			\[
			(T - a I) = \begin{bmatrix}
				0 & 1\\
				0 & 0
			\end{bmatrix}  \eqand
			W = R(T - a I) = \alpha \begin{bmatrix}1\\0\end{bmatrix} \eqword{for} \alpha \in \F{}.
			\]
			So, clearly, the jordan basis for $T_W$ is the single vector ${ (1,0)^T = \e{1} }$ and this is equal to the nullspace ${ N(T - a I) }$ meaning that we have a cycle of length 2 associated with the eigenvalue $a$.\\\\
			If, on the other hand, we had ${ T(x, y) = (ax, ay) }$ with matrix \wrt the standard basis,
			\[
			\begin{bmatrix}
				a & 0\\
				0 & a
			\end{bmatrix},
			\]
			then ${ W = R(T - a I) = \0 }$ with basis ${ \emptyset = \{\} }$ and the nullspace ${ N(T - a I) }$ is the whole space (in this case the $xy$-plane). So, in this case, we have no cycles of length > 1 associated with the eigenvalue $a$ and, instead, we have two lone eigenvectors.\\\\
			The converse case is ${ T(x, y) = (ax, by) }$ with matrix \wrt the standard basis,
			\[
			\begin{bmatrix}
				a & 0\\
				0 & b
			\end{bmatrix}.
			\]
			Then 
			\[ 
			(T - a I) = \begin{bmatrix}
				0 & 0\\
				0 & b - a
			\end{bmatrix}   \eqand
			W = R(T - a I) = \alpha \begin{bmatrix}0\\1\end{bmatrix} \eqword{for} \alpha \in \F{}.
			\] 
			In this case, the nullspace ${ N(T - a I) \centernot\subseteq R(T - a I) }$ and there are no cycles of length > 1; just one lone eigenvector associated with the eigenvalue $a$. A basis of the space is completed by another eigenvector for a different eigenvalue $b$.
		}
		
		\bigskip
		\labeledTheorem{(\textbf{Jordan Canonical Basis.}) Let $T$ be a linear operator on a vector space $V$ such that the characteristic polynomial of $T$ factorizes. Then there exists a Jordan canonical basis for $T$.}{jordan-basis-existence}
		\begin{proof}
			Let $V$ have dimension $n$. The proof will follow an induction on $n$.\\
			
			For ${ n = 1 }$, the proposition holds trivially as any vector in the space is an eigenvector that spans the space.\\
			
			Assume that the proposition holds for any vector space of dimension less than ${ n = \dim V > 1 }$. Let ${ \setc{\lambda_i}{1 \leq i \leq k} }$ be the set of distinct eigenvalues of $T$. We arbitrarily select the first eigenvalue $\lambda_1$ and define (where $R$ denotes the range),
			\[ W = R(T - \lambda_1 I), \; r = \operatorname{rank} (T - \lambda_1 I) = \dim W. \]
			By \autoref{prop:range-nullspace-origin-and-whole-space-are-all-T-invariant}, $W$ is $(T - \lambda_1 I)$-invariant and therefore, by \autoref{prop:T-invariance-is-equiv-to-T-minus-lambda-invariance}, $W$ is also $T$-invariant. So we can define $T_W$, the restriction of $T$ to $W$.
			
			\note{For all ${ \v \in V }$ we have,
				\[ (T - \lambda_1 I)^{m_1}(T - \lambda_2 I)^{m_2}\cdots(T - \lambda_k I)^{m_k}\,\v = \0 \]
				and, since
				\[ \forall \w \in W \logicsep \exists \v \in V \suchthat \w = (T - \lambda_1 I)\v, \] 
				for all ${ \w \in W }$ we have,
				\[ (T - \lambda_1 I)^{m_1 - 1}(T - \lambda_2 I)^{m_2}\cdots(T - \lambda_k I)^{m_k}\,\w = \0. \]
			}
			
			Then, by \autoref{prop:char-poly-of-restriction-to-invariant-space-divides-char-poly-of-T}, the characteristic polynomial of $T_W$ divides that of $T$ and so, we can deduce, also factorizes. Furthermore, $T_W$ is a linear operator over an $r$-dimensional vector space where ${ r < n }$ because, by the definition of $\lambda_1$ as an eigenvalue of $T$, ${ (T - \lambda_1 I) }$ must have a non-trivial nullspace. So $T_W$ is a linear operator over a vector space of dimension less than $n$ such that the characteristic polynomial of $T_W$ factorizes. Therefore, we can use the induction hypothesis to assert the existence of a jordan canonical basis for $T_W$ -- which we shall denote $J_W$.\\
			
			Let $S_i$ for ${ 1 \leq i \leq k }$ be the generalized eigenvectors in $J_W$ corresponding to the eigenvalue $\lambda_i$. Since $S_i$ is a subset of a jordan basis, it is therefore a linearly independent set of disjoint cycles of generalized eigenvectors. Since it is the subset corresponding to $\lambda_i$, it is therefore a linearly independent set of disjoint cycles of generalized eigenvectors corresponding to $\lambda_i$.\\
			
			Let ${ \setc{Z_j}{1 \leq j \leq p} }$ be the set of cycles of generalized eigenvectors whose disjoint union is equal to $S_1$. For each cycle, let
			\[ Z_j' = \{\y_j\} \cup Z_j \eqword{for some} \y_j \in V \]
			such that ${ (T - \lambda_1 I)\y_j }$ is the end vector of $Z_j$. Such a $\y_j$ is guaranteed to exist because 
			\[ Z_j \subseteq W = R(T - \lambda_1 I). \]
			Then $Z_j'$ is also a cycle of generalized eigenvectors of $T$ corresponding to $\lambda_1$.\\
			%\autoref{prop:distinct-eigenvalue-gen-eigenspaces-are-lin-independent}
			Now, let ${ I = \setc{z_j}{1 \leq j \leq p} }$ be the set of initial vectors of the cycles $Z_j$. Since ${ I \subseteq J_W }$ and $J_W$ is a basis and hence linearly independent, $I$ is linearly independent. Furthermore, $I$ is a subset of the nullspace ${ N(T - \lambda_1 I) }$.
			\note{The nullspace ${ N(T - \lambda_1 I) }$ comprises the eigenvectors of $T$ for the eigenvalue $\lambda_1$ but these are only in the space $W$ -- the range of ${ T - \lambda_1 I }$ -- if they are the initial vectors in a cycle of length > 1. Hence ${ I \subseteq N(T - \lambda_1 I) }$.}
			
			Next, we extend $I$ to a basis for the nullspace which, by the dimension formula for finite-dimensional vector spaces (\autoref{theo:linear_map_dimension_formula}), has dimension ${ n - r }$,
			\[ I_N = \{ z_1, \dots, z_p, z_{p+1}, \dots, z_{n-r} \}. \]
			\note{Here,
				\[ p = \dim (R(T - \lambda_1 I) \cap N(T - \lambda_1 I)). \]
				If there are no cycles (of length > 1) for the eigenvalue $\lambda_1$, then ${ p = 0 }$ and the basis of the nullspace of $N(T - \lambda_1 I)$ is comprised solely of ${ n - r }$ eigenvectors. Otherwise ${ z_1, \dots, z_p }$ are the eigenvectors that are initial vectors of cycles of generalized eigenvectors for $\lambda_1$ and they are extended to a basis for $N(T - \lambda_1 I)$ by adding the eigenvectors that are not in cycles (of length > 1).
			}
			If ${ p < n - r }$ (i.e. if there are eigenvectors that are not in cycles of length > 1), then ${ z_{p+1}, \dots, z_{n-r} }$ consists of eigenvectors not in $W$. These eigenvectors can be considered as cycles of length 1 and used to extend the cycles $Z_j'$ such that, 
			\[ Z_j' = \{z_j\} \eqword{for} p+1 \leq j \leq n-r. \]
			So, ${ Z' = \setc{Z_j'}{1 \leq j \leq n-r} }$ is a collection of disjoint cycles of generalized eigenvectors corresponding to $\lambda_1$. Let
			\[ S_1' = \bigcup_{j=1}^{n-r} Z_j' = Z_1' \cup Z_2' \cup \cdots \cup Z_{n-r}'. \]
			Then, since the initial vectors of the cycles in $Z'$ form a linearly independent set, by \autoref{prop:for-given-eigenvalue-the-union-of-cycles-of-linearly-independent-eigenvectors-is-a-linearly-independent-set}, $S_1'$ is a linearly independent disjoint union of the cycles. Furthermore,
			\[ \cardinality{S_1'} = \cardinality{S_1} + (n - r). \]
			If we define,
			\[ J = S_1' \cup S_2 \cup \cdots \cup S_k \]
			then $J$ is linearly independent by \autoref{prop:union-of-lin-ind-subsets-of-gen-eigenspaces-is-lin-ind} and
			\[ \cardinality{J} = \cardinality{J_W} + (n - r) = r + (n - r) = n. \]
			Therefore $J$ is a basis of $V$ comprising disjoint cycles of generalized eigenvectors of $T$ and, as such, is a jordan canonical basis for $T$.
		\end{proof}
	
	
		\bigskip
		\labeledProposition{Let $T$ be a linear operator on a vector space $V$ such that the characteristic polynomial of $T$ factorizes. Then, for each eigenvalue, the dimension of its generalized eigenspace is equal to its multiplicity.
		}{dim-of-gen-eigenspace-equal-to-multiplicity}
		\begin{proof}
			Let ${ \setc{\lambda_i}{1 \leq k \leq k} }$ be the distinct eigenvalues of $T$ with the corresponding multiplicities ${ \setc{m_i}{1 \leq k \leq k} }$. By \autoref{theo:jordan-basis-existence}, there exists a jordan basis for $T$. Let this basis be $J$ and the matrix of $T$ \wrt $J$,
			\[ [T]_J = \inv{[J]}[T(J)]. \]
			Since $[T]_J$ is in Jordan Normal Form, it is upper-triangular and, therefore, the multiplicity of any eigenvalue $\lambda_i$ is equal to the number of columns of the matrix that have $\lambda_i$ as the diagonal element. This, in turn, is equal to the number of vectors in the jordan basis $J$ that are generalized eigenvectors corresponding to the eigenvalue $\lambda_i$.\\
			
			So if, for each $i$, 
			\[ U_i = K_{\lambda_i} \cap J \]
			is the subset of the jordan basis that comprises generalized eigenvectors corresponding to the eigenvalue $\lambda_i$, then
			\[ \forall i \logicsep \cardinality{U_i} = m_i. \]
			
			Since $U_i$ is a linearly independent set in $K_{\lambda_i}$ we must have,
			\[ \cardinality{U_i} \leq \dim K_{\lambda_i} \implies m_i \leq \dim K_{\lambda_i} \]
			and by \autoref{prop:sum-of-eigenvalue-multiplicities-is-dimension-of-space}, we have that
			\[ \sum_i m_i = \dim V. \]
			Furthermore, since the spaces $K_{\lambda_i}$ are linearly independent and the direct sum of the spaces is a subspace of $V$, we must have,
			\[ \sum_i \dim K_{\lambda_i} \leq \dim V. \]
			Therefore,
			\[\begin{aligned}
				&& \dim V = \sum_i m_i \leq \sum_i \dim K_{\lambda_i} &\leq \dim V \\
				&\implies & \sum_i m_i = \sum_i \dim K_{\lambda_i} &= \dim V.
			\end{aligned}\]
			So we can deduce,
			\[\begin{aligned}
				&& \forall i (\dim K_{\lambda_i} - m_i \geq 0) \; &\land \; \sum_i \dim K_{\lambda_i} - m_i = 0 \\
				&\implies & \forall i (\dim K_{\lambda_i} &- m_i = 0).
			\end{aligned}\]
			Therefore, for each $i$, ${ \dim K_{\lambda_i} = m_i }$ as required.	
		\end{proof}
		\begin{corollary}
			If $T$ is a linear operator on a vector space $V$ such that the characteristic polynomial of $T$ factorizes, then $T$ is diagonalizable iff, for each eigenvalue, its eigenspace is equal to its generalized eigenspace.
		\end{corollary}
		\begin{proof}
			For any linear operator such as $T$, by \autoref{prop:dim-of-gen-eigenspace-equal-to-multiplicity}, for each distinct eigenvalue $\lambda$ of $T$ with multiplicity $m$,
			\[ \dim K_\lambda = m. \]
			Also, by \autoref{prop:lin-operator-diagonalizable-iff-all-eigenspace-dimensions-equal-to-multiplicity-of-eigenvalue}, $T$ is diagonalizable iff, 
			\[ \dim E_\lambda = m. \]
			So we can put these two together to say that $T$ is diagonalizable iff
			\[ \dim E_\lambda = \dim K_\lambda. \]
			Furthermore, since $E_\lambda$ is a subspace of $K_\lambda$ (\autoref{prop:generalized-eigenspace-is-T-invariant-subspace}),
			\[ \dim E_\lambda = \dim K_\lambda \implies E_\lambda = K_\lambda. \]
		\end{proof}
		\begin{corollary}
			If $T$ is a linear operator on a vector space $V$ such that the characteristic polynomial of $T$ factorizes, then for every eigenvalue $\lambda$ with multiplicity $m$,
			\[ K_\lambda = N((T - \lambda I)^m) \]
			where $N$ denotes the nullspace.
		\end{corollary}
		\begin{proof}
			If ${ \v \in N((T - \lambda I)^m) }$ then, by the definition of the generalized eigenspace (\ref{def:generalized-eigenspace}), ${ \v \in K_\lambda }$. Therefore ${ N((T - \lambda I)^m) \subseteq K_\lambda }$.\\
			
			Conversely, if ${ \v \in K_\lambda }$ then there exists some minimial ${ p \in \N{} }$ such that,
			\[ (T - \lambda I)^p\,\v = \0. \]
			If we consider the $p$-length cycle whose end vector is this $\v$,
			\[ C = \{ (T - \lambda I)^{p-1}\,\v, \dots, (T - \lambda I)\v, \v \}, \]
			then, by \autoref{theo:gen-eigenvecs-in-a-chain-are-linearly-independent}, the vectors in the cycle $C$ are linearly independent. By \autoref{prop:dim-of-gen-eigenspace-equal-to-multiplicity}, the dimension of $K_\lambda$ is $m$ and, since the dimension is an upper bound on the length of a linearly independent set in the space, we must have ${ p \leq m }$ and so,
			\[ (T - \lambda I)^m\,\v = (T - \lambda I)^{m-p}\,(T - \lambda I)^p\,\v = (T - \lambda I)^{m-p}\,\0 = \0. \]
			It follows then, that ${ \v \in N((T - \lambda I)^m) }$ and so ${ K_\lambda \subseteq N((T - \lambda I)^m) }$.
		\end{proof}
		\begin{corollary}
			If $T$ is a linear operator on a vector space $V$ such that the characteristic polynomial of $T$ factorizes, then $V$ is a direct sum of the generalized eigenspaces of $T$.
		\end{corollary}
		\begin{proof}
			By \autoref{prop:dim-of-gen-eigenspace-equal-to-multiplicity}, for each distinct eigenvalue $\lambda_i$ with multiplicity $m_i$,
			\[ \dim K_{\lambda_i} = m_i. \]
			By \autoref{prop:distinct-eigenvalue-gen-eigenspaces-are-lin-independent}, the spaces $K_{\lambda_i}$ for the different $\lambda_i$, are linearly independent. Therefore if we let $B_i$ be a basis for $K_{\lambda_i}$ then
			\[ B = \bigcup_i B_i \]
			is a disjoint union and the set $B$ is linearly independent because, for ${ \b_i \in B_i }$,
			\[\begin{aligned}
				&& \b_1 + \b_2 + \cdots + \b_k &= \0 \\
				&\iff & \b_1 = \b_2 = \cdots = \b_k &= \0
			\end{aligned}\]
			and for each $\b_i$,
			\[ \b_i = \alpha_1 \b_{i1} + \cdots + \alpha_k \b_{ik} = \0 \implies \alpha_1,\dots\alpha_k = 0. \]
			So $B$ is a linearly independent set of vectors in the space $V$ with
			\[ \cardinality{B} = \sum_i \cardinality{B_i} = \sum_i \dim K_{\lambda_i} = \sum_i m_i. \]
			By \autoref{prop:sum-of-eigenvalue-multiplicities-is-dimension-of-space}, we have,
			\[ \sum_i m_i = \dim V \]
			and so, $B$ is a basis for $V$.\\
			
			Therefore, by \autoref{prop:bases-of-direct-summands-have-disjoint-union-equal-to-basis-of-space}, we have,
			\[ K_{\lambda_1} \oplus K_{\lambda_2} \oplus \cdots \oplus K_{\lambda_k} = V. \qedhere\]
		\end{proof}
		
		
		\sep
		\subsubsubsection{Generalized Eigenvectors and Jordan Blocks}
		Consider a simple case of repeated eigenvalues: a matrix $A$ that simply represents a uniform scaling by $a$, for some constant $a$,
		\[ 
		A = \begin{bmatrix}
			a & 0\\
			0 & a
		\end{bmatrix}.
		\]
		In this case, clearly, the only eigenvalue is ${ \lambda = a }$. From which we obtain,
		\[ (A - aI) = 	\begin{bmatrix}
			0 & 0\\
			0 & 0
		\end{bmatrix}
		\]
		which has a 2-dimensional nullspace equal to the entire vector space over which it operates. So we can use the standard basis as the two eigenvectors. If the diagonalized matrix is $D$ and the change of basis matrix to the eigenbasis is $P$ (which in this case is the identity) then ${ AP = PD }$ which is the matrix equation,
		\[ 
		\begin{bmatrix}
			a & 0\\
			0 & a
		\end{bmatrix}
		\begin{bmatrix}
			1 & 0\\
			0 & 1
		\end{bmatrix} =
		\begin{bmatrix}
			1 & 0\\
			0 & 1
		\end{bmatrix}
		\begin{bmatrix}
			a & 0\\
			0 & a
		\end{bmatrix}.
		\]
		
		\bigskip
		On the other hand, if the diagonal entries of $A$ are distinct so that the matrix becomes a non-uniform, rectangular scaling,
		\[ 
		A = \begin{bmatrix}
			a & 0\\
			0 & b
		\end{bmatrix}
		\]
		we have, in this case, eigenvalues: ${ \lambda_1 = a, \, \lambda_2 = b }$. From which we obtain,
		\[ (A - aI) = 	\begin{bmatrix}
			0 & 0\\
			0 & b-a
		\end{bmatrix} \eqand
		(A - bI) = 	\begin{bmatrix}
			a-b & 0\\
			0 & 0
		\end{bmatrix}	
		\]
		which, each have a 1-dimensional nullspace, the direct sum of them forming the entire vector space. In fact, the eigenvectors are the two standard basis vectors and, similarly to the previous case, we have the following matrix equation,
		\[ 
		\begin{bmatrix}
			a & 0\\
			0 & b
		\end{bmatrix}
		\begin{bmatrix}
			1 & 0\\
			0 & 1
		\end{bmatrix} =
		\begin{bmatrix}
			1 & 0\\
			0 & 1
		\end{bmatrix}
		\begin{bmatrix}
			a & 0\\
			0 & b
		\end{bmatrix}.
		\]
		
		\bigskip
		However, in the case,
		\[ 
		A = \begin{bmatrix}
			a & 1\\
			0 & b
		\end{bmatrix}
		\]
		we have the same eigenvalues ${ \lambda_1 = a, \, \lambda_2 = b }$, but the eigenvector of ${ \lambda_2 = b }$ is not the same.
		\[ (A - aI) = 	\begin{bmatrix}
			0 & 1\\
			0 & b-a
		\end{bmatrix} \eqand
		(A - bI) = 	\begin{bmatrix}
			a-b & 1\\
			0 & 0
		\end{bmatrix}	
		\]
		In this case, the eigenvector of ${ \lambda_2 = b }$ is
		\[ \begin{bmatrix}\frac{1}{b-a}\nn 1\end{bmatrix}. \]
		The reason is that, here, we have a stretching of $b$ in the second dimension (say, the $y$-direction) that also has a component of 1 in the $x$-direction -- which is being stretched by $a$. So, in order to have a vector whose components are \textit{both} stretched by $b$ we need the $x$ component to obey,
		\[ 1 + ax = bx \iff x = 1/(b-a). \] 
		\TODO{relate this to modular arithmetic and resonance}\\
		
		Here the matrix equation is,
		\[ 
		\begin{bmatrix}
			a & 1\\
			0 & b
		\end{bmatrix}
		\begin{bmatrix}
			1 & \frac{1}{b-a}\\
			0 & 1
		\end{bmatrix} =
		\begin{bmatrix}
			1 & \frac{1}{b-a}\\
			0 & 1
		\end{bmatrix}
		\begin{bmatrix}
			a & 0\\
			0 & b
		\end{bmatrix}.
		\]
		
		\bigskip
		Now, if we allow $b$ to go to $a$ then the first component of the eigenvector is going to go to infinity but we are also approaching,
		\[ 
		\begin{bmatrix}
			a & 1\\
			0 & b
		\end{bmatrix} \to
		\begin{bmatrix}
			a & 1\\
			0 & a
		\end{bmatrix} \leadsto
		(A - aI) = (A - bI) =
		\begin{bmatrix}
			0 & 1\\
			0 & 0
		\end{bmatrix}	
		\]
		which is a repeated eigenvalue with a generalized eigenvector. Specifically, the eigenvector is ${ (1,0)^T }$ and the generalized eigenvector satisfies,
		\[
		(A - aI)\v = (1,0)^T \implies \v = (0,1)^T
		\]
		so that 
		\[ (A - aI)^2 \, \v = (A - aI) (1,0)^T = \0. \]
		\note{Notice also, that since $A$ is a ${ 2 \times 2 }$ matrix, in accordance with \autoref{prop:n-eigenvalues-are-roots-of-n-ary-matrix-polynomial}, we have,
			\[ (A - aI)^2 = 0. \]}
		
		So, in matrix equations, we have
		\[ 
		\begin{bmatrix}
			0 & 1\\
			0 & 0
		\end{bmatrix}\begin{bmatrix}0\\ 1\end{bmatrix} = \begin{bmatrix}1\\ 0\end{bmatrix}.
		\]
		
		If we try to use the generalized eigenvector in the same way as a "normal" eigenvector and form the diagonalized matrix in the normal way then it doesn't work as intended because, clearly
		\[ 
		\begin{bmatrix}
			a & 1\\
			0 & a
		\end{bmatrix}
		\begin{bmatrix}
			1 & 0\\
			0 & 1
		\end{bmatrix} \neq
		\begin{bmatrix}
			1 & 0\\
			0 & 1
		\end{bmatrix}
		\begin{bmatrix}
			a & 0\\
			0 & a
		\end{bmatrix}.
		\]
		but, in fact,
		\[ 
		\begin{bmatrix}
			a & 1\\
			0 & a
		\end{bmatrix}
		\begin{bmatrix}
			1 & 0\\
			0 & 1
		\end{bmatrix} =
		\begin{bmatrix}
			1 & 0\\
			0 & 1
		\end{bmatrix}
		\begin{bmatrix}
			a & 1\\
			0 & a
		\end{bmatrix}.
		\]
		
		\smallskip
		The modification to the diagonal matrix reflects the fact that, for the generalized eigenvector,
		\[
		\begin{bmatrix}
			a & 1\\
			0 & a
		\end{bmatrix}\begin{bmatrix}0\\ 1\end{bmatrix} = \begin{bmatrix}1\\ 0\end{bmatrix} + a \begin{bmatrix}0\\ 1\end{bmatrix} =
		\begin{bmatrix}
			1 & 0\\
			0 & 1
		\end{bmatrix}\begin{bmatrix}1\\ a\end{bmatrix}.
		\]
		The matrix,
		\[ 
		\begin{bmatrix}
			a & 1\\
			0 & a
		\end{bmatrix}
		\]
		is a \textit{Jordan block} for the eigenvalue $a$.
	}

	\sep
	\subsubsubsection{Examples of Jordan Canonical Basis}
	\begin{exe}
		\ex{Let ${ T : P_2(\C{}) \longmapsto P_2(\C{}) }$ be a linear operator over the degree-2 complex polynomials and let $T$ be defined by
			\[ T(p) = -p - p'. \]
			If we use the standard basis for $P_2(\C{})$, ${ \{ 1, z, z^2 \} }$ then, the matrix of $T$ \wrt this basis,
			\[ A = 	\begin{bmatrix}
						-1  &  0 &  0\\
						 0  & -1 &  0\\
						 0  &  0 & -1
					\end{bmatrix} -
					\begin{bmatrix}
						0  &  1 &  0\\
						0  &  0 &  2\\
						0  &  0 &  0
					\end{bmatrix} =
					\begin{bmatrix}
						-1  & -1 &  0\\
						 0  & -1 & -2\\
						 0  &  0 & -1
					\end{bmatrix}.
			\]
			Clearly, the characteristic polynomial is ${ p(t) = -(t+1)^3 }$ and there is a single eigenvalue, -1, with multiplicity 3. The generalized eigenspace is the whole space of dimension 3, equal to the multiplicity (as predicted by \autoref{prop:dim-of-gen-eigenspace-equal-to-multiplicity}), and any basis of the space $P_2(\C{})$ is also a basis of the generalized eigenspace.
			\note{Note, though, that using, for example, the standard basis of $P_2(\C{})$ would not produce a Jordan Canonical Form matrix.}
			
			The eigenspace, meanwhile, is the nullspace ${ N(T + I) }$ where
			\[ A + I = 	\begin{bmatrix}
							0  & -1 &  0\\
							0  &  0 & -2\\
							0  &  0 &  0
						\end{bmatrix}
			\]
			and, in terms of the linear map,
			\[ (T + I)(p) = -p - p' + p = -p'. \]
			So the nullspace is
			\[ N(A + I) = t\begin{bmatrix}1\\0\\0\end{bmatrix} \eqword{for} t \in \C{} \]
			and
			\[ N(T + I) = \setc{ p(z) \in P_2(\C{}) }{ p' = 0 } = \setc{ p(z) = \alpha_0 }{ \alpha_0 \in \C{} }. \]
			That's to say, the eigenspace is the space of constant polynomials.\\
			
			If we look for a cycle with initial vector $(1,0,0)^T$ then we want,
			\[ (A + I)^2\,\v = \begin{bmatrix}1\\0\\0\end{bmatrix} = 
				\begin{bmatrix}
					0 & 0 & 2\\
					0 & 0 & 0\\
					0 & 0 & 0
				\end{bmatrix}\v \implies
				\v = \begin{bmatrix}0\\0\\\frac{1}{2}\end{bmatrix}
			\]
			and
			\[\begin{aligned}
				&& (T + I)^2\,(p) &= (T + I)(-p') = p'' = 1 \\
				&\implies & p(z) &= \frac{1}{2}z^2 + \alpha z + \beta \eqword{for} \alpha,\beta \in \C{}.
			\end{aligned}\]
			So the simplest cycle is: ${ \frac{1}{2}z^2, -z, 1 }$ and we can choose as a jordan basis,
			\[ J = \{ 2, -2z, z^2 \}. \]
			Then the matrix of $T$ \wrt this basis is in Jordan Canonical Form,
			\[ [T]_J = \inv{[J]}[T(J)] = 	
				\begin{bmatrix}
					\frac{1}{2} & 0 & 0\\
					0 & -\frac{1}{2} & 0\\
					0 & 0 & 1
				\end{bmatrix}
				\begin{bmatrix}
					-2 & 2 & 0\\
					0 & 2 & -2\\
					0 & 0 & -1
				\end{bmatrix} =
				\begin{bmatrix}
					-1 & 1 & 0\\
					0 & -1 & 1\\
					0 & 0 & -1
				\end{bmatrix}.
			\]
		}
	\end{exe}






% -------------------- break ------------------


	\pagebreak
	\searchableSubsection{Inner Product Spaces}{linear algebra}{

		\bigskip
		\boxeddefinition{\textbf{(Inner Product Space)} Let $\F{}$ denote either the field of real numbers $\R{}$ or the field of complex numbers $\C{}$.\\
			
			An \textbf{inner product space} is a vector space $V$ over the field $\F{}$ together with a map
			\[ \inner{\cdot}{\cdot} : V \times V \longmapsto \F{} \]
			called an \textbf{inner product} that satisfies the following conditions for all vectors ${ \x,\y,\z \in V }$ and all scalars ${ \alpha,\beta \in \F{} }$.
			
			\begin{enumerate}[label=(\roman*)]
				\item{\textbf{Conjugate Symmetry: } ${ \inner{\x}{\y} = \conj{\inner{\y}{\x}} }$,}
				\item{\textbf{Linearity in the first argument: }\\ $~$\hspace{134pt} ${ \inner{\alpha\x + \beta\y}{\z} = \alpha\inner{\x}{\z} + \beta\inner{\y}{\z} }$,}
				\item{\textbf{Positive definiteness: } ${ \inner{\x}{\x} \geq 0 }$ with ${ \inner{\x}{\x} = 0 \iff \x = \0 }$.}
			\end{enumerate}
		}\label{def:inner-product}
	
		\bigskip
		\labeledProposition{For a vector space $V$ over the field $\F{}$, whether $\F{}$ is the real number field $\R{}$ or the complex number field $\C{}$, we have, for all ${ \x \in \F{} }$,
			\[ \inner{\x}{\x} \in \R{}. \]
		}{inner-product-of-vector-with-itself-is-real-valued}
		\begin{proof}
			Property (i) of the inner product (\ref{def:inner-product}) --- conjugate symmetry --- implies that
			\[ \inner{\x}{\x} = \conj{\inner{\x}{\x}} \]
			and, by \autoref{prop:complex-conjugate-properties}, it then follows that
			\[ \ImPart(\inner{\x}{\x}) = 0. \]
			Therefore ${ \inner{\x}{\x} \in \R{} }$.
		\end{proof}
		\note{This is why the inner product can be described as "positive definite" even on complex fields where there is no concept of positive and negative because the complex numbers are not ordered (ref: \autoref{prop;complex-field-is-not-ordered}).}
		
		\bigskip
		\labeledProposition{The inner product is \textbf{conjugate linear} in its second argument. That's to say, if $V$ is an inner product space over the field $\F{}$ and ${ \x,\y,\z \in V }$ and ${ \alpha,\beta \in \F{} }$, then
			\[ \inner{\x}{\alpha\y + \beta\z} = \conj{\alpha}\inner{\x}{\y} + \conj{\beta}\inner{\x}{\z}.  \]
		}{inner-product-is-conjugate-linear-in-second-argument}
		\begin{proof}
			Using the definition of the inner product in \ref{def:inner-product}:
			\[\begin{aligned}
				\inner{\x}{\alpha\y + \beta\z} &= \conj{\inner{\alpha\y + \beta\z}{\x}} &\sidecomment{by conjugate symmetry} \\
				&= \conj{\alpha\inner{\y}{\x} + \beta\inner{\z}{\x}} &\sidecomment{by linearity of 1st argument} \\
				&= \conj{\alpha} \conj{\inner{\y}{\x}} + \conj{\beta}\conj{\inner{\z}{\x}} &\sidecomment{by complex conjugate properties: \ref{prop:complex-conjugate-properties}} \\
				&= \conj{\alpha} \inner{\x}{\y} + \conj{\beta} \inner{\x}{\z} &\sidecomment{by conjugate symmetry}. \qedhere
			\end{aligned}\]
		\end{proof}
	
		\bigskip
		\labeledProposition{For any inner product space, for $\x$ in the space,
			\[ \inner{\0}{\x} = \inner{\x}{\0} = 0. \]
		}{inner-product-with-zero-vector-is-zero}
		\begin{proof}
			\[\begin{aligned}
				&& \inner{\0}{\x} &= \inner{\0 + \0}{\x} &\sidecomment{by props. of $\0$ vector} \\
				&\iff & \inner{\0}{\x} &= \inner{\0}{\x} + \inner{\0}{\x} &\sidecomment{by prop. (ii) of inner product} \\
				&\iff & \inner{\0}{\x} - \inner{\0}{\x} &= \inner{\0}{\x} &\sidecomment{by props. of $\R{}$} \\
				&\iff & 0 &= \inner{\0}{\x}. \nnn
				&& \inner{\x}{\0} &= \inner{\x}{\0 + \0} \\
				&\iff & \inner{\x}{\0} &= \inner{\x}{\0} + \inner{\x}{\0} &\sidecomment{} \\
				&\iff & 0 &= \inner{\x}{\0}.
			\end{aligned}\]
		\end{proof}
	
	
	
		\biggerskip
		\subsubsubsection{Example}
		\begin{exe}
			\ex{It is possible to define an inner product over the space of degree-$n$ real polynomials $P_n(\R{})$ by, for ${ p,q \in P_n(\R{}) }$,
				\[ \inner{p}{q} = \sum_{i = 1}^{n+1} p(x_i)q(x_i) \]
				for any distinct ${ x_1,x_2,\dots,x_{n+1} \in \R{} }$. This inner product satisfies 
				\[ \inner{\v}{\v} = 0 \iff \v = \0 \]
				because, for $p(x_i)^2$ to equal 0 at $n+1$ distinct $x_i$ values, it must have $n+1$ roots -- which is not possible unless $p$ is the zero function (refer: \autoref{prop:three_points_identify_quadratic}).\\
				
				Similarly, we could define the following inner product over the space of degree-$n$ complex polynomials $P_n(\C{})$,
				\[ \inner{p}{q} = \sum_{i = 1}^{n+1} p(x_i) \, \conj{q(x_i)}. \]
			}
		\end{exe}
	
		\bigskip
		\subsubsubsection{Orthogonality}
		\boxeddefinition{\textbf{(Orthogonality)} The vectors $\x$ and $\y$ in an innner product space are said to be \textbf{orthogonal} --- denoted ${ \x \perp \y }$ --- iff ${ \inner{\x}{\y} = 0 }$.
		}\label{def:orthogonality}
	
		\boxeddefinition{\textbf{(Orthogonal Complement)} Let $V$ be an inner product space and ${ S \subset V }$. Then the \textbf{orthogonal complement} of $S$ is defined as,
			\[ S^\perp = \setc{\v \in V}{\forall \x \in S \logicsep \v \perp \x}. \]
		}\label{def:orthogonal-complement}
	
		\medskip
		\labeledProposition{If a set of non-zero vectors in an inner product space is pairwise orthogonal, then it is also linearly independent.\\
			
			That's to say, if ${ \v_1, \v_2, \dots, \v_k \in V }$ are non-zero vectors in an inner product space such that, for each ${ 1 \leq i \neq j \leq k }$, we have ${ \v_i \perp \v_j }$, then ${ S = \{ \v_1, \v_2, \dots, \v_k \} }$ is linearly independent.
		}{pairwise-orthogonal-set-is-linearly-independent}
		\begin{proof}
			Assume there exists a linear dependence relation between some subset of $S$,
			\[ \alpha_1 \x_1 + \alpha_2 \x_2 + \cdots + \alpha_m \x_m = \0 \]
			for ${ \x_1,\x_2,\dots,\x_m \in S }$ and ${ \alpha_1,\alpha_2,\dots,\alpha_m \neq 0 }$. Then, any vector in the dependence relation can be expressed as a linear combination of the others,
			\[ \x_1 = -\frac{1}{\alpha_1}(\alpha_2 \x_2 + \cdots + \alpha_m \x_m). \]
			Now, since the elements of $S$ are pairwise orthogonal, we have
			\[ \inner{\x_i}{\x_j} = 0 \eqword{for} 1 \leq i \neq j \leq m. \]
			Therefore,
			\[\begin{aligned}
				&& \inner{\x_1}{\x_2} &= 0 \\
				&\iff & \inner{-\frac{1}{\alpha_1}(\alpha_2 \x_2 + \cdots + \alpha_m \x_m)}{\x_2} &= 0 &\sidecomment{} \nn
				&\iff & -\frac{1}{\alpha_1}\inner{(\alpha_2 \x_2 + \cdots + \alpha_m \x_m)}{\x_2} &= 0 &\sidecomment{by \ref{def:inner-product}} \nn
				&\iff & \alpha_2\inner{\x_2}{\x_2} + \alpha_3\inner{\x_3}{\x_2} + \cdots + \alpha_m\inner{\x_m}{\x_2} &= 0 &\sidecomment{by \ref{def:inner-product}} \nn
				&\iff & \alpha_2\inner{\x_2}{\x_2} + 0 + \cdots + 0 &= 0 &\sidecomment{by pairwise orthogonality} \nn
				&\iff & \alpha_2\inner{\x_2}{\x_2} &= 0 \nn
				&\iff & \x_2 &= \0. &\sidecomment{by \ref{def:inner-product}}
			\end{aligned}\]
			But all the vectors in $S$ are non-zero by construction so this is a contradiction.
		\end{proof}	
	
	
		

		\bigskip
		\subsubsection{The Standard Inner Product}
		\bigskip
		\boxeddefinition{The \textbf{Standard Inner Product} for real vector spaces is the Euclidean dot product \autoref{defn:dot-product}.\\
			
			For complex vector spaces the \textbf{Standard Complex Inner Product} is similar to the dot product but with conjugation of the second argument:\\
			
			For ${ \x,\y \in \C{n} }$,
			\[ \inner{\x}{\y} = \y^* \x = x_1 \conj{y_1} + \cdots + x_n \conj{y_n} \]
			where ${ \y^* = \conj{\y}^T }$ is the hermitian conjugate (\ref{def:hermitian-conjugate}).
			
			\note{Some authors have the standard complex inner product as ${ \inner{\x}{\y} = \x^T \, \conj{\y} }$ but this is for compatibility with an inner product that is linear in the second argument (and conjugate linear in the first).}
			
			This inner product exhibits conjugate symmetry because, using properties of the complex conjugate (\ref{prop:complex-conjugate-properties}) and modulus (\ref{prop:complex-modulus-properties}),
			\[\begin{aligned}
				\inner{\x}{\y} &= x_1 \conj{y_1} + \cdots + x_n \conj{y_n} \nn
				&= \conj{\conj{x_1} y_1} + \cdots + \conj{\conj{x_n} y_n} &\sidecomment{} \nn
				&= \conj{ \conj{x_1} y_1 + \cdots + \conj{x_n} y_n } &\sidecomment{} \nn
				&= \conj{ y_1 \conj{x_1} + \cdots + y_n \conj{x_n} } \nn
				&= \conj{ \inner{\y}{\x} }.
			\end{aligned}\]
		
			And it is positive definite because,
			\[\begin{aligned}
				\inner{\x}{\x} &= x_1 \conj{x_1} + \cdots + x_n \conj{x_n} \\
				&= \modulus{x_1}^2 + \cdots + \modulus{x_n}^2
			\end{aligned}\]
			and for each ${ \modulus{x_i}^2 }$,
			\[ x_i = 0 \implies \modulus{x_i}^2 = 0 \eqand x_i \neq 0 \implies \modulus{x_i}^2 > 0. \]
		}\label{def:standard-inner-product}
	
		\bigskip
		\labeledProposition{The eigenvectors corresponding to distinct eigenvalues of a Hermitian matrix are orthogonal.}{hermitian-matrix-eigenvectors-for-distinct-eigenvalues-are-orthogonal}
		\begin{proof}
			Let $A$ be a hermitian matrix (\ref{def:hermitian-matrix}). Let $\lambda_1$ and $\lambda_2$ be distinct eigenvalues of $A$ and let $\v_1$ be an eigenvector corresponding to $\lambda_1$ and $\v_2$ an eigenvector corresponding to $\lambda_2$. Then,
			\[\begin{aligned}
				&& \v_1^* A \v_2 &= \v_1^* A^* \v_2     &\sidecomment{$\because A$ is hermitian}\\
				&\iff & \v_1^* (A \v_2) &= (A \v_1)^* \v_2     &\sidecomment{by props. of hermitian conjugate \ref{def:hermitian-conjugate}} \\
				&\iff & \v_1^* (\lambda_2 \v_2) &= (\lambda_1 \v_1)^* \v_2    &\sidecomment{by eigenvalue property} \\
				&\iff & \v_1^* \lambda_2 \v_2 &= \conj\lambda_1 \v_1^* \v_2    &\sidecomment{by \ref{prop:hermitian-conj-of-scalar-x-matrix-is-conj-of-scalar-x-hermitian-conj-of-matrix}} \\
				&\iff & \lambda_2 \v_1^* \v_2 &= \lambda_1 \v_1^* \v_2    &\sidecomment{by \ref{prop:hermitian-matrix-has-only-real-eigenvalues}} \\
				&\iff & (\lambda_2 - \lambda_1) \v_1^* \v_2 &= 0 \\
				&\therefore & \v_1^* \v_2 &= 0.   &\sidecomment{${ \because \lambda_1 \neq \lambda_2 \implies \lambda_2 - \lambda_1 \neq 0 }$}
			\end{aligned}\]
			This last result says that the standard complex inner product of $\v_1$ and $\v_2$ is zero. We can therefore deduce that $\v_1$ and $\v_2$ are orthogonal.
		\end{proof}
		\TODO{does this result imply that the eigenvectors are orthogonal \wrt every basis and every inner product or only the standard basis and inner product?}
	
	
		
		\bigskip
		\subsubsection{Normed Spaces}
		\bigskip
		\boxeddefinition{An inner product induces a \textbf{norm} on a vector space $V$ such that the norm of a vector ${ \v \in V }$ --- denoted ${ \norm{\v} }$ --- is defined as
			\[ \norm{\v} = \sqrt{\inner{\v}{\v}}. \]
			Since the inner product is positive definite we have,
			\[ \inner{\v}{\v} \geq 0 \in \R{}  \implies \norm{\v} \geq 0 \in \R{} \]
			and also, 
			\[ ( \inner{\v}{\v} = 0 \iff \v = \0 ) \implies ( \norm{\v} = 0 \iff \v = \0 ). \]
			
			Therefore, the norm is a real-valued positive definite function
			\[ f: V \longmapsto \R{}. \]
			A vector space equipped with a norm is called a \textbf{normed} vector space.
		}\label{def:vector-norm}
		\boxeddefinition{A \textbf{unit vector} is a vector in a normed vector space that has norm equal to 1.\\
			
			If a vector ${ \v \in V }$ of arbitrary norm is divided by its norm (i.e. scalar multiplied by the reciprocal of its norm), then
			\[ \u = \frac{1}{\norm{\v}} \v \]
			is a unit vector colinear with $\v$. This process is known as \textbf{normalization} of the vector $\v$.
		}
	
		\medskip
		\labeledProposition{If $\x$ is a vector in a normed space over a field $\F{}$ and ${ \alpha \in \F{} }$ then,
			\[ \norm{\alpha \x} = \modulus{\alpha} \norm{\x}. \]
		}{norm-of-scaled-vector-is-modulus-of-scalar-times-norm-of-vector}
		\begin{proof}
			\[ \norm{\alpha \y} = \sqrt{\inner{\alpha\y}{\alpha\y}} = \sqrt{\alpha \conj{\alpha} \inner{\y}{\y}} = \sqrt{\modulus{\alpha}^2} \sqrt{\inner{\y}{\y}} = \modulus{\alpha} \norm{\x}. \]
		\end{proof}
		
	
		\biggerskip
		\subsubsubsection{Generalized Geometry}
		In a bare inner product space there are no points or coordinates and no metric defining distance between points or coordinates. Nevertheless, we can define the angle between two vectors in terms of the inner product and the induced norm.\\
		
		\boxeddefinition{\textbf{(Angle in Real Vector Space)} Let $V$ be an inner product space over the reals $\R{}$ and ${ \x,\y \in V }$. Then the angle $\theta$ between the vectors $\x$ and $\y$ is defined as,
			\[ \cos\theta = \frac{\inner{\x}{\y}}{\norm{\x}\norm{\y}} \iff \theta = \inv\cos\left( \frac{\inner{\x}{\y}}{\norm{\x}\norm{\y}} \right). \]
			
			In the case that the vectors are orthogonal we have ${ \inner{\x}{\y} = 0 }$ so that,
			\[ \theta = \inv\cos(0) = \pm \frac{\pi}{2}. \]
		}\label{def:angle-in-real-vector-space}
	
		\boxeddefinition{\textbf{(Angle in Complex Vector Space)} In a complex vector space the geometrical significance of the "angle" as defined above is not so clear. As a result other concepts of angle are often used in complex spaces (see: \href{https://arxiv.org/pdf/math/9904077.pdf}{Scharnhorst,K. - Angles in Complex Vector Spaces}). One such alternative is typically referred to as the \textbf{Euclidean Angle} and defined by,
				\[ \theta = \inv\cos\left( \frac{\RePart(\inner{\x}{\y})}{\norm{\x}\norm{\y}} \right) \]
				where the inner product used is the complex standard inner product \ref{def:standard-inner-product}. This angle treats the complex scalars as two real numbers and then takes the angle between two such-defined real vectors.\\\\			
				Another commonly used definition of angle in complex spaces is the \textbf{Hermitian Angle} defined by,
				\[ \theta = \inv\cos\left( \frac{\modulus{\inner{\x}{\y}}}{\norm{\x}\norm{\y}} \right) \]
				which is the ratio of the orthogonal projection (using the complex inner product) of $\x$ onto $\y$ divided by the norm of $\x$ (or the reverse).
		}\label{def:angle-in-complex-vector-space}
		
		\bigskip
		\note{In a Euclidean space if two parametric lines have the same direction vector,
			\[ l_1(t_1) = \V{p}_1 + t_1\v, \; \; l_2(t_2) = \V{p}_2 + t_2\v \]
			then the distance between them remains constant at all points on the lines. This is Euclid's 5th postulate, known as {\normalfont The Parallel Postulate} (\href{https://en.wikipedia.org/wiki/Parallel_postulate}{wikipedia}).\\\\
			In this case, the distance is defined as,
			\[ d(t_1) = \min_{t_2} \, \norm{(\V{p}_2 + t_2\v) - (\V{p}_1 + t_1\v)}  \]
			where the norm used here is the Euclidean norm (\ref{def:euclidean-norm}). This Euclidean norm is defined as,
			\[ \norm{\v} = \sqrt{v_1^2 + v_2^2 + \cdots + v_n^2} \]
			for ${ \v \in \R{n} }$. Each component of $\v$ represents a scaling of a standard basis vector ${ v_i \e{i} }$ whose resultant length --- and therefore, norm --- will be $v_i$. The vector $\v$ is the sum of these component vectors,
			\[ \v = \sum_{i = 1}^n v_i \e{i} = v_1 \e{1} + v_2 \e{2} + \cdots + v_n \e{n} \]
			and, since the standard basis vectors are orthogonal and so subtend right angles with each other, we can use pythagoras theorem to deduce that
			\[\begin{aligned}
				\norm{\v}^2 &= \norm{v_1 \e{1}}^2 + \norm{v_2 \e{2}}^2 + \cdots + \norm{v_n \e{n}}^2 \\[4pt]
				&= v_1^2 + v_2^2 + \cdots + v_n^2.
			\end{aligned}\]
			That's to say, the Euclidean norm, which is also the metric (i.e. definition of distance) in Euclidean space, relies on the assumption of an orthogonal basis. Meanwhile, the constancy of the metric value \wrt the coordinates (i.e. that the distance between parallel lines is constant) makes Euclidean space "flat".\\\\
			If we consider WGS-84 GPS coordinate space as an example of a non-Euclidean space: the axes --- latitude and longitude --- form an orthogonal basis at the equator and also, even though the longitude lines converge at the poles, they remain orthogonal to the lines of latitude. So, even though the coordinate bases are orthogonal everywhere, the metric function that returns the distance between two coordinates in the space, depends on the latitude. It is for this reason that it is a non-Euclidean space.\\\\
			In general, an inner product space may not be a metric space, may not be a coordinate space, and --- if we are working with coordinates \wrt a basis --- the basis may not be orthogonal.
		}
		
		\note{If the metric is equal to the norm then parallel vectors will remain the same distance apart? (i.e. the space is flat?) Maybe because the distance between two coordinate points remains the same? Between two coordinate points we can define a vector whose length is the vector norm, if that is also the distance, then the distance depends in a constant way on the coordinates and so the space is not "curved". (?) If we take vectors defined as displacements in the gps coordinate system then the length of the vector depends only on the difference in the coordinates not in where the starting point is (as we would expect because vectors do not have a particular starting point) but the distance represented by the displacement depends on the latitude of the starting point.}
		
		
		\biggerskip
		\begin{lemma}\label{lem:squared-norm-of-sum-of-vectors}
			Let $V$ be an inner product space and ${ \x,\y \in V }$. Then,
			\[ \norm{\x + \y}^2 = \inner{\x}{\x} + \inner{\y}{\y} + 2\RePart(\inner{\x}{\y}). \]
		\end{lemma}
		\begin{proof}
			\[\begin{aligned}
				\norm{\x + \y}^2 &= \inner{\x + \y}{\x + \y} \\
				&= \inner{\x}{\x + \y} + \inner{\y}{\x + \y} &\sidecomment{} \\
				&= \inner{\x}{\x} + \inner{\x}{\y} + \inner{\y}{\x} + \inner{\y}{\y} &\sidecomment{} \\
				&= \inner{\x}{\x} + \inner{\x}{\y} + \conj{\inner{\x}{\y}} + \inner{\y}{\y} &\sidecomment{by \ref{def:inner-product} prop. (i)} \\
				&= \inner{\x}{\x} + \inner{\y}{\y} + 2\RePart(\inner{\x}{\y}).  &\sidecomment{${ \because z + \conj{z} = 2\RePart(z) }$}
			\end{aligned}\]
		\end{proof}
	
		\biggerskip
		\labeledTheorem{\textbf{(Generalized Pythagoras Theorem.)} Let $V$ be an inner product space and ${ \x,\y \in V }$. If $\x$ and $\y$ are orthogonal then,
			\[ \norm{\x + \y}^2 = \norm{\x}^2 + \norm{\y}^2. \]
		}{generalized-pythagoras-theorem}
		\begin{proof}\nl[2]
			If $\x$ and $\y$ are orthogonal then ${ \inner{\x}{\y} = 0 }$. Then the \autoref{lem:squared-norm-of-sum-of-vectors} gives us,
			\[ \norm{\x + \y}^2 = \inner{\x}{\x} + \inner{\y}{\y} + 0 = \norm{\x}^2 + \norm{\y}^2. \qedhere \]
		\end{proof}
		 
	
		\biggerskip
		\labeledTheorem{\textbf{(Cauchy-Schwarz Inequality.)} Let ${ \x,\y \in V }$ be vectors in an inner product space over the field $\F{}$ where $\F{}$ is either the field of real numbers $\R{}$ or the field of complex numbers $\C{}$. Then,
			\[ \modulus{\inner{\x}{\y}} \leq \norm{\x}\norm{\y} \]
			where the equality holds iff $\x$ and $\y$ are linearly dependent.
		}{cauchy-schwarz-inequality}
		\begin{proof}\nl[2]			
			Assume that ${ \x = \0 }$ then,
			\[\begin{aligned}
				&& \modulus{\inner{\0}{\y}} &\leq \norm{\0}\norm{\y} \\
				&\iff & \modulus{0} &\leq (0) \norm{\y} &\sidecomment{by \ref{prop:inner-product-with-zero-vector-is-zero} and inner product prop. (iii)} \\
				&\iff & 0 &= 0.
			\end{aligned}\]
			If ${ \x \neq 0 }$ and ${ \y = \0 }$ we obtain the same result as, by \ref{prop:inner-product-with-zero-vector-is-zero}, ${ \inner{\x}{\0} = 0 }$ also. Clearly also, the same result holds if both are zero. So, if at least one of $\x$ and $\y$ is $\0$ then we have,
			\[ \modulus{\inner{\x}{\y}} = \norm{\x}\norm{\y}. \]
			
			Conversely, assume that $\x$ and $\y$ are both non-zero. We can find the component of $\y$ that is orthogonal to $\x$ by forming a linear combination of $\x$ and $\y$ that is orthogonal to $\x$,
			\[ \inner{\alpha_1 \x + \alpha_2 \y}{\x} = 0 \eqword{for} \alpha_1,\alpha_2 \in \F{}. \]
			Solving for the scalars ${ \alpha_1, \alpha_2 }$,
			\[\begin{aligned}
				&& \inner{\alpha_1 \x + \alpha_2 \y}{\x} &= 0 \\
				&\iff & \alpha_1 \inner{\x}{\x} + \alpha_2 \inner{\y}{\x} &= 0 &\sidecomment{} \\
				&\iff & \alpha_1  &= - \frac{ \alpha_2 \inner{\y}{\x} }{ \inner{\x}{\x} }. \\
			\end{aligned}\]
			If we set ${ \alpha_2 = 1 }$ and define ${ \alpha = \frac{ \inner{\y}{\x} }{ \inner{\x}{\x} } }$ then,
			\[ \z = \y - \frac{ \inner{\y}{\x} }{ \inner{\x}{\x} } \x = \y - \alpha \x \]
			is orthogonal to $\x$.\\
			
			If we take the inner product of $\z$ with itself,
			\[\begin{aligned}
				&& \inner{\z}{\z} &= \inner{\y - \alpha \x}{\y - \alpha \x} \\
				&\iff & \inner{\z}{\z} &= \inner{\y}{\y - \alpha \x} - \alpha \inner{\x}{\z} \\
				&\iff & \inner{\z}{\z} &= \inner{\y}{\y - \alpha \x} &\sidecomment{${ \because \z \perp \x }$} \\
				&\iff & \inner{\z}{\z} &= \inner{\y}{\y} - \conj{\alpha} \inner{\y}{\x} &\sidecomment{by \ref{prop:inner-product-is-conjugate-linear-in-second-argument}} \nn
				&\iff & \inner{\z}{\z} &= \inner{\y}{\y} - \conj{\frac{ \inner{\y}{\x} }{ \inner{\x}{\x} }} \inner{\y}{\x} \nn
				&\iff & \inner{\x}{\x} \inner{\z}{\z} &= \inner{\x}{\x} \inner{\y}{\y} - \conj{\inner{\y}{\x}} \inner{\y}{\x} \nn
				&\iff & \inner{\x}{\x} \inner{\z}{\z} &= \inner{\x}{\x} \inner{\y}{\y} - \inner{\x}{\y} \conj{\inner{\x}{\y}} \nn
				&\iff & \norm{\x}^2 \norm{\z}^2 &= \norm{\x}^2 \norm{\y}^2 - \modulus{\inner{\x}{\y}}^2.
			\end{aligned}\]
		
			Since the norm is positive definite (\ref{def:vector-norm}), 
			\[\begin{aligned}
				&& \norm{\x}^2 \norm{\z}^2 &\geq 0  \nn
				&\iff & \norm{\x}^2 \norm{\y}^2 - \modulus{\inner{\x}{\y}}^2 &\geq 0  \nn
				&\iff & \norm{\x}^2 \norm{\y}^2 &\geq \modulus{\inner{\x}{\y}}^2 \nn
				&\iff & \norm{\x} \norm{\y} &\geq \modulus{\inner{\x}{\y}}.
			\end{aligned}\]
			The equality case happens when ${ \norm{\x}^2 \norm{\z}^2 = 0 }$ and, since $\x$ is non-zero by hypothesis, this implies that ${ \z = \0 }$. It then follows that
			\[ \z = \y - \alpha \x = \0 \iff \y = \alpha \x  \]
			meaning that $\x$ and $\y$ are colinear and so, linearly dependent. (The vector $\z$ was constructed to be the component of $\y$ that is orthogonal to $\x$ and so, if $\x$ and $\y$ are colinear, $\z$ will be $\0$.) Furthermore, the converse implication --- that equality implies linear dependence --- holds because, using \autoref{prop:norm-of-scaled-vector-is-modulus-of-scalar-times-norm-of-vector},
			\[ \modulus{\inner{\x}{\y}} = \modulus{\inner{\alpha \y}{\y}} = \modulus{\alpha \, \norm{\y}^2} = \modulus{\alpha} \norm{\y} \norm{\y} = \norm{\alpha\y} \norm{\y}. \]
			If we have instead ${ \y = \alpha \x }$, then the result is the same because ${ \modulus{\conj{\alpha}} = \modulus{\alpha} }$.\\
			

			\bigskip
			\note{In the case where ${ \F{} = \R{} }$ we can use the following proof.\\\\ For all ${ \alpha \in \R{} }$ we have,
			\[\begin{aligned}
				&& \norm{\alpha \x + \y}^2 &\geq 0 \\
				&\iff & \inner{\alpha\x + \y}{\alpha\x + \y} &\geq 0 \\
				&\iff & \inner{\alpha\x}{\alpha\x + \y} + \inner{\y}{\alpha\x + \y} &\geq 0 \\
				&\iff & \inner{\alpha\x}{\alpha\x} + \inner{\alpha\x}{\y} + \inner{\y}{\alpha\x} + \inner{\y}{\y} &\geq 0 \\
				&\iff & \alpha^2\inner{\x}{\x} + 2\alpha\inner{\x}{\y} + \inner{\y}{\y} &\geq 0 &\sidecomment{by inner product properties}.
			\end{aligned}\]
			We can re-arrange this result as a real-valued quadratic in $\alpha$,
			\[ p(\alpha) = \inner{\x}{\x}\alpha^2 + 2\inner{\x}{\y}\alpha + \inner{\y}{\y} \]
			which has discriminant
			\[\begin{aligned}
				d &= (2\inner{\x}{\y})^2 - 4\inner{\x}{\x}\inner{\y}{\y} \\
				&= 4(\, \inner{\x}{\y}^2 - \inner{\x}{\x}\inner{\y}{\y} \,).
			\end{aligned}\]
			Since we have that ${ \forall \alpha \in \R{} \logicsep p(\alpha) \geq 0 }$ we can deduce that the discriminant ${ d \leq 0 }$ and so,
			\[\begin{aligned}
				&& 4(\, \inner{\x}{\y}^2 - \inner{\x}{\x}\inner{\y}{\y} \,) &\leq 0 \nn
				&\iff & \inner{\x}{\y}^2 &\leq \inner{\x}{\x}\inner{\y}{\y} \nn
				&\iff & -\sqrt{\inner{\x}{\x}\inner{\y}{\y}} \leq \inner{\x}{\y} &\leq \sqrt{\inner{\x}{\x}\inner{\y}{\y}} \nn
				&\iff & \abs{\inner{\x}{\y}} &\leq \norm{\x}\norm{\y}. \qedhere
			\end{aligned}\]}
		\end{proof}
		\begin{corollary}\label{coro:real-or-imaginary-parts-of-inner-product-leq-product-of-norms}
			Let ${ \x,\y \in V }$ be vectors in an inner product space over the field $\F{}$ where $\F{}$ is either the field of real numbers $\R{}$ or the field of complex numbers $\C{}$. Then,
			\[ \RePart(\inner{\x}{\y}), \, \ImPart(\inner{\x}{\y}) \leq \norm{\x}\norm{\y}. \qedhere \]
		\end{corollary}
		\begin{proof}
			This is a consequence of Cauchy-Schwarz and \autoref{prop:complex-modulus-properties} property (iii),
			\[ \RePart(\inner{\x}{\y}), \, \ImPart(\inner{\x}{\y}) \leq \modulus{\inner{\x}{\y}} \leq \norm{\x}\norm{\y}. \]
		\end{proof}
	
		\biggerskip
		\labeledTheorem{\textbf{(Triangle Inequality for Norms.)} Let $V$ be an inner product space and ${ \x,\y \in V }$. Then,
			\[ \norm{\x + \y} \leq \norm{\x} + \norm{\y}. \]
		}{triangle-inequality-for-norms}
		\begin{proof}\nl[2]
			This follows from the \autoref{lem:squared-norm-of-sum-of-vectors} and a corollary of Cauchy-Schwarz \ref{coro:real-or-imaginary-parts-of-inner-product-leq-product-of-norms}:
			\[\begin{aligned}
				\norm{\x + \y}^2 &= \inner{\x}{\x} + \inner{\y}{\y} + 2\RePart(\inner{\x}{\y}) \\
				&\leq \inner{\x}{\x} + \inner{\y}{\y} + 2\norm{\x}\norm{\y}  &\sidecomment{by Cauchy-Schwarz} \\
				&= (\, \norm{\x} + \norm{\y} \,)^2. \qedhere
			\end{aligned}\]
		\end{proof}
		\note{Compare with the Triangle Inequality for simple signed magnitudes: \ref{sssection:triangle-inequality}.}
	}





% -------------------- break ------------------


	\pagebreak
	\searchableSubsection{Orthonormal Bases and Orthogonal Operators}{linear algebra}{
		\biggerskip
		\boxeddefinition{A set of pairwise mutually orthogonal (see: \ref{def:orthogonality}) unit vectors in an inner product space is called an \textbf{orthonormal set}.
		}
		\medskip
		\subsubsection{Orthonormal Bases}
		\boxeddefinition{A basis consisting of an orthonormal set of vectors is known as an \textbf{orthonormal basis}.
		}
	
		\medskip
		\labeledProposition{Let ${ U = \{\u_1,\dots, \u_n\} }$ be an orthonormal basis. Then, for any ${ \u_i,\u_j \in U,\, i \neq j }$,
			\[ \inner{\u_i}{\u_j} = 0 \eqand \inner{\u_i}{\u_i} = 1 = \inner{\u_j}{\u_j}. \]}{properties_of_orthonormal_basis_vectors}
		\begin{proof}
			The elements of $U$ are pairwise mutually orthogonal so --- by the definition of orthogonal vectors (\ref{def:orthogonality}) --- ${ \inner{\u_i}{\u_j} = 0 }$ for ${ i \neq j }$. Furthermore,
			\[ \inner{\u_i}{\u_i} = \norm{\u_i}^2 = 1 \]
			because the length of unit vectors is 1.
		\end{proof}
	
		\bigskip
		\begin{corollary}\label{coro:dim-V-orthogonal-vectors-are-basis}
			A set of pairwise orthogonal vectors in an inner product space $V$ with cardinality $\dim V$ is a basis of $V$.
		\end{corollary}
		\begin{proof}
			This is a consequence of \autoref{prop:pairwise-orthogonal-set-is-linearly-independent}.
		\end{proof}
	
		\biggerskip
		\subsubsection{Orthonormal Bases in Coordinate Spaces}
		\bigskip
		\labeledProposition{Let ${ B = \{\v_1,\v_2,\dots,\v_n\} }$ be an orthonormal basis of an inner product space $V$ and let ${ \w \in V }$. Let the coordinates of $\w$ \wrt the basis $B$ be $\alpha_i$ for ${ 1 \leq i \leq n }$. Then the coordinates $\alpha_i$  are given by 
			\[ \alpha_i = \inner{\w}{\v_i}. \]
		}{inner-prod-of-vector-with-orthonormal-basis-vector-is-coordinate}
		\begin{proof}
			\[\begin{aligned}
				\inner{\w}{\v_i} &= \inner{\alpha_1\v_1 + \alpha_2\v_2 + \cdots + \alpha_n\v_n}{\v_i} \\
				&= \alpha_1\inner{\v_1}{\v_i} + \alpha_2\inner{\v_2}{\v_i} + \cdots + \alpha_n\inner{\v_n}{\v_i} &\sidecomment{} \\
				&= \alpha_i\inner{\v_i}{\v_i} &\sidecomment{$\because$ pairwise orthogonal} \\
				&= \alpha_i. &\sidecomment{$\because \norm{\v_i} = 1$}  \qedhere
			\end{aligned}\]
		\end{proof}
	
		\medskip
		\labeledProposition{Let ${ p: V \times V \longmapsto \R{} }$ be a non-standard inner product defined on a vector space $V$ and ${ B \subset V }$ be an orthonormal basis \wrt the inner product $p$. Let ${ \v,\w \in V }$ and let ${ \v_B,\w_B \in V }$ be vectors ${ \v,\w }$ expressed in coordinates \wrt the basis $B$. Then the inner product
			\[ p(\v, \w) = \v_B \dotprod \w_B, \]
			$p$ is equal to the standard inner product (\ref{def:standard-inner-product}) on the vectors expressed \wrt the orthonormal basis.
		}{any-inner-product-of-two-vectors-is-equal-to-dot-product-of-the-vectors-wrt-orthonormal-basis}
		\begin{proof}
			Let the inner product $p$ be denoted by the usual inner product notation ${ \inner{\u_1}{\u_2} = p(\u_1,\u_2) }$ and let the orthonormal basis be ${ B = \{\b_1,\dots,\b_n\} }$. Then,
			\[\begin{aligned}
				\inner{\v_B}{\w_B} &= \inner{\alpha_1\b_1 + \cdots + \alpha_n\b_n}{\beta_1\b_1 + \cdots + \beta_n\b_n} \\
				&= \inner{\alpha_1\b_1}{\beta_1\b_1} + \cdots + \inner{\alpha_n\b_n}{\beta_n\b_n} &\sidecomment{$\because i \neq j \implies \inner{\b_i}{\b_j} = 0 $}\\
				&= \alpha_1 \conj{\beta_1} \inner{\b_1}{\b_1} + \cdots + \alpha_n \conj{\beta_n} \inner{\b_n}{\b_n} \\
				&= \alpha_1 \conj{\beta_1} + \cdots + \alpha_n \conj{\beta_n}.    &\sidecomment{$\because \inner{\b_i}{\b_i} = 1 $} \qedhere
			\end{aligned}\]
		\end{proof}
	
		\biggerskip
		\subsubsubsection{Example}
		\begin{exe}
			\ex{Consider an inner product defined over $\R{2}$ given by
				\[ \inner{\x}{\y} = \x^T A \y  \eqword{where}  A = \begin{bmatrix} 5 & 2\\ 2 & 1 \end{bmatrix}. \]
				This is an inner product because:				
				\subheading{Symmetry}
				Since $A$ is symmetric and the inner product --- being a ${ 1 \times 1 }$ matrix --- is also symmetric we have,
					\[ \inner{\x}{\y} = \inner{\x}{\y}^T = (\x^T A \y)^T = \y^T A^T \x = \y^T A \x = \inner{\y}{\x}. \]
				\subheading{Linearity in first argument}	
				By the linearity of matrix multiplication,
					\[ \inner{\alpha\x + \z}{\y} = (\alpha\x + \z) A \y = \alpha \x A \y + \z A \y = \alpha\inner{\x}{\y} + \inner{\z}{\y}. \]
				\subheading{Reflexive positive definiteness and only 0 for $\0$}
				If we define ${ \x = (x_1, x_2)^T \in \R{2} }$ then,
				\[ \inner{\x}{\x} = 5x_1^2 + 4x_1x_2 + x_2^2 = (\sqrt{5}x_1 + \frac{2x_2}{\sqrt{5}}) + \frac{x_2^2}{5} \]
				which expression clearly implies that
				\[ \inner{\x}{\x} \geq 0 \eqand \x = \0 \implies \inner{\x}{\x} = 0. \]
				On the other hand, if ${ \inner{\x}{\x} = 0 }$, since we have,
				\[ (\sqrt{5}x_1 + \frac{2x_2}{\sqrt{5}}) \geq 0 \eqand \frac{x_2^2}{5} \geq 0 \]
				then we can deduce that
				\[\begin{aligned}
					&& (\sqrt{5}x_1 + \frac{2x_2}{\sqrt{5}}) &= 0 = \frac{x_2^2}{5} \\
					&\implies & x_2 &= 0 \\
					&\implies & x_1 &= 0 \\
					&\therefore & \x &= \0.
				\end{aligned}\]
				Having established that we have defined an inner product on $\R{2}$, we can use it to determine orthogonality of vectors in the space. If we let ${ \v = (1,1)^T }$ then the space of vectors orthogonal to $\v$ is the orthogonal complement (\ref{def:orthogonal-complement}) to the span of $\v$,
				\[ O = (\operatorname{Lin} \{\v\})^\perp = \setc{ \w \in \R{2} }{ \inner{\w}{\v} = 0 }. \]
				Since,
				\[\begin{aligned}
					&& \inner{\w}{\v} &= 0 \nn
					&\iff & \w^T \begin{bmatrix} 5 & 2\\ 2 & 1 \end{bmatrix} \begin{bmatrix}1 \\ 1\end{bmatrix} &= 0 \nn
					&\iff & \begin{bmatrix}w_1 \\ w_2\end{bmatrix} \begin{bmatrix} 5 & 2\\ 2 & 1 \end{bmatrix} \begin{bmatrix}1 \\ 1\end{bmatrix} &= 0 \nn
					&\iff & 7 w_1 + 3 w_2 &= 0
				\end{aligned}\]
				then the set $O$ is
				\[ O = \setc{ (w_1,w_2) \in \R{2} }{ 7 w_1 + 3 w_2 = 0 }. \]
				The set $O$ is clearly a 1-d vector space and so we can select any vector from it as a basis of it,
				\[ O = t \begin{bmatrix}-3 \\ 7\end{bmatrix} \eqword{for} t \in R{}. \]
				Then we have an orthonormal basis (orthogonal \wrt this inner product) if we normalize the two basis vectors,
				\[ \left\{ \begin{bmatrix}1 \\ 1\end{bmatrix}, \begin{bmatrix}-3 \\ 7\end{bmatrix} \right\}. \]
				So we need to calculate the norms,
				\[ \begin{bmatrix}1 & 1\end{bmatrix} \begin{bmatrix} 5 & 2\\ 2 & 1 \end{bmatrix} \begin{bmatrix}1 \\ 1\end{bmatrix} = 7 + 3 = 10, \]
				\[ \begin{bmatrix}-3 & 7\end{bmatrix} \begin{bmatrix} 5 & 2\\ 2 & 1 \end{bmatrix} \begin{bmatrix}-3 \\ 7\end{bmatrix} = 3 + 7 = 10. \]
				\note{It is a coincidence that these values have turned out to be equal.}
				So the set
				\[ B = \left\{ \frac{1}{\sqrt{10}} \begin{bmatrix}1 \\ 1\end{bmatrix}, \frac{1}{\sqrt{10}} \begin{bmatrix}-3 \\ 7\end{bmatrix} \right\} \]
				is an orthonormal basis for the inner product space defined by $\R{2}$ equipped with the inner product ${ \x^T A \y }$.\\
				
				If we now express a couple of vectors \wrt the orthonormal basis $B$,
				\[ \inv{[B]} = \frac{1}{\sqrt{10}} \begin{bmatrix}7 & 3 \\ -1 & 1\end{bmatrix}. \]
				\[ 
					\y = \begin{bmatrix}3 \\ 4\end{bmatrix}, \; 
					\y_B = \frac{1}{\sqrt{10}} \begin{bmatrix}7 & 3 \\ -1 & 1\end{bmatrix} \begin{bmatrix}3 \\ 4\end{bmatrix} = 
						\frac{1}{\sqrt{10}} \begin{bmatrix}33 \\ 1\end{bmatrix}. 
				\]
				\[ 
					\z = \begin{bmatrix}6 \\ 1\end{bmatrix}, \; 
					\z_B = \frac{1}{\sqrt{10}} \begin{bmatrix}7 & 3 \\ -1 & 1\end{bmatrix} \begin{bmatrix}6 \\ 1\end{bmatrix} = 
						\frac{1}{\sqrt{10}} \begin{bmatrix}45 \\ -5\end{bmatrix}. 
				\]
				then their inner product
				\[\begin{aligned}
					\inner{\y}{\z} &= \begin{bmatrix}3 & 4\end{bmatrix} A \begin{bmatrix}6 \\ 1\end{bmatrix} \nn
					&= \begin{bmatrix}3 & 4\end{bmatrix} \begin{bmatrix} 5 & 2\\ 2 & 1 \end{bmatrix} \begin{bmatrix}6 \\ 1\end{bmatrix} \nn
					&= \begin{bmatrix}3 & 4\end{bmatrix} \begin{bmatrix} 32 \\ 13 \end{bmatrix} \nn
					&= 32 \times 3 + 13 \times 4 = 148,
				\end{aligned}\]
				while the dot product of the vectors \wrt the orthonormal basis is
				\[\begin{aligned}
					\y_B \dotprod \z_B &= \frac{1}{\sqrt{10}} \begin{bmatrix}33 \\ 1\end{bmatrix} \dotprod \frac{1}{\sqrt{10}} \begin{bmatrix}45 \\ -5\end{bmatrix} \nn
					&= \frac{1}{10} (33 \times 45 + 1 \times -5) = 148.
				\end{aligned}\]
			}
		\end{exe}
	
		\biggerskip
		\subsubsubsection{Intuition of Orthogonal Bases}
		If we consider what an orthonormal basis would look like in coordinate vectors: Obviously, the standard basis is an orthonormal basis as,
			\[ \begin{bmatrix}1\\0\\0\end{bmatrix} \dotprod \begin{bmatrix}0\\1\\0\end{bmatrix} = 0 \]
		and clearly for any ${ i,j }$ such that ${ i \neq j }$, ${ \e{i} \dotprod \e{j} = 0 }$ and ${\e{i}, \e{j} }$ have unit length.\\\\
		We can form other orthonormal bases in $\R{3}$ though. The vectors,
		\[ \begin{bmatrix}1\\1\\0\end{bmatrix} \dotprod \begin{bmatrix}-1\\1\\0\end{bmatrix} = 0 \]
		are orthogonal but do not have unit length. We can make them unit length, though, by dividing them by $\sqrt{2}$ so that,
		\[ \begin{bmatrix}1/\sqrt{2}\\1/\sqrt{2}\\0\end{bmatrix},\, \begin{bmatrix}-1/\sqrt{2}\\1/\sqrt{2}\\0\end{bmatrix},\, \begin{bmatrix}0\\0\\1\end{bmatrix} \]
		is also an orthonormal basis of $\R{3}$.\\\\
		Another orthonormal basis of $\R{3}$ is
		\[ \begin{bmatrix}\cos\theta\\\sin\theta\\0\end{bmatrix},\, \begin{bmatrix}-\sin\theta\\\cos\theta\\0\end{bmatrix},\, \begin{bmatrix}0\\0\\1\end{bmatrix} \]
		for any angle $\theta$ measured anticlockwise from the $x$-axis. Actually this is the general case of which the previous example is a special case when ${ \theta = \pi / 2 }$.
		\pagebreak	
		\begin{figure}[h!]
			\includegraphics[width=\linewidth]{\resourceDir/img/orthonormal_bases_theta_pi_by_2.png}
			\caption{$ (\cos{\pi/4}, \sin{\pi/4})^T = (1/\sqrt{2}, 1/\sqrt{2})^T,\;  (-\sin{\pi/4}, \cos{\pi/4})^T = (-1/\sqrt{2}, 1/\sqrt{2})^T $}
		\end{figure}
		\\But for any value of the angle $\theta$ these remain orthogonal as can be seen if we generate the basis vectors for ${ \theta = \pi/3 }$.
		\begin{figure}[h!]
			\includegraphics[width=\linewidth]{\resourceDir/img/orthonormal_bases_theta_pi_by_3.png}
			\caption{$ (\cos{\pi/3}, \sin{\pi/3})^T = (1/2, \sqrt{3}/2)^T,\;  (-\sin{\pi/3}, \cos{\pi/3})^T = (-\sqrt{3}/2, 1/2)^T $}
		\end{figure}
		\clearpage
	

		\subsubsection{Orthogonal Operators}
		\bigskip
		\boxeddefinition{A matrix whose columns form an orthonormal basis is called an \textbf{orthogonal matrix}.\\\\
			The operation of left multiplication by such a matrix is called an \textbf{orthogonal operator}.
		}
	
		\medskip
		\labeledProposition{A matrix $A$ is orthogonal iff ${ A^T = \inv{A}}$.\\ \TODO{This proof isn't compatible with complex spaces!}}{orthogonal_matrix_transpose_is_inverse}
		\begin{proof}
			$A$ is an orthogonal matrix so, by definition, its columns form an orthogonal basis. Then,
			\begin{align*}
			&& A^TA &=
					\begin{bmatrix}
						a_{11} & a_{21} & \cdots & a_{n1}\\
						a_{12} & a_{22} & \cdots & a_{n2}\\
						\vdots \\
						a_{1n} & a_{2n} & \cdots & a_{nn}
					\end{bmatrix}
					\begin{bmatrix}
						a_{11} & a_{12} & \cdots & a_{1n}\\
						a_{21} & a_{22} & \cdots & a_{2n}\\
						\vdots \\
						a_{n1} & a_{n2} & \cdots & a_{nn}
					\end{bmatrix} \\
			&&  &= \begin{bmatrix}
						a_{11}^2 + \cdots + a_{n1}^2 & a_{11}a_{12} + \cdots + a_{n1}a_{n2} & \cdots\\
						a_{12}a_{11} + \cdots + a_{n2}a_{n1} & a_{12}^2 + \cdots + a_{n2}^2 & \cdots\\
						\vdots \\
						\cdots & \cdots & \cdots & a_{1n}^2 + \cdots + a_{nn}^2
					\end{bmatrix}
			\end{align*}
			Along the main diagonal the components take the form
			\[ a_{1j}^2 + a_{2j}^2 + \cdots + a_{nj}^2 = a_j \dotprod a_j \]
			where ${ a_j }$ is the $j$th column of the matrix $A$. But the columns of $A$ are vectors in an orthonormal basis and so ${ a_j \dotprod a_j= 1 }$.\\
			Furthermore, the off-diagonal values take the form
			\[ a_{1j}a_{1j'} + \cdots + a_{nj}a_{nj'} = a_j \dotprod a_{j'} \]
			for ${ j \neq j' }$. Since the columns are from an orthonormal basis we know that ${ a_j \dotprod a_{j'} = 0 }$.\\
			Therefore the resultant matrix looks like,
			\[ A^TA = \begin{bmatrix}
						1 & 0 & \cdots & 0\\
						0 & 1 & \cdots & 0\\
						\vdots\\
						0 & 0 & \cdots & 1
					  \end{bmatrix}
					  = I_n.
			\]
			Clearly, the same effect would be seen for ${ AA^T }$ and so,
			\[ A^TA = AA^T = I \iff A^T = \inv{A}. \]
			
			Conversely, we can reverse the logic and it is easy to see that if ${ A^T A = I }$ then,
			\begin{enumerate}[label=(\roman*)]
				\item{since the diagonal entries are all 1, if $\u_i$ is a column of $A$, then ${ \u_i \dotprod \u_i = 1 }$;}
				\item{since the off-diagonal entries are all 0, if ${ \u_i, \u_j }$ with ${ i \neq j }$ are distinct columns of $A$, then  ${ \u_i \dotprod \u_j = 0 }$.}
			\end{enumerate}
			Therefore, the columns of $A$ form an orthogonal basis and thus, by definition, $A$ is an orthogonal matrix.
		\end{proof}
	
		\note{This provides another way of seeing \autoref{prop:inner-prod-of-vector-with-orthonormal-basis-vector-is-coordinate}: If $[B]$ is the matrix whose columns are the vectors of an orthonormal basis then --- just as with any change of basis --- the vector $\x$ \wrt the standard basis, has coordinates \wrt the basis $B$ given by,
			\[ \inv{[B]}\x. \]
			But, since $[B]$ is an orthogonal matrix,
			\[ \inv{[B]}\x = [B]^T\x \]
			which is the vector whose $i$-th row component is the dot product of the $i$-th column of $[B]$ and $\x$.
		}
	
		\bigskip
		\labeledProposition{Left multiplication by an orthogonal matrix preserves the dot product. In other words, for all vectors ${ \v,\w }$, $A$ is an orthogonal matrix if and only if,
			\[ A\v \dotprod A\w = \v\dotprod\w. \]\\ \TODO{This proof isn't compatible with complex spaces!}
		}{left_mult_by_ortho_matrix_preserves_dot_product}
		\begin{proof}
			Using \autoref{prop:orthogonal_matrix_transpose_is_inverse} and the matrix formula for the dot product we can deduce that, for all vectors ${ \v,\w }$,
			\begin{align*}
			&& A\v \dotprod A\w &= (A\v)^T A\w \\
			&&  &= \v^TA^TA\w &\sidecomment{} \\
			&&  &= \v^T\w &\sidecomment{$A$ is orthogonal so ${A^TA = I}$} \\
			&&  &= \v \dotprod \w
			\end{align*}
			which is to say that an orthogonal matrix preserves the dot product.\\
			
			Conversely, if we assume that $A$ preserves the dot product then, for all vectors ${ \v,\w }$,
			\begin{align*}
			&& A\v \dotprod A\w &= \v \dotprod \w  \\
			&\iff & (A\v)^TA\w &= \v^T\w & \\
			&\iff & \v^TA^TA\w &= \v^T\w & \\
			&\iff & \v^TA^TA\w - \v^T\w &= 0 & \sidecomment{both terms are scalars}\\
			&\iff & \v^T(A^TA - I)\w &= 0. &
			\end{align*}
			Now for any arbitrary matrix $B$, 
			\[ \e{i}^T B \e{j} = b_{ij} \]
			where $b_{ij}$ is the $(i,j)$th element of $B$. Then, for
			\[ \e{i}^T B \e{j} = 0 \]
			to be true \textit{for all possible} ${ \e{i},\e{j} }$ would require that, 
			\[ \forall i,j \logicsep b_{ij} = 0 \iff B = [0] \]
			where $[0]$ is the zero matrix. Therefore,
			\[ \forall \v,\w \logicsep \v^T(A^TA - I)\w = 0 \iff A^TA - I = [0] \iff A^TA = I \]
			which, by \autoref{prop:orthogonal_matrix_transpose_is_inverse}, implies that $A$ is orthogonal. So if $A$ preserves the dot product then it is orthogonal.
		\end{proof}
	
	
		\bigskip
		\labeledProposition{The determinant of any orthogonal matrix is 1 or -1.}{determinant_of_orthogonal_matrix}
		\begin{proof}
			If a matrix $A$ is orthogonal then ${ A^TA = I }$ which implies that,
			\[ det(A^T)det(A) = det(I) = 1. \]
			By \autoref{prop:determinant_of_matrix_equal_to_its_transpose}, ${ det(A^T) = det(A) }$ so,
			\[ det(A^T)det(A) = det(A)^2 = 1 \iff \sqrt{det(A)} = 1 \iff det(A) = \pm 1.  \qedhere \]
		\end{proof}
	
		\note{If an orthogonal operator has determinant equal to 1 it is described as \textbf{orientation preserving} and if it is equal to -1 it is described as \textbf{orientation reversing}.}
		
		\bigskip
		\labeledProposition{The orthogonal matrices form a subgroup of $GL_n(\F{})$.}{orthogonal-matrices-are-subgroup}
		\begin{proof}
			Let ${ S = \setc{ A \in GL_n(\F{}) }{ A^TA = I } }$. Then,
			\begin{itemize}
				\item{$S$ is nonempty because ${ I \in S }$.}
				\item{For ${ B,C \in S }$,
					\[ (BC)^T(BC) = C^TB^T(BC) = C^T(B^TB)C = I \]
					so ${ BC \in S }$.
				}
				\item{For ${ B \in S }$, by \autoref{prop:orthogonal_matrix_transpose_is_inverse}, ${ \inv{B} = B^T }$ and
					\[ (B^T)^TB^T = BB^T = B\inv{B} = I \]
					so $S$ contains inverses.
				}
			\end{itemize}
			Therefore ${ S \leq GL_n(\F{}) }$.	
		\end{proof}
		
		\medskip
		\note{The subgroup of the general linear group formed by the orthogonal matrices is called the \textbf{orthogonal group} and is denoted $O_n$.}
		
		\biggerskip
		\subsubsection{Gram-Schmidt Orthonormalisation}
		\boxeddefinition{The process known as \textbf{Orthonormalisation} takes a set of linearly independent vectors and returns an orthonormal set of the same cardinality as the original set and in the same space spanned by the original set.\\ Since the orthonormal set has the same cardinality as the original set, by \autoref{coro:dim-V-orthogonal-vectors-are-basis}, it is an orthonormal basis for the space spanned by the original set.}
		
		\biggerskip
		The Gram-Schmidt algorithm is as follows:
		\begin{algorithm}
			\begin{algorithmic}
				\REQUIRE A $k$-length linearly independent set of vectors ${ S = \{\v_1,\dots,\v_k\} \subset V }$ where $V$ is an inner product space, and an initially-empty set ${ O = \{\} }$.
				\ENSURE A $k$-length orthonormal set of vectors ${ O = \{\u_1, \dots, \u_k \} }$ \suchthat ${ \operatorname{Lin} O = \operatorname{Lin} S }$.
				\STATE % empty line
				\FOR {$i=1$ to $k$}
				\STATE find $\w$, a basis for the orthogonal complement of $O$ in ${ O \cup \{\v_i\} }$.
				\STATE ${ \u_i \leftarrow \frac{\w}{\norm{\w}} }$
				\ENDFOR 
			\end{algorithmic}	
		\end{algorithm}
		% Note: This article: https://www.math-linux.com/latex-26/faq/latex-faq/article/how-to-write-algorithm-and-pseudocode-in-latex-usepackage-algorithm-usepackage-algorithmic says, at the end, that the algorithmic package used here is not compatible with hyperref.
		
	
		\medskip
		The process of finding the orthogonal complement on iteration number $i$ is to find the set of vectors
		\[ O^\perp = \setc{\w \in O \cup \{\v_i\}}{\forall \u \in O \logicsep \w \perp \u}. \]
	
		For ${ \w \in O^\perp }$ we need, for every ${ \u_j \in O }$,
		\[\begin{aligned}
			&& \inner{\w}{\u_j} &= 0 \\
			&\iff & \inner{\alpha_1 \u_1 + \cdots + \alpha_{i-1} \u_{i-1} + \alpha_i \v_i}{\u_j} &= 0 \\
			&\iff & \inner{\alpha_j \u_j + \alpha_i \v_i}{\u_j} &= 0 &\sidecomment{by orthogonality} \\
			&\iff & \alpha_j + \alpha_i \inner{\v_i}{\u_j}  &= 0 &\sidecomment{${ \because \norm{\u_j} = 1 }$} \\
			&\iff & \alpha_j &= -\alpha_i \inner{\v_i}{\u_j}.
		\end{aligned}\]
		So, if we set ${ \alpha_i = 1 }$ then we get
		\[ \w = \v_i - \inner{\v_i}{\u_1}\u_1 - \inner{\v_i}{\u_2}\u_2 - \cdots - \inner{\v_i}{\u_{i-1}}\u_{i-1}. \]
		Clearly, our only degree of freedom in the definition of the vector $\w$ is the value of $\alpha_i$. However, if we were to use some other value, ${ \alpha_i = \beta }$, then the result would be ${ \beta \w }$, a scaling of $\w$ by $\beta$. As a result, this is clearly a 1-dimensional space in which any vector will suffice as a basis. So we can use $\w$ as the basis vector and then, it is a simple matter to normalize it and add it to the set $O$.
		
		
		
		\biggerskip
		\subsubsection{Orthogonal Diagonalisation}
		\bigskip
		\boxeddefinition{\textbf{(Orthogonal Diagonalisation)} A matrix is said to be \textbf{orthogonally diagonalisable} if there is an orthogonal matrix that diagonalises it.}
		
		\medskip
		\labeledProposition{A matrix is orthogonally diagonalisable iff there exists an orthonormal eigenbasis.}{orthogonally-diagonalisable-iff-exists-an-orthonormal-eigenbasis}
		\begin{proof}
			This is a consequence of the definition of orthogonal diagonalisation, the definition of an orthogonal matrix \addref, and the equivalence of diagonalisation and the existence of an eigenbasis \addref.
		\end{proof}
	
%		\bigskip
%		\labeledTheorem{\textbf{(Spectral Theorem for Symmetric Matrices.)} A matrix is orthogonally diagonlisable if and only if it is symmetric.}{matrix-orthogonally-diagonalisable-iff-symmetric}
%		\begin{proof}
%		\end{proof} TODO
	
		\bigskip
		\labeledProposition{If a matrix is orthogonally diagonalisable then it is symmetric.}{matrix-orthogonally-diagonalisable-implies-symmetric}
		\begin{proof}
			If a matrix $A$ is orthogonally diagonalisable then there exists an orthogonal matrix $P$ such that,
			\[ D = \inv{P}AP \]
			is diagonal. Since the matrix $D$ is diagonal, it is symmetrical and equal to its transpose so we can reason,
			\[ D^T = D \implies (\inv{P}AP)^T = \inv{P}AP. \]
			Furthermore, $P$ is an orthogonal matrix so, by \addref, ${ P^T = \inv{P} }$. So,
			\[\begin{aligned}
				&& (\inv{P}AP)^T &= \inv{P}AP \\
				&\iff & P^T A^T (\inv{P})^T &= \inv{P}AP \\
				&\iff & \inv{P} A^T (P^T)^T &= \inv{P}AP \\
				&\iff & \inv{P} A^T P &= \inv{P}AP \\
				&\iff & A^T &= A.  \qedhere
			\end{aligned}\]
		\end{proof}
	
		\bigskip
		\labeledProposition{If a matrix is symmetric then eigenvectors corresponding to distinct eigenvalues are orthogonal.}{symmetric-matrix-eigenvecs-of-distinct-eigenvals-are-orthogonal}
		\begin{proof}
		\end{proof}
	}


% --------------------


\pagebreak
\searchableSubsection{Linear and Affine Transformations}{linear algebra}{
	\bigskip\bigskip
	\subsubsection{Affine Spaces}
	\boxeddefinition{An \textbf{affine space} is a generalization of a Euclidean space in which there is no particular point designated as the origin. As a result vectors can be viewed as displacements rather than points.\\\\
		Let $V$ be a vector space and $P$ be a set of points. Then we can form an affine space over $V$ and $P$ by defining the vectors in $V$ as displacements connecting members of $P$ such that, for ${ Q_1,Q_2 \in P,\, \v \in V }$,
		\[ Q_1 + \v = Q_2 \iff Q_2 - Q_1 = \v \iff Q_1 - Q_2 = -\v. \]
	}
	\boxeddefinition{A \textbf{frame} of an affine space is an extension of a basis of its underlying vector space to include a point designated as an origin. If ${ \v_1,\dots,\v_n }$ is a basis of a vector space $V$ and $Q$ is a point in the set of points $P$, then ${ F = (\v_1,\dots,\v_n, Q) }$ is a frame of the affine space over $V$ and $P$.}
	\boxeddefinition{The \textbf{dimension} of an affine space is the dimension of the underlying vector space.}
	
	\bigskip
	\labeledProposition{Any linear combination of points in an affine space where the coefficients sum to 0 results in a vector.}{sum_affine_points_with_coeffs_sum_to_zero_are_vectors}
	\begin{proof}
		Let $S$ be a sum of $n$ points ${ Q_i \in P }$ in an affine space associated with a vector space $V$ such that,
		\[ S = \sum_{i=0}^n \alpha_i Q_i  \eqand \sum_{i=0}^n \alpha_i = 0. \]
		Then, if we take the partial sum of the first two points,
		\[ S_2 = \alpha_1 Q_1 + \alpha_2 Q_2 = \alpha_1(Q_1 - Q_2) + (\alpha_1 + \alpha_2)Q_2 \]
		and then the next partial sum of the first three points,
		\begin{align*}
		S_3 &= \alpha_1(Q_1 - Q_2) + (\alpha_1 + \alpha_2)Q_2 + \alpha_3 Q_3 \\
		&= \alpha_1(Q_1 - Q_2) +  (\alpha_1 + \alpha_2)(Q_2 - Q_3) +  (\alpha_1 + \alpha_2 + \alpha_3) Q_3
		\end{align*}
		we can see that, by induction, the $n$th sum is,
		\begin{align*}
		S &= \alpha_1(Q_1 - Q_2) \\
		&\hspace{20pt} + (\alpha_1 + \alpha_2)(Q_2 - Q_3)\\
		&\hspace{20pt} + (\alpha_1 + \alpha_2 + \alpha_3)(Q_3 - Q_4)\\
		&\hspace{24pt} \vdots\\
		&\hspace{20pt} + (\alpha_1 + \cdots + \alpha_{n-1})(Q_{n-1} - Q_n)\\
		&\hspace{20pt} + (\alpha_1 + \cdots + \alpha_n)Q_n.
		\end{align*}
		But we have ${ \alpha_1 + \cdots + \alpha_n = 0 }$ so the final term is 0. As a result $S$ is a summation of terms of the form,
		\[ (\alpha_1 + \cdots + \alpha_{i-1})(Q_{i-1} - Q_i) \]
		where ${ Q_{i-1} - Q_i }$ is a vector. Therefore $S$ is a linear combination of vectors in $V$ and is therefore also a vector in $V$.
	\end{proof}
	\begin{corollary}
		\label{coro:sum_affine_points_with_coeffs_sum_to_one_are_vectors}
		Any linear combination of points in an affine space where the coefficients sum to 1 results in a point.
	\end{corollary}
	\begin{proof}
		In the preceding proof if we had, instead, ${ \alpha_1 + \cdots + \alpha_n = 1 }$ then the final term would equal $Q_n$ and the resulting summation would be a vector plus the point $Q_n$. Therefore the sum is a point.
	\end{proof}

	\pagebreak
	\subsubsection{Intuition of Affine Spaces}
	\begin{wrapfigure}{l}{0.5\textwidth}
		\includegraphics[width=0.9\linewidth]{\resourceDir/img/320px-Affine_space_R3.png} 
		\label{fig:affine_space_schematic}
	\end{wrapfigure}
	A translated linear subspace of a vector space like $P_2$ above that no longer passes through the origin is referred to as an \textbf{affine subspace}. It is not a vector space as ${ \0 \notin P_2 }$ and ${ \V{a},\V{b} \in P_2 }$ but ${ \V{a} + \V{b} \notin P_2 }$. 
	However, if we instead consider displacements between points --- e.g. ${ \V{b} - \V{a} }$ --- then we see the relationship between affine spaces and vector spaces: ${ \V{b} - \V{a} \in P_1 }$. The displacements between points in $P_2$ form a linear subspace. So we can define an affine space $A$ based on the set of points in $P_2$ and the vectors in $P_1$.
	

	\bigskip
	\subsubsection{Affine Combinations}
	\boxeddefinition{An \textbf{affine combination} of vectors is a combination such that the coefficients sum to 1.}
	\note{The definition of an affine combination differs from that of a convex combination in that the coefficients of a convex combination are additionally required to be non-negative.}
	
	\bigskip
	In an affine space there is no particular point designated as the origin but we can describe vector displacements between points as an ordered pair of points, for example, ${ (p,a) = \V{pa} }$. Affine combinations of displacements agree on the resulting point with linear combinations in the corresponding Euclidean space.\\
	For example, imagine a point $p$ in an affine space is at coordinates ${ (-1,4) }$ in the corresponding Euclidean space and similarly points $a$ and $b$ are at ${ (3,4) }$ and ${ (6,1) }$ respectively. Then, if we take an affine combination of the displacements to $a$ and $b$ the resulting point is independent of the chosen origin point.
%	\begin{wrapfigure}{l}{0.5\textwidth}
%		\includegraphics[width=\linewidth]{\resourceDir/img/affine_combinations_example.png} 
%		\label{fig:affine_combinations_example}
%	\end{wrapfigure}
%	
	
	\begin{figure}[h!]
		\centering
		\includegraphics[width=300px]{\resourceDir/img/affine_combinations_example.png}
		\caption{\small The diagram shows the affine combination ${ 2\V{a} - \V{b} = 2\V{pa} - \V{pb} }$ where $\V{a}$ denotes the usual position vector in the Euclidean space.}
		\label{fig:affine_combinations_example}
	\end{figure}

	
	
	\bigskip\bigskip\bigskip
	\subsubsection{Affine Transformations}
	\boxeddefinition{Let ${ T: A_1 \longmapsto A_2 }$ be a mapping between the affine spaces $A_1$ and $A_2$. Then $T$ is an \textbf{affine transformation} if:
		\begin{itemize}
			\item{$T$ maps vectors to vectors and points to points.}
			\item{$T$ is a linear transformation over the vectors in the underlying vector space.}
			\item{${ T(Q + \v) = T(Q) + T(\v) }$.}
		\end{itemize}
	}

	\medskip
	\note{In an affine space a \textbf{translation} can be regarded as a change of frame in which we \textbf{change the origin point} while the vector basis may remain unchanged.}
	
	\medskip
	\labeledProposition{Affine transformations preserve parallelism.}{affine_transformations_preserve_parallelism}
	\begin{proof}
		Let $A$ be an affine space defined over a set of points $P$ and a vector space $V$ and let ${ Q_1,Q_2 \in P }$ and ${ \v \in V }$. Then, for ${ s,t \in \F{} }$,
		\[ l_1 = Q_1 + t\v \eqand l_2 = Q_2 + s\v \]
		are parallel lines in $A$. Let $T$ be an arbitrary affine transformation over $A$. Then,
		\[ l_1' = T(l_1) = T(Q_1) + tT(\v) \eqand l_2' = T(l_2) = T(Q_2) + sT(\v) \]
		are also parallel.
	\end{proof}

	\medskip
	\labeledProposition{An affine transformation that preserves the dot product is left multiplication by an orthogonal matrix.}{dot-product-preserving-affine-trans-is-left-mult-by-ortho-matrix}
	\begin{proof}
		Firstly, note that if a transformation $m$ preserves the dot product and also fixes the standard basis vectors $\e{i}$ then,
		\[ m(\e{i}) = \e{i} \eqand x_i = \x \dotprod \e{i} = m(\x) \dotprod m(\e{i}) = m(\x) \dotprod \e{i} = m(\x)_i. \]
		Therefore, such a transformation $m$ is the identity transformation.\\
		Now, assume a transformation $m'$ preserves the dot product (but does not necessarily fix the standard basis vectors) in $\R{n}$ and let 
		\[ B' = \{ m'(\e{1}),\dots,m'(\e{n}) \} \]
		be the transformed standard basis vectors. Then, if $[B']$ is the matrix whose columns are the elements of $B'$ then, because $m'$ preserves the dot product, $[B']$ is an orthogonal matrix as, by \autoref{prop:orthogonal-matrices-are-subgroup}, is $\inv{[B']}$. So, composing this with $m'$,
		\[ m'' = \inv{[B']}m' \]
		is a transformation that both preserves the dot product and fixes the standard basis vectors. Therefore we have,
		\[ m'' = I_n = \inv{[B']}m' \iff [B'] = m' \]
		so that $m'$ is left multiplication by $[B']$, an orthogonal matrix.
	\end{proof}

	\bigskip\bigskip
	\subsubsubsection{Properties of affine transformations}
	We can characterize affine transformations according to their form and behaviour in the Euclidean spaces $\R{2}$ and $\R{3}$. Specifically, whether the transformation:
	\begin{enumerate}
		\item{Fixes the origin: ${ T(\0) = \0 }$}
		\item{Preserves the dot product: ${ T(\v)\dotprod T(\w) = \v\dotprod\w }$, matrix is orthogonal}
		\item{Preserves distances: ${ \norm{T(\v) - T(\w)} = \norm{\v - \w} }$}
		\item{Preserves angles: matrix is scalar multiple of orthogonal matrix}		
		\item{Preserves orientation: transformation does not include a reflection, determinant of matrix is positive}
		\item{Preserves parallelism: transformation exhibits affine property (linearity under affine combinations)}
	\end{enumerate}

	As we can see here, the property of preserving parallelism depends only on the affine property so clearly, all affine transformations exhibit this behaviour. As a consequence of preserving parallelism, affine transformations preserve the dimension of affine subspaces (points, lines, planes, etc.). They do not preserve distances between points however, but they do preserve ratios of distances between points lying on a straight line.\\
	
	\bigskip
	\subsubsubsection{Classes of affine transformations}
	The most common subclassifications of affine transformations are:
	\begin{itemize}
		\item{Linear: preserves the origin.}
		\item{Conformal: preserves angles.}
		\item{Isometry (also known as a congruent transformation): preserves distances in metric spaces and so also implicity, angles. In a Euclidean space these transformations are known as a Euclidean isometries or rigid transformations.}
		\item{Rigid Motion: Euclidean isometry / rigid transformation that also preserves orientation.}
	\end{itemize}

	\bigskip
	\subsubsection{Isometries}
	\bigskip
	A Euclidean isometry or rigid motion, for example, carries a triangle to a congruent triangle. So, it preserves distances and angles but not necessarily orientation (a reflection flips the orientation).\\
	The composition of two rigid motions is a rigid motion and the inverse of a rigid motion is also a rigid motion. Therefore, the rigid motions of $\R{n}$ form a group under composition of operations. This group is called the \textit{group of motions} and denoted $M_n$.
	
	\bigskip
	\labeledProposition{A rigid motion that fixes the origin preserves the dot product.}{non-translational-isometries-preserve-dot-product}
	\begin{proof}
		Let $m$ be a rigid motion that fixes the origin. Then $m$ is an isometry and so preserves distances and also $m$ maps the origin to the origin. So we have,
		\[ \norm{m(\v) - m(\w)} = \norm{\v - \w} \eqand m(\0) = \0. \]
		We can rewrite the isometry property as,
		\begin{align*}
		&& \sqrt{(m(\v) - m(\w))\dotprod(m(\v) - m(\w))} &= \sqrt{(\v - \w)\dotprod(\v - \w)}\\
		&\iff & (m(\v) - m(\w))\dotprod(m(\v) - m(\w)) &= (\v - \w)\dotprod(\v - \w). &\sidecomment{}
		\end{align*}
		Now, if we take the case where ${ \w = \0 = m(\w) }$,
		\begin{align*}
		&& (m(\v) - \0)\dotprod(m(\v) - \0) &= (\v - \0)\dotprod(\v - \0) \\
		&\iff & m(\v) \dotprod m(\v) &= \v \dotprod \v. &\sidecomment{}
		\end{align*}
		and we can deduce that ${ m(\x) \dotprod m(\x) = \x \dotprod \x }$ for any vector $\x$. Using this along with the properties of the dot product we obtain,
		\begin{align*}
		&& (m(\v) - m(\w))\dotprod(m(\v) - m(\w)) &= (\v - \w)\dotprod(\v - \w) \\
		&\iff & m(\v)\dotprod m(\v) + m(\w)\dotprod m(\w) - 2m(\v)\dotprod m(\w) &= \v\dotprod\v + \w\dotprod\w - 2\v\dotprod\w &\sidecomment{} \\
		&\iff & -2m(\v)\dotprod m(\w) &= -2\v\dotprod\w &\sidecomment{} \\
		&\iff & m(\v)\dotprod m(\w) &= \v\dotprod\w. &\sidecomment{}
		\end{align*}
		
		Therefore, $m$ preserves the dot product and, by \autoref{prop:dot-product-preserving-affine-trans-is-left-mult-by-ortho-matrix}, is left multiplication by an orthogonal matrix.
	\end{proof}
	\begin{corollary}
		\label{coro:isometry-fixes-origin-left-mult-by-orthogonal-matrix}
		A rigid motion that fixes the origin is left multiplication by an orthogonal matrix and, therefore, also a linear operator.
	\end{corollary}

	\medskip
	\labeledProposition{Left multiplication by any orthogonal matrix is a Euclidean isometry (a rigid motion) that fixes the origin.}{left-mult-by-ortho-matrix-is-rigid-motion}
	\begin{proof}
		By, \autoref{prop:left_mult_by_ortho_matrix_preserves_dot_product}, left multiplication by an orthogonal matrix preserves the dot product. So, if $m$ is an affine transformation such that ${ m(\x) = A\x  }$ where $A$ is an orthogonal matrix then,
		\begin{align*}
		&& \norm{m(\v - \w)}^2 = m(\v - \w)\dotprod m(\v - \w) &= (\v - \w)\dotprod (\v - \w) = \norm{\v - \w}^2 \\
		&\iff & \norm{m(\v - \w)} &= \norm{\v - \w} &\sidecomment{} \\
		\end{align*}
		But also, left multiplication by a matrix is a linear operator so ${ m(\v - \w) = m(\v) - m(\w) }$ meaning that,
		\[ \norm{m(\v - \w)} = \norm{m(\v) - m(\w)} = \norm{\v - \w} \]
		which is the isometry property for $m$.
	\end{proof}
	
	
	\bigskip\bigskip
	\subsubsubsection{Linear}
	Isometries that fix the origin are linear operators and rigid motions in Euclidean space.\\
		
	
	\bigskip
	\subsubsubsection{Translation}
	If $T$ is a translation then, in general, ${ T(\0) \neq \0 }$ so translations do not fix the origin and, as a result, are \textbf{not} linear transformations. However, translations preserve distances and angles (and orientation because there is no reflection) so they are rigid motions. For example:
	\begin{exe}
		\ex{Let ${ \v = (v_1,\dots,v_n) }$ be any fixed vector in $\R{n}$. Then translation by $\v$ is the map,
			\[ t_v(\x) = \x + \v = \begin{bmatrix}x_1 + v_1\\\vdots\\x_n + v_n\end{bmatrix} \]
			which can be seen to be isometric (a rigid transformation) by,
			\begin{align*}
			&& t_v(\x) - t_v(\V{y}) = (\x + \v) - (\V{y} +\v) &= \x - \V{y} \\
			&\implies & \norm{t_v(\x) - t_v(\V{y})} &= \norm{\x - \V{y}}. &\sidecomment{}
			\end{align*}
		}
	\end{exe}

	\medskip
	\labeledProposition{Every rigid motion $m$ is the composition of an orthogonal linear operator and a translation. In other words, for some orthogonal matrix $A$ and fixed vector $\v$, it takes the form,
		\[ m(\x) = A\x + \v. \]
	}{every_rigid_motion_is_linear_op_plus_translation}
	\begin{proof}
		Let ${ \v = m(\0) }$ and ${ t_v(\x) = \x + \v }$ with inverse ${ t_{-v}(\x) = \x - \v }$. Then composing this with $m$, the resulting transformation,
		\[ (t_{-v} \circ m)(\x) = m(\x) - \v \]
		continues to be isometric --- because it is the composition of isometric transformations --- and it fixes the origin because ${ (t_{-v} \circ m)(\0) = m(\0) - \v = m(\0) - m(\0) = \0 }$. It is therefore, by \autoref{coro:isometry-fixes-origin-left-mult-by-orthogonal-matrix}, left multiplication by an orthogonal matrix. So we can represent it as,
		\[ (t_{-v} \circ m)(\x) = t_{-v}(m(\x)) = A\x. \]
		Since ${ t_{-v} = \inv{t_v} }$ we can apply $t_{v}$ to both sides of the equation,
		\[ m(\x) = t_{v}(A\x) = A\x + \v. \]
		The obtained representation is uniquely determined by $m$ as ${ \v = m(\0) }$ is clearly unique and then the translation $t_{-v}$ is uniquely determined by $\v$ and then ${ A = (t_{-v} \circ m) }$ is unique for a given $\v$ and $m$.
	\end{proof}

	\medskip\note{For a rigid motion ${ m(\x) = A\x + \v }$, $m$ is orientation-preserving if the matrix $A$ is orientation-preserving and orientation-reversing if $A$ is orientation-reversing.}
	
	\bigskip
	\subsubsubsection{Rotation}
	Rotations preserve distances, angles and orientation and so are rigid motions. Rotations also fix a vector which is known as the axis of rotation. If the axis of rotation contains the origin then they fix the origin and so are linear operators.
	
	\medskip
	\labeledTheorem{The rotations of $\R{2}$ and $\R{3}$ about the origin are the linear operators whose matrices with respect to the standard basis are orthogonal and have determinant 1.}{rotations-of-r2-r3-about-origin-are-ortho-mat-det-1}
	\begin{proof}
		A rotation about the origin $m$ involves rotating the standard basis vectors through an angle $\theta$. It is in the definition of this rotation that the image of the standard basis vectors continue to subtend the same angle, ${ \pi/2 }$. Therefore, the rotation must preserve angles. It is also part of the definition that the image under rotation is not scaled so the rotation must preserve distances and must be a congruent transformation. Since the axis of rotation passes through the origin the origin is unchanged by this rotation and so these rotations are rigid motions that fix the origin and have the form,
		\[ m(\x) = A\x \]
		where $A$ is an orthogonal matrix. Additionally, rotations do not change the orientation of an shape and so their matrices have determinant 1.
	\end{proof}

	\medskip
	\note{The rotation matrices --- orthogonal matrices with determinant 1 --- form a subgroup of the group $O_n$ of orthogonal matrices called the \textbf{special orthogonal group} and denoted $SO_n$.}
	
	\labeledProposition{Every member of the special orthogonal group ${ A \in SO_2 }$ is the matrix of a rotation.}{all-members-SO2-are-rotations}
	\begin{proof}
		Let ${ A \in SO_2 }$. Then $A$ is a ${ 2 \times 2 }$ orthogonal matrix with determinant 1. Let $\v_1$ be the first column of $A$ which, since $A$ is orthogonal, is a unit vector. Now assume that $R$ is the matrix of a rotation whose first column is $\v_1$ --- which is possible because $\v_1$ is a unit vector so $R$ can be orthogonal. Then the matrix
		\[ B = \inv{R}A \]
		fixes $\e{1}$ and also, as the composition of two orthogonal vectors, is orthogonal. Therefore the second column of $B$ is a unit vector orthogonal to $\e{1}$ which could be $\e{2}$ or $-\e{2}$. 
		
		However, $R$ is an orthogonal matrix with determinant 1 and so is a member of $SO_2$ which means that ${ \inv{R}A = B }$ is also in $SO_2$. 
		
		This, in turn, means that $B$ has determinant 1 which implies that the second column of $B$ is not $-\e{2}$ and is, therefore, $\e{2}$. So, we have obtained the result that ${ B = I = \inv{R}A }$ which implies that ${ R = A }$.
	\end{proof}

	\bigskip
	\subsubsubsection{Rotating $\bm{\R{2}}$ about the origin}
	Rotating the 2-d plane about the origin means that the axis of rotation is just the origin (so the fixed vector is $\0$).
	
	For example:
	\begin{exe}
		\ex{A rotation $\rho_{\theta}$ of the plane through an angle $\theta$ is a linear operator on $\R{2}$ whose matrix with respect to the standard basis is
			\[ R = 	\begin{bmatrix}
			\cos\theta & -\sin\theta\\
			\sin\theta & \cos\theta
			\end{bmatrix}.
			\]
			We can see that this is a rotation if we take ${ \x = (x_1,x_2)^T \in \R{2} }$ and write it in polar coordinates,
			\[ \x = (r,\alpha). \]
			So, relating the polar and rectangular coordinates,
			\[ \x = (r\cos\alpha, r\sin\alpha)^T. \]
			When we left-multiply by $R$,
			\begin{align*}
			R\x &= \begin{bmatrix}
			\cos\theta & -\sin\theta\\
			\sin\theta & \cos\theta
			\end{bmatrix}
			\begin{bmatrix}
			r\cos\alpha\\
			r\sin\alpha
			\end{bmatrix} \\
			&= \begin{bmatrix}
			r\cos\alpha\cos\theta - r\sin\alpha\sin\theta\\
			r\cos\alpha\sin\theta + r\sin\alpha\cos\theta
			\end{bmatrix} \\
			&= 	\begin{bmatrix}
			r\cos{(\alpha + \theta)}\\
			r\sin{(\alpha + \theta)}
			\end{bmatrix}.
			\end{align*}
			Note that $R$ is orthogonal because
			\[  \begin{bmatrix}
			\cos\theta\\
			\sin\theta
			\end{bmatrix} \dotprod 
			\begin{bmatrix}
			-\sin\theta\\
			\cos\theta
			\end{bmatrix} = -\cos\theta\sin\theta + \sin\theta\cos\theta = 0 \]
			and ${ det\,R = 1 }$,
			\[ 
			\begin{vmatrix}
			\cos\theta & -\sin\theta\\
			\sin\theta & \cos\theta
			\end{vmatrix} = \cos^2\theta + \sin^2\theta = 1.
			\]
		}
	\end{exe}

	\bigskip
	\subsubsubsection{Rotating $\bm{\R{3}}$ about the origin}	 
	\boxeddefinition{Define $\rho$ as a rotation in $\R{3}$ around the origin if:
	\begin{enumerate}[label=(\roman*)]
		\item{$\rho$ is a rigid motion (orientation-preserving Euclidean isometry) that fixes the origin;}
		\item{$\rho$ also fixes a nonzero vector $\v$;}
		\item{$\rho$ operates as a rotation on the plane $P$ orthogonal to $\v$.}
	\end{enumerate}}
	\note{Note that this definition could be described as selecting a 2-dimensional subspace of $\R{3}$ and performing a 2-dimensional rotation on it as if it were $\R{2}$.}
	Condition (i) implies, by \autoref{coro:isometry-fixes-origin-left-mult-by-orthogonal-matrix}, that $\rho$ is left multiplication by an orthogonal matrix. Condition (ii) states that $\rho$ has an eigenvector $\v$ with eigenvalue 1. Then, because $\rho$ preserves angles, the plane $P$ referenced in condition (iii) that is orthogonal to the eigenvector $\v$, must map to a plane that is orthogonal to the map of $\v$ in the image of $\rho$. But $\v$ is fixed by $\rho$ and is unchanged in the image. Also $\v$ uniquely identifies a plane orthogonal to it. Therefore the plane $P$ is unchanged in the image also. In other words, $P$ is an invariant subspace. So, condition (iii) says that the restriction of $\rho$ to this invariant subspace is a rotation.
	
	\bigskip
	For example:
	\begin{exe}
		\ex{A rotation of $\R{3}$ about the origin can be described by a pair $(\v, \theta)$ consisting of a unit
			vector $\v$, a vector of length 1, which lies in the axis of rotation, and a nonzero angle
			$\theta$, the angle of rotation. The two pairs $(\v, \theta)$ and $(-\v, -\theta)$ represent the same rotation.
			We also consider the identity map to be a rotation, though its axis is indeterminate.\\
			The matrix representing a rotation through the angle $\theta$ about the vector $\e{1}$ is obtained easily from the $2 \times 2$ rotation matrix. It is
			\[ A = \begin{bmatrix}
					1 & 0 & 0\\
					0 & \cos\theta & -\sin\theta\\
					0 & \sin\theta & \cos\theta
					\end{bmatrix}.
			\]
			Multiplication by $A$ fixes the first coordinate $x_1$ of a vector and operates by rotation on $(x_2, x_3)^T$. All rotations of $\R{3}$ are linear operators, but their matrices can be fairly complicated.
		}
	\end{exe}

	\bigskip
	\labeledProposition{Every element of $SO_3$ has eigenvalue 1.}{elements-of-SO3-have-eigenvalue-1}
	\begin{proof}
		Let ${ A \in SO_3 }$. Then $A$ is an orthogonal ${ 3 \times 3 }$ matrix with determinant equal to 1. Reasoning from orthogonality of $A$ we have,
		\begin{align*}
		&& A^TA &= I \\
		&\iff & A^TA - A^T &= I - A^T &\sidecomment{} \\
		&\iff & A^T(A - I) &= I - A^T &\sidecomment{} \\
		&\iff & A^T(A - I) &= (I - A)^T. &\sidecomment{by \autoref{prop:matrix-transpose-distributes-over-addition}}
		\end{align*}
		If we take the determinants of both sides of this equation we obtain,
		\begin{align*}
		&& det(A^T)\cdot det(A - I) &= det((I-A)^T) &\sidecomment{by \autoref{prop:determinant_of_matrix_product}} \\
		&\iff & det(A)\cdot det(A - I) &= det(I - A) &\sidecomment{by \autoref{prop:determinant_of_matrix_equal_to_its_transpose}} \\
		&\iff & det(A - I) &= det(I - A). &\sidecomment{${ det(A) = 1 }$}
		\end{align*}
		But the dimension of $A$ being 3 implies that
		\[ det(-A) = (-1)^3 det(A) = -det(A) \]
		so that,
		\[ det(A - I) = det(I - A) \iff det(A - I) = 0. \]
		Therefore $A$ has the eigenvalue 1.
	\end{proof}

	\bigskip
	\labeledProposition{The elements of $SO_3$ are precisely the rotations about the origin of $\R{3}$.}{elements-of-SO3-are-the-rotations-around-origin-in-R3}
	\begin{proof}
		Let ${ \rho: \R{3} \longmapsto \R{3} }$ be defined as ${ \rho(\x) = A\x }$ where ${ A \in SO_3 }$. Then,
		\begin{itemize}
			\item{by \autoref{prop:left-mult-by-ortho-matrix-is-rigid-motion} and orthogonality of ${ A \in SO_3 }$, left multiplication by $A$ is a rigid motion that fixes the origin. So $\rho$ is isometric and fixes the origin;}
			\item{\autoref{prop:elements-of-SO3-have-eigenvalue-1} shows that every ${ A \in SO_3 }$ has eigenvalue 1 which implies that $\rho$ fixes a nonzero vector;}
			\item{if we let $\v$ be the nonzero vector fixed by $\rho$ (i.e. its eigenvector with eigenvalue 1), then we can normalize it to find the unit vector parallel to $\v$, say $\u_1$. Next we can find two unit vectors orthogonal to $\u_1$ --- say $\u_2$ and $\u_3$ --- and these must be a basis for the plane orthogonal to $\v$. Furthermore, if we select $\u_2$ and $\u_3$ to be orthogonal to each other than ${ B = \{\u_1,\u_2,\u_3\} }$ is an orthonormal basis of $\R{3}$.\\
				Now if we define ${ P = \inv{[B]} }$ then,
				\[ A' = \inv{P}AP \]
				is similar to the matrix $A$ and so has the same determinant, 1. Furthermore, because $B$ is an orthonormal basis, the matrices ${ [B],\, \inv{[B]} = P }$ are orthogonal. Since both $P$ and $A$ are orthogonal, ${ \inv{P}AP = A' }$ is orthogonal also. Since $A'$ is orthogonal and has determinant equal to 1, it is a member of $SO_3$.\\
				If we examine the structure of $A'$, we see that the first column of $A'$ is $\v_1$ --- the unit vector in the direction of $\v$. Since $\v$ is an eigenvector of $\rho$ with eigenvalue 1, the first column of $A'$ is $\e{1}$ and since $A'$ is orthogonal, the other columns are orthogonal to the first. So the block structure of $A'$ looks like,
				\[ A' = \begin{bmatrix}
						1 & 0 \\
						0 & R
						\end{bmatrix} 
				\]
				where $R$ is a ${ 2 \times 2 }$ matrix.\\
				We know that the determinant of $A'$ is 1 and this implies that the determinant of $R$ is also 1. Furthermore, $R$ must also be orthogonal and so ${ R \in SO_2 }$. So, by \autoref{prop:all-members-SO2-are-rotations}, $R$ is a rotation. Therefore $R$ represents a rotation of the plane orthogonal to $\v$ and this implies that $\rho$ rotates the plane orthogonal to $\v$ as required.
			}
		\end{itemize}
	\end{proof}
	
	\bigskip
	\subsubsubsection{Uniform Scaling}
	Uniform scaling is a scalar multiple of an orthogonal matrix and is therefore a linear operator which means that it fixes the origin. It also preserves angles but not distances between points.
	
	
	\bigskip\bigskip
	\subsubsection{Conformal Transformations}
	\bigskip
	\subsubsubsection{Non-uniform Scaling}
	Non-uniform scaling, however, its matrix is not a scalar multiple of an orthogonal matrix and, as such, it does not preserve angles. An important example is:
	\begin{exe}
		\ex{\textbf{Mercator projection}:\\\\
			This is a map projection that was designed so that rhumb lines (lines of constant bearing over the surface of the earth) are straight lines on the map. To achieve this the projection ensures that a square on the surface of the earth presents as a square on the map.\\
			Then, modelling the earth as a sphere of radius $R$, if lines of latitude are horizontal grid lines across the map, then each actual line of latitude with circumference ${ 2\pi R \cos\phi }$ where $\phi$ is the angle of latitude will present on the map as the same length as the equator, which in reality is ${ 2\pi R }$. So they appear to be a line of length ${ \cos\phi }$ times longer than they actually are, i.e. they are stretched by ${ \sec{\phi} }$.\\
			So, the map projection will stretch the width of a square on the surface of the earth by ${ \sec{\phi} }$ and to maintain it as a square, it is necessary to stretch the height of the square by the same amount, ${ \sec{\phi} }$. The actual square on the surface of the earth has height (approximately for a small square) ${ R\Delta\phi }$ so, on the map we need a height ${ \Delta y \propto \Delta\phi\sec\phi }$. Therefore, we have,
			\[ \frac{dy}{d\phi} = \sec\phi \implies y = \ln{(\tan\phi + \sec\phi)} + c \]
			where $c$ is a constant that we can set to 0. 
			See \url{https://www.math.ubc.ca/~israel/m103/mercator/mercator.html} for a description of this derivation. For more information on conformal map projections generally see: \href{https://arxiv.org/pdf/1412.7690.pdf}{Map Projection, York University, Toronto} and \href{http://diposit.ub.edu/dspace/bitstream/2445/121898/2/memoria.pdf}{Conformal Cartographic
				Representations - University of Barcelona}.
		}
	\end{exe}
	
	\subsubsubsection{Reflection}
	Reflection usually refers to a Euclidean Isometry (rigid transformation over a Euclidean space) that fixes a hyperplane (so a line in $\R{2}$ and a plane in $\R{3}$) but does not preserve orientation so is not a rigid motion. However, it may also refer to a transformation that fixes an affine space of lower dimension than a hyperplane --- for example, reflection in a point --- in which case it does preserve orientation and is, therefore, a rigid motion (in fact, reflection in the origin in $\R{2}$ is equal to rotation by $\pi$). Reflection may fix the origin or may not, depending on whether or not the origin is contained in the affine space fixed by the reflection.
	
	\bigskip
	\subsubsection{Non-Rigid non-Conformal Transformations}
	\bigskip
	\subsubsubsection{Shear}
	Shear neither preserves distances nor angles. It does preserve parallelism though (as do all affine transformations) and it also fixes the origin, so it is a linear operator.
	
	\bigskip\bigskip
	\note{For more in-depth treatment of affine spaces and transformations see:
		\begin{itemize}
			\item{First two lectures of \href{https://web.ma.utexas.edu/users/dafr/M375T/}{University of Texas - Multivariable Analysis}.}
			\item{\url{https://www.maa.org/sites/default/files/pdf/pubs/books/meg/meg_ch12.pdf}}
		\end{itemize}
	}
}


% ---------------- break -------------
\pagebreak

\searchableSubsection{Projections}{linear transformations, projections}{
	\biggerskip
	\subsubsection{Linear Operators as Projections}
	\boxeddefinition{A linear operator ${ T: V \longmapsto V }$ is a \textbf{projection} on ${ W_1 \subseteq V }$ if
		\begin{enumerate}[label=(\roman*)]
			\item{there exists a subspace ${ W_2 \subseteq V }$ such that ${ W_1 \oplus W_2 = V }$;}
			\item{for ${ \v = \w_1 + \w_2 \in V }$ where ${ \w_1 \in W_1, \, \w_2 \in W_2 }$, we have
				\[ T\v = T(\w_1 + \w_2) = \w_1. \]
			}
		\end{enumerate}
	}

	\biggerskip
	\subsubsubsection{Intuition}
	If we attempt to project the $x$-axis of $\F{3}$ onto the $z$-axis,
	\[ T\left( \begin{bmatrix}1\\0\\0\end{bmatrix} \right) = \begin{bmatrix}0\\0\\1\end{bmatrix} \]
	then this leads to the matrix,
	\[ A = 	\begin{bmatrix}
				0 & 0 & 0\\
				0 & 0 & 0\\
				1 & 0 & 0
			\end{bmatrix}.
	\]
	But this is \textbf{not} a projection because ${ A^2 = 0 \neq A }$. The $z$-axis is not invariant under $A$; in fact, $A$ maps the $z$-axis to the origin.\\
	
	If we try to fix this by also satisfying,
	\[ T\left( \begin{bmatrix}0\\0\\1\end{bmatrix} \right) = \begin{bmatrix}0\\0\\1\end{bmatrix} \]
	we get the matrix,
	\[ A = 	\begin{bmatrix}
				0 & 0 & 0\\
				0 & 0 & 0\\
				1 & 0 & 1
			\end{bmatrix}.
	\]
	\textit{This} matrix $A$, \textit{does} satisfy ${ A^2 = A }$ and \textit{is} therefore a projection of the $x$-axis onto the $z$-axis.\\
	
	
	\biggerskip
	\subsubsubsection{Deductions}
		
	\labeledProposition{If $T$ is a linear operator over a vector space $V$ and also, $T$ is a projection of $V$ onto $W$, then $W$ is $T$-invariant and the restriction of $T$ to $W$, $T_w$ is the identity operator on $W$, $I_w$.}{restriction-to-subspace-of-projection-onto-same-subspace-is-identity-operator}
	\begin{proof}
		If $T$ is a projection of $V$ onto $W$ then, for all ${ \v \in V }$ there is some ${ \w \in W }$ such that,
		\[ T\v = \w. \]
		Therefore, since ${ W \subseteq V }$ we also have, for all ${ \w_1 \in W }$ and some ${ \w_2 \in W }$,
		\[ T\w_1 = \w_2 \in W \implies TW \subseteq W. \]
		So $W$ is shown to be $T$-invariant.\\
		
		Since $W$ is $T$-invariant, we can define the restriction of $T$ to $W$,
		\[ T_w : W \longmapsto W \suchthat T_w\w_1 = T\w_1 = \w_2. \]
		But also, since $T$ is a projection onto $W$, it is also idempotent,
		\[ T^2 = T \implies T^2\,\v = T\v \implies {T_w}^2\,\w = T^2\,\w = T\w = T_w\w. \]
		Putting these two together we obtain, for any ${ \w_1,\w_2 \in W }$ such that ${ T_w\w_1 = \w_2 }$,
		\[ {T_w}^2\,\w_1 = T_w\w_1 = \w_2 = T_w(T_w\w_1) = T_w\w_2 = {T_w}^2\,\w_2. \]
		We have obtained 
		\[ T_w\w_2 = \w_2 \]
		which implies that ${ T_w = I_w }$, the identity operator on $W$.
	\end{proof}
}

\end{document}