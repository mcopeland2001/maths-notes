\documentclass[../MathsNotesBase.tex]{subfiles}


\date{\vspace{-6ex}}


\begin{document}
\searchableSection{Linear Transformations}{linear algebra}

	\searchableSubsection{\texorpdfstring{Basic Properties of Linear\\ Transformations}{Basic Properties of Linear Transformations}}{linear algebra}{
		\label{ssection:basic-properties-linear-transformations}
		\bigskip\bigskip
		
		The analogue for vector spaces of a homomorphism of groups is a map,
		\[ T: V \longmapsto W \]
		from one vector space over a field $\F{}$ to another, which is compatible with addition and scalar multiplication:
		\[ T(\V{v_1} + \V{v_2}) = T(\V{v_1}) + T(\V{v_2}) \eqand T(c\V{v_1}) = cT(\V{v_1}), \]
		for all ${ \V{v_1},\V{v_2} \in V,\, c \in \F{} }$.\\
		
		\note{Note that another way of describing this is that \textbf{linear combinations are preserved across linear transformations}. That's to say, if
			\[ \u = \alpha_1 \v_1 + \cdots + \alpha_n \v_n \eqand \w = \alpha_1 f(\v_1) + \cdots + \alpha_n f(\v_n) \]
			then, if $f$ is linear we also have,
			\[ f(\u) = \w. \]
		}
		
		\boxeddefinition{A homomorphism between two vector spaces that is also compatible with scalar multiplication is called a \textbf{linear transformation} or \textbf{linear map or mapping}. That's to say, if $T$ is a linear map over a vector space defined over a field $\F{}$; each $\v_i$ is drawn from the vector space; and each $\alpha_i$ is drawn from $\F{}$ then,
			\[ T\left( \sum_i \alpha_i \v_i \right) = \sum_i \alpha_i T(\v_i). \]
		}
		
		\note{The compatibility with addition of vectors implies that a linear transformation is a homomorphism between additive groups of vectors.}
		
		\note{Linear transformations \textbf{preserve} linear combinations in their arguments but this must be distinguished carefully from \textbf{being equal} to linear combinations of their arguments. A typical linear transformation is \textbf{not} expressible as a linear combination of it's sole argument and --- in fact --- the image of a vector under a linear map only fails to be linearly independent to the original vector in the case that the original vector is an eigenvector ${ A\v = c\v. }$\\ It is, however, worth noting that a linear map between finite vector spaces may be thought of as the application of a coordinate vector to a different basis than that against which it was originally defined. Therefore, any linear combination of objects may be thought of as a linear map between the space of coefficients and the space of objects. For example, the linear combination,
			\[ \alpha_1 x_1 + \alpha_2 x_2 + \cdots + \alpha_n x_n \]
		may be thought of as a linear map from the space of coefficients ${ \alpha_i }$ to the space of objects ${ x_i }$,
			\[ T\left(\begin{bmatrix}\alpha_1\\\alpha_2\\\vdots\\\alpha_n\end{bmatrix}\right) = \alpha_1 x_1 + \alpha_2 x_2 + \cdots + \alpha_n x_n. \]
		}
		
		\begin{corollary}\label{coro:linear-maps-map-the-origin-to-itself}
			As with all homomorphisms, linear maps always map the identity to the identity. For linear maps this means mapping the zero vector to the zero vector.
		\end{corollary}
		
		\bigskip
		\labeledProposition{A linear map is homogeneous of degree 1.}{lin-map-is-homogeneous-degree-1}
		\begin{proof}
			${ L(\alpha \v) = \alpha L(\v) }$ for all ${ \alpha \in \F{},\; \v \in V }$.
		\end{proof}

		
		\medskip
		\labeledProposition{Linear Dependence is always preserved across any linear transformation.}{lin-dependence-preserved-across-any-lin-transform}
		\begin{proof}
			Let ${ \alpha_1 \v_1 + \alpha_2 \v_2 + \cdots + \alpha_n \v_n = \0 }$ be a linear relation between the vectors ${ \{\v_1,\dots,\v_n\} }$. If $L$ is a linear map then,
			\begin{align*}
				&& L(\alpha_1 \v_1 + \alpha_2 \v_2 + \cdots + \alpha_n \v_n) &= L(\0) \\
				&\iff & \alpha_1 L(\v_1) + \alpha_2 L(\v_2) + \cdots + \alpha_n L(\v_n) &= \0. \qedhere \\
			\end{align*}
		\end{proof}
		
		
		\biggerskip
		\subsubsection{The Kernel and the Image of a Linear Transformation}
		\boxeddefinition{
			Let ${ T: V \longmapsto W }$ be any linear transformation. Then the \textbf{kernel (or nullspace)} of $T$ is defined as,
		\[ ker\,T = \setc{\v}{T(\v) = \0}  \]
		and the \textbf{image} of $T$ as,
		\[ im\,T = \setc{\w \in W}{\exists \v \in V \logicsep \w = T(\v)}. \]
		}
		
		\medskip
		\labeledProposition{The kernel of ${ T: V \longmapsto W }$ is a subspace of $V$ and the image is a subspace of $W$.}{kernel_and_image_of_linear_map_are_subspaces}
		\begin{proof}
			$T$ is a homomorphism between additive groups of vectors and so the proof that the kernel and image are subspaces is the same as for general homomorphisms (see \ref{sssection:image_of_homomorphism}).
		\end{proof}
	
		\medskip
		\labeledProposition{The fibres of a linear transformation are the additive cosets of the kernel.}{lin_transform_fibres_are_additive_cosets_of_kernel}
		\begin{proof}
			Let ${ T: V \longmapsto W }$ be any linear transformation with kernel ${ K = ker\,T }$. Then, for any fixed ${ \v \in V }$,
			\[ \forall \V{k} \in K \logicsep T(\v + \V{k}) = T(\v) + T(\V{k}) = T(\v) + \0 = T(\v). \]
			So every element in the additive coset ${ \v + K }$ maps to the same value $T(\v)$ in $W$. Therefore we have,
			\[ im\,T = \setc{\w \in W}{\exists (\v + K) \subseteq V \logicsep \{\w\} = T(\v + K)}. \]
			We could also express this using the inverse image as in \ref{def:inverse_image} but the notation would be easily confused for the inverse transformation.
		\end{proof}
		\smallskip
		\begin{corollary}
			Let ${ T: V \longmapsto W }$ be any linear transformation with kernel ${ K = ker\,T }$ and let there be ${ \v \in V,\, \w \in W }$ such that 
				\[ T(\v) = \w. \] 
			Then,
				\[ T(\x) = \w \iff \x \in (\v + K). \]
		\end{corollary}
		\begin{corollary}\label{coro:linear-transformation-is-injective-iff-kernel-is-trivial}
			Let ${ T: V \longmapsto W }$ be any linear transformation with kernel ${ K = \operatorname{ker} T }$. $T$ is injective iff the kernel is trivial ${ K = \{\0\} }$.
		\end{corollary}
	
		\medskip
		\labeledProposition{Linear Independence is preserved across a linear transformation iff the transformation is injective.}{linear_independence_not_preserved_across_homomorphic_lin_transform}
		\begin{proof}
			Let ${ T: V \longmapsto W }$ be any linear transformation with kernel ${ K = ker\,T }$. If the kernel is nontrivial then there exists a nonempty basis of the kernel ${ B_K = \{ \V{k_1}, \dots, \V{k_n} \} }$. Being a basis $B_K$ is linearly independent so that,
			\[ \alpha_1\V{k_1} + \cdots + \alpha_n\V{k_n} = \0 \iff \alpha_1,\dots,\alpha_n = 0. \]
			However, $T(B_K)$ the image of $B_K$ under $T$ is
			\[ \{ \0, \dots, \0 \} \]
			which, by \autoref{prop:set_containing_zero_vector_not_linearly_independent}, is obviously not linearly independent.\\\\
			We can also see it more directly from the definition of a linear relation.
			\begin{align*}
			&& T(\alpha_1\V{k_1} + \cdots + \alpha_n\V{k_n}) &= \0 \\
			&\iff & \alpha_1T(\V{k_1}) + \cdots + \alpha_nT(\V{k_n}) &= \0  &\sidecomment{} \\
			&\iff & \alpha_1\0 + \cdots + \alpha_n\0 &= \0  &\sidecomment{}
			\end{align*}
			So clearly,
			\[ \alpha_1T(\V{k_1}) + \cdots + \alpha_nT(\V{k_n}) = \0 \centernot\implies \alpha_1,\dots,\alpha_n = 0 \]
			which proves that if $T$ is not injective then it does not preserve linear independence.\\\\
			Conversely, if $T$ does not preserve linear independence then, if ${ U = \{ \u_1,\dots,\u_n \} \subset V }$ is a linearly independent set in $V$, there is a nontrivial linear relation between the vectors in $T(U)$ the image of $U$ under $T$. That's to say,
			\[ \alpha_1T(\u_1) + \cdots + \alpha_nT(\u_n) = \0  \hspace{10pt}\text{ where } \prod_{i=1}^{n}\alpha_i \neq 0. \]
			But this means that,
			\begin{align*}
			&& \alpha_1T(\u_1) + \cdots + \alpha_nT(\u_n) &= \0 \\
			&\iff & T(\alpha_1\u_1 + \cdots + \alpha_n\u_n) &= \0 &\sidecomment{} \\
			&\iff & \alpha_1\u_1 + \cdots + \alpha_n\u_n \in ker\,T. &\sidecomment{}
			\end{align*}
			Since $U$ is linearly independent there is no nontrivial linear relation between its elements and since ${ \prod_{i=1}^{n}\alpha_i \neq 0 }$ we can conclude that,
			\[ \alpha_1\u_1 + \cdots + \alpha_n\u_n \neq \0 \implies ker\,T \neq \{\0\} \]
			and therefore this proves that if $T$ does not preserve linear independence then it is not injective.
		\end{proof}
	
		\note{Note that \textbf{linear dependence}, however, is preserved across any linear map (\autoref{prop:lin-dependence-preserved-across-any-lin-transform}).}
		
		\bigskip
		\subsubsection{Examples of Linear Transformations}
		\begin{exe}
			\item{As previously seen in section \ref{ssection:matrices_as_linear_transformations}, matrix multiplication on the left is a linear transformation. Let $A$ be an ${ m \times n }$ matrix with entries in $\F{}$ and consider $A$ as an operator on column vectors ${ A: \F{n} \longmapsto \F{m} }$. The kernel of $A$ is the set of vectors that are solutions to ${ A\V{x} = \0 }$ while the image (or range) is the set of vectors $\V{b}$ such that ${ A\V{x} = \V{b} }$ has a solution.\\
				The solutions ${ \setc{\V{x} \in \F{n}}{A\V{x} = \V{b}} }$ for some fixed ${ \V{b} \in \F{m} }$ are the additive coset ${ \v + K }$ where $K$ is the kernel of $A$ and ${ \v \in \F{n} }$ is such that ${ A\v = \V{b} }$. Compare with \ref{ex:coset_with_kernel}.
			}
			\item{Also previously seen in \ref{ssection:polynomials_as_vector_spaces} is that polynomials can be modeled as vectors. Let $P_n$ be the vector space of real polynomials of degree ${ \leq n }$. Then the derivative is a linear transformation ${ P_n \longmapsto P_{n-1} }$. The kernel of the derivative is the set of degree 0 polynomials (i.e. constant functions) and the additive cosets of the kernel are ${ f(x) + c }$, for ${ f(x) \in P_n }$ and constant $c$.
			}
		\end{exe}
		
		
		\bigskip\bigskip
		\subsubsection{The Dimension of a Linear Transformation}
		\boxeddefinition{The dimension of the image is called the \textbf{rank} while the dimension of the kernel is known as the \textbf{nullity}.}
		
		\medskip\note{Dimension and rank also exist for Groups (see \href{https://en.wikipedia.org/wiki/Rank_of_a_group}{wikipedia}) where it refers to the minimal generating set for the group.}
		
		\bigskip
		\labeledTheorem{\textbf{(The Dimension Formula.)} Let ${ T: V \longmapsto W }$ be a linear transformation, and assume that $V$ is finite dimensional. Then,
			\[ \dim V = \dim (\operatorname{ker} T) + \dim (\operatorname{im} T) = \operatorname{rank} + \operatorname{nullity}. \]
		}{linear_map_dimension_formula}
		\begin{proof}
			Let ${ \{\V{k_1}, \dots, \V{k_m}\} }$ be a basis of ${ \operatorname{ker} T }$. Then, by \autoref{prop:lin_ind_set_can_be_extended_to_basis}, it may be extended to a basis of $V$,
			\[ B = \{\V{k_1}, \dots, \V{k_m}, \V{u_1}, \dots, \V{u_n}\}. \]
			So, for any ${ \v \in V }$, $\v$ may be expressed as a linear combination of the vectors in $B$. Therefore, for any ${ \w \in \operatorname{im} T }$,
			\begin{align*}
			&& \w &= T(\alpha_1\V{k_1} + \cdots + \alpha_m\V{k_m} + \beta_1\V{u_1} + \cdots + \beta_n\V{u_n})  \\
			&\iff &  &= T(\alpha_1\V{k_1}) + \cdots + T(\alpha_m\V{k_m}) + T(\beta_1\V{u_1}) + \cdots + T(\beta_n\V{u_n}) &\\
			&\iff &  &= \0 + T(\beta_1\V{u_1}) + \cdots + T(\beta_n\V{u_n})   &\sidecomment{} \\
			&\iff &  &= \beta_1T(\V{u_1}) + \cdots + \beta_nT(\V{u_n})   &\sidecomment{} \\
			\end{align*}
			This shows that ${ B' = \{T(\V{u_1}), \dots, T(\V{u_n})\} }$ spans $im\,T$. Furthermore, if there were a linear relation between the elements of $B'$ then,
			\begin{align*}
			&& \beta_1T(\V{u_1}) + \cdots + \beta_nT(\V{u_n}) &= \0 \\
			&\iff & T(\beta_1\V{u_1} + \cdots + \beta_n\V{u_n}) &= \0 & \\
			&\iff & \beta_1\V{u_1} + \cdots + \beta_n\V{u_n} &\in ker\,T &\sidecomment{} \\
			&\iff & \beta_1\V{u_1} + \cdots + \beta_n\V{u_n} &= \alpha_1\V{k_1} + \cdots + \alpha_m\V{k_m} &\sidecomment{} \\
			\end{align*}
			where this last result implies a linear relation between the vectors of $B$. Since $B$ is a basis this linear relation can only be the trivial relation and so ${ \beta_1, \dots, \beta_n = 0 }$ and $B'$ is linearly independent also.
		\end{proof}
		
		\medskip\note{Notes about the Dimension Formula:
			\begin{itemize}
				\item{The Dimension Formula does not imply that the range of a linear operator and its kernel partition the space (as in a direct sum). The kernel may be in the range. For example, the operator
					\[ T(a,b) = (0,a) \]
				has equal range and nullspace as
				\[ R(T) = N(T) = \setc{(0,y)}{y \in \F{}}. \]
				In this case ${ T^2 = T_0 }$, the zero operator.
				}
				\item{This formula bears a resemblance to Lagrange's Theorem applied to homomorphisms of finite groups (\ref{coro:order_of_image_divides_both_order_of_domain_and_codomain}),
					\[ \cardinality{G} = \cardinality{ker\,\phi} \cdot \cardinality{im\,\phi}. \]
					The difference, however, is that the Dimension Formula of Linear Transformations is dealing with the generators of a group while Lagrange's Theorem is dealing with the orders of the groups. The orders of the groups are the number of elements in the group that are generated by the generators of the group. In the case of a real vector space, the vectors generated by the basis vectors are uncountably infinite due to scalar multiplication by real numbers and so cardinality doesn't apply in the same way.}
				\item{This formula \textbf{only applies to finite-dimensional vector spaces}. This should be clear as we simply cannot do this kind of arithmetic with $\infty$. For example, if the rank is infinite then the dimension of the kernel would be ${ \infty - \infty = ? }$.}
			\end{itemize}
		}
	
		
		\bigskip
		\labeledTheorem{If ${ T: V \longmapsto W }$ is a linear transformation over a finite-dimensional vector space $V$, then the quotient space of $V$ by the kernel of $T$ is a space that is in bijective correspondence to the range of $T$.}{for-lin-map-quotient-by-kernel-is-bijection-with-range}
		\begin{proof}
			Let ${ B_K = \{ \V{k}_1, \dots, \V{k}_k \} }$ be a basis of the kernel of $T$. By \autoref{prop:lin_ind_set_can_be_extended_to_basis}, it may be extended to a basis of $V$,
			\[ B = \{ \V{k}_1, \dots, \V{k}_k, \b_1, \dots, \b_{n-k} \}. \]
			Then, for any ${ \v \in V }$,
			\[\begin{aligned}
				T\v &= T(\alpha_1 \V{k}_1 + \cdots + \alpha_k \V{k}_k + \alpha_{k+1} \b_1 + \cdots + \alpha_n \b_{n-k}) \\
				&= T(\alpha_1 \V{k}_1 + \cdots + \alpha_k \V{k}_k) + T(\alpha_{k+1} \b_1 + \cdots + \alpha_n \b_{n-k}) &\sidecomment{} \\
				&= \0 + T(\alpha_{k+1} \b_1 + \cdots + \alpha_n \b_{n-k}) &\sidecomment{} \\
				&= \alpha_{k+1} T(\b_1) + \cdots + \alpha_n T(\b_{n-k}) &\sidecomment{} \\
			\end{aligned}\]
			Therefore ${ B_R = \{ T(\b_1), \dots, T(\b_{n-k}) \} }$ spans the range of $T$. A linear relation between the elements of $B_R$ would imply,
			\[\begin{aligned}
				&& \alpha_1 T(\b_1) + \cdots + \alpha_{n-k} T(\b_{n-k}) &= \0 \\
				&\iff & T(\alpha_1 \b_1 + \cdots + \alpha_{n-k} \b_{n-k}) &= \0,
			\end{aligned}\]
			which means that the vector,
			\[ \alpha_1 \b_1 + \cdots + \alpha_{n-k} \b_{n-k} \in \operatorname{ker} T \]
			which is impossible by construction of the basis $B$ as it would imply a linear relation with the elements of $B_K$.\\
			Therefore, $B_R$ is linearly independent and a basis of the range (i.e. the image) of $T$. Furthermore, ${ B_{K'} = \{  \b_1, \dots, \b_{n-k} \} }$  is a basis of the quotient space of $V$ by the kernel of $T$ and the map
			\[ \phi: \operatorname{span} B_{K'} \longmapsto \operatorname{span} B_R = R(T) \; \suchthat \phi(\v) = T\v \]
			is a bijection because:
			\begin{itemize}
				\item{For any ${ \v \in R(T) }$, there exists some ${ \beta_1, \dots, \beta_{n-k} }$ such that,
					\[ \v = \beta_1 T(\b_1) + \cdots + \beta_{n-k} T(\b_{n-k})) = T(\beta_1 \b_1 + \cdots + \beta_{n-k} \b_{n-k}) \]
					and, clearly, ${ \beta_1 \b_1 + \cdots + \beta_{n-k} \b_{n-k} \in \operatorname{span} B_{K'} }$. Therefore $\phi$ is surjective.
				}
				\item{For any ${ \v_1, \v_2 \in V }$ such that ${ \phi(\v_1) = \phi(\v_2) }$ we have,
					\[\begin{aligned}
						&& T\v_1 &= T\v_2 \\
						&\iff & T(\alpha_1 \b_1 + \cdots + \alpha_{n-k} \b_{n-k}) &= T(\beta_1 \b_1 + \cdots + \beta_{n-k} \b_{n-k}) &\sidecomment{} \\
						&\iff & \alpha_1 T(\b_1) + \cdots + \alpha_{n-k} T(\b_{n-k}) &= \beta_1 T(\b_1) + \cdots + \beta_{n-k} T(\b_{n-k})
					\end{aligned}\]
					which, by the linear independence of $B_R$ implies that ${ \alpha_i = \beta_i, \; 1 \leq i \leq n-k }$. This in turn implies that,
					\[\begin{aligned}
						&& \alpha_1 \b_1 + \cdots + \alpha_{n-k} \b_{n-k} &= \beta_1 \b_1 + \cdots + \beta_{n-k} \b_{n-k} \\
						&\iff & \v_1 &= \v_2. &\sidecomment{} \\
					\end{aligned}\]
					Therefore $\phi$ is injective. \qedhere
				}
			\end{itemize}
		\end{proof}
		\note{Note that this is the linear algebra version of \autoref{theo:first_isomorphism_theorem} and also a combination of \autoref{prop:lin_transform_fibres_are_additive_cosets_of_kernel} and the definition of the vector quotient space \ref{def:vector-quotient-space}.}

	
		\biggerskip
		\subsubsection{The Algebra of Linear Transformations}
		\bigskip
		\notation{If $T_1$ and $T_2$ are being used to denote linear maps then ${ T_1 T_2 }$ will denote their function composition and any other common operations performed on them (e.g. addition, subtraction, scalar multiplication or division, etc.) will denote a pointwise function definition. That's to say, for example,
			\[ (T_1 + T_2)\v = T_1\v + T_2\v. \]
		}
	
		\bigskip
		\labeledProposition{Any linear combination of linear maps is a linear map.}{linear-combination-of-linear-maps-is-linear-map}
		\begin{proof}
			Let ${ M = \sum_i \alpha_i T_i }$ be a map formed as an arbitrary linear combination of linear maps $T_i$. Then, following the pointwise definition,
			\[\begin{aligned}
				M(\beta_1\v_1 + \beta_2\v_2) &= \sum_i \alpha_i T_i (\beta_1\v_1 + \beta_2\v2) &\sidecomment{pointwise defn.}\nn
				&= \sum_i \alpha_i T_i \beta_1\v_1 + \alpha_i T_i \beta_2\v2 &\sidecomment{linearity of $T_i$}\nn
				&= \sum_i \alpha_i T_i \beta_1\v_1 + \sum_i \alpha_i T_i \beta_2\v2 &\sidecomment{'+' associative, commutative}\nn
				&= \beta_1 \sum_i \alpha_i T_i \v_1 + \beta_2 \sum_i \alpha_i T_i \v2 &\sidecomment{linearity of $T_i$}\nn
				&= \beta_1 M(\v_1) + \beta_2 M(\v2) &\sidecomment{pointwise defn.}.\nn
			\end{aligned}\]
			In fact, we can extend this to any arbitrary linear combination of vectors,
			\[\begin{aligned}
				M\left( \sum_j \beta_j \v_j \right) &= \sum_i \alpha_i T_i \left( \sum_j \beta_j \v_j \right) &\sidecomment{pointwise defn.}\nn
				&= \sum_i \sum_j \alpha_i T_i \beta_j \v_j &\sidecomment{linearity of $T_i$}\nn
				&= \sum_j \sum_i \alpha_i T_i \beta_j \v_j &\sidecomment{'+' associative, commutative}\nn
				&= \sum_j \beta_j \sum_i \alpha_i T_i \v_j &\sidecomment{linearity of $T_i$}\nn
				&= \sum_j \beta_j M(\v_j) &\sidecomment{pointwise defn.}.\nn
			\end{aligned}\]
		\end{proof}
	
	}




% ----------------------



	\pagebreak
	\searchableSubsection{Linear Transformations as Matrices}{linear algebra}{
		\bigskip
		\labeledProposition{Left multiplication by a ${ m \times n }$ matrix is a linear transformation ${ \F{n} \longmapsto \F{m} }$.}{left_multiplication_by_matrix_are_lin_transforms}
		\begin{proof}
			Let ${ T: \F{n} \longmapsto \F{m} }$ be a linear transformation. Then $T$ is a map from $n$-vectors to $m$-vectors that is compatible with the vector space operations. Let $A$ be an ${ m \times n }$ matrix then ${  A\V{x} = \V{b} }$ where ${ \V{x} \in \F{n} }$ and ${ \V{b} \in \F{m} }$ showing that ${ T(\V{x}) = A\V{x} = \V{b} }$ is a map of the form ${ \F{n} \longmapsto \F{m} }$. Furthermore,
			\[ T(\V{x_1} + \V{x_2}) = A(\V{x_1} + \V{x_2}) = A\V{x_1} + A\V{x_2} = T(\V{x_1}) + T(\V{x_2})\]
			and
			\[ T(c\V{x}) = A(c\V{x}) = cA\V{x} = cT(\V{x}) \]
			which shows that left multiplication preserves vector addition and scalar multiplication so that ${ T(\V{x}) = A\V{x} = \V{b} }$ is a linear map as required.
		\end{proof}
	
		\medskip
		\labeledProposition{Every linear transformation ${ \F{n} \longmapsto \F{m} }$ is left multiplication by a particular ${ m \times n }$ matrix.}{every-lin-transform-is-left-multiplication-by-a-particular-matrix}
		\begin{proof}
			For any ${ \V{x} = \langle x_1,\dots,x_n \rangle \in \F{n} }$ we can write it as,
			\[ x_1\e{1} + \cdots + x_n\e{n}. \]
			Therefore if ${ T: \F{n} \longmapsto \F{m} }$ then,
			\[ T(\x) = T(x_1\e{1} + \cdots + x_n\e{n}) = T(\e{1})x_1 + \cdots + T(\e{n})x_n \in \F{m} \]
			and so letting ${ A \in \F{m \times n} }$ be,
			\[ 
				A =
				\begin{bmatrix}
				T(\e{1}) & \cdots & T(\e{n})
				\end{bmatrix}
			\]
			we have,
			\[ T(\V{x}) = A\V{x}. \qedhere \]
		\end{proof}
		\begin{corollary}
			Any linear transformation between spaces isomorphic to $\F{n}$ and $\F{m}$ (refer to \autoref{prop:vector_space_isomorphic_to_coordinate_space_of_same_dimension} and \ref{ex:isomorphism_with_Fn}) is left multiplication by a particular ${ m \times n }$ matrix.
		\end{corollary}
	
		\medskip\note{This is why linear transformations from a space to itself can be wholly characterized by what they do to the axes and also why every such linear transformation can be considered a change of basis and vice-versa.\\
			Conceptually, a linear transformation changes the coordinates of a selection of transformed vectors. The confusion comes about because we implement the matrix of the linear transformation $A$ by transforming the basis against which the coordinates are applied,
			\[ 
				A =
				\begin{bmatrix}
				T(\e{1}) & \cdots & T(\e{n})
				\end{bmatrix}.
			\]
			Whereas a change of basis transforms the basis against which coordinates are applied \textbf{and then updates the coordinates to balance out the change}.\\
			This can be seen if we deconstruct the change of basis formula:
			\[ B\x_B = B'\x_{B'} \iff \x_{B'} = \inv{(B')}B\x_B \]
			\[ \x_{B'} = P\x_B = \inv{(B')}B\x_B \]
			This can be thought of as first obtaining the coordinates against the standard basis ${ B\x_B }$ and then applying the inverse of the target basis so as to obtain the equivalent coordinates against the target basis. But, note, we could also consider the whole thing as a linear transformation represented by $P$.\\\\		
			The biggest difference, however, is that a linear transformation can also be between different spaces --- say from $n$-dimensional space to $m$-dimensional space --- in which case it cannot be thought of as a change of basis as a vector ${ \v \in \F{n} }$ cannot be equivalently expressed using a basis of $\F{m}$ because the two spaces are not isomorphic.
		}
		
		\bigskip
		\subsubsection{Examples of Linear Transformations as Matrices}
		\begin{exe}
			\ex{Let ${ T: \R{2} \longmapsto \R{2} }$ be a linear transformation such that,
				\[
				T(\e{1}) = \begin{bmatrix}
				1 \\
				2
				\end{bmatrix} \eqand
				T(\e{2}) = \begin{bmatrix}
				-1 \\
				0
				\end{bmatrix}.
				\]
				The transformation $T$ has been completely described in this way because, for any ${ \x = \langle x_1, x_2 \rangle \in \R{2} }$ we have,
				\begin{align*}
				&& T(\x) &= T(x_1\e{1} + x_2\e{2}) \\
				&\iff & T(\x) &= x_1T(\e{1}) + x_2T(\e{2}) \\ 
				&\iff & T(\x) &= x_1\begin{bmatrix}
				1 \\
				2
				\end{bmatrix} +
				x_2\begin{bmatrix}
				-1 \\
				0
				\end{bmatrix}
				=
				\begin{bmatrix}
				x_1 - x_2 \\
				2x_1
				\end{bmatrix}. &
				\end{align*}
				So, $T$ is also left multiplication by the matrix,
				\[ A = \begin{bmatrix}
				1 & -1 \\
				2 & 0
				\end{bmatrix}. \]
				\bigskip
			}
			\ex{Consider a linear map ${ T: \R{n} \longmapsto \R{m} }$ and the matrix representing it ${ A \in \R{mn} }$. Suppose the equation ${ A\x = \b }$ has the known solution
				\[ \x = \begin{bmatrix}1\\2\\0\\-1\\0\end{bmatrix} +
						s \begin{bmatrix}2\\1\\1\\0\\0\end{bmatrix} +
						t \begin{bmatrix}1\\1\\0\\-1\\1\end{bmatrix}
						\hspace{30pt} s,t \in \R{}.
				\]
				What can be said about the linear transformation $T$ from looking at the solution $\x$?
				\begin{itemize}
					\item{The dimension of the domain, ${ n = 5 }$, is clear since the solution $\x$ is a vector in the domain space.}
					\item{The nullity - dimension of the kernel - is 2, since there are two free variables ${ s,t }$. The basis of the 2-dimensional nullspace is the two vectors being multiplied by these variables.}
					\item{Using the dimension formula (\autoref{theo:linear_map_dimension_formula}) we can deduce that the rank of $T$ is $n$ - nullity. So the rank is ${ 5 - 2 = 3 }$. This is also supported by the fact that the particular solution has 3 non-zero components.}
				\end{itemize}
				What can \textbf{not} be said?
				\begin{itemize}
					\item{The dimension of the codomain $m$ cannot be derived from looking at the solution $\x$. The dimension of the image of $T$ is given by its range because the kernel maps to the origin in the codomain and so the image of the kernel has dimension zero. Since we are not told whether or not the linear map is surjective we cannot know the dimension of the codomain space - only the image of $T$ in the codomain space.}
				\end{itemize}
				\bigskip
			}
			\ex{The minimal linear transformation is multiplication by a minimal matrix --- a one-by-one matrix, i.e. a scalar. So
				\[ T(\v) = A\v = a\v. \]
				Since the real number line, for example, may be considered a one-dimensional vector space, multiplication of two real numbers, for example, may be considered as a linear transformation.
				\bigskip
			}\label{ex:minimal-linear-transformation}
		\end{exe}
	
		\bigskip\bigskip
		\subsubsection{Linear Transformations and Change of Basis}\label{sssection:linear-trans-and-change-of-basis}
		\medskip
		It is often possible to achieve powerful simplifications of problems by selecting appropriate bases. In this section we will look at linear transformations represented by matrices between arbitrary bases of spaces. So here we are looking at the relationship between linear transformations and change of basis.\\

		\medskip\label{def:matrix_of_lin_transform_wrt_bases}
		\boxeddefinition{If the matrix $A$ of a linear transformation ${ T: V \longmapsto W }$ is defined as, for ${ \x \in V }$,
			\[ T(\x) = A\x = \b \in W \]
			then \textbf{the matrix of $T$ with respect to the bases ${ B \subset V }$ and ${ B' \subset W }$} is defined as the matrix $A$ that satisfies,
			\[ A\x_{B} = \b_{B'} \in W \]
			and also
			\[  T(\x) = [B']A\x_{B} = \b \]
		}
	
		\medskip
		\subsubsection{Intuition of the Matrix of $T$ with respect to Bases}
		\medskip
		Let ${ T: V \longmapsto W }$ be a linear transformation and let ${ B_V = \{\V{v_1},\dots,\V{v_n}\} }$ be a basis of $V$ and ${ B_W = \{\V{w_1},\dots,\V{w_m}\} }$ be a basis of $W$. Then ${ T(\V{v_i}) \in W }$ and so there is some ${ m \times n }$ matrix ${ A = (a_{ij}) }$ such that,
		\[
			\begin{bmatrix}
			\V{w_1} & \cdots & \V{w_m}
			\end{bmatrix}
			A
			=
			\begin{bmatrix}
			T(\V{v_1}) & \cdots & T(\V{v_n})
			\end{bmatrix}.		
		\]
			where,
		\[ T(\V{v_j}) = 
			\begin{bmatrix}
				\V{w_1} & \cdots & \V{w_m}
			\end{bmatrix}
			\begin{bmatrix}
				a_{1j}\\
				\vdots \\
				a_{mj}
			\end{bmatrix}
					= a_{1j}\V{w_1} + \cdots + a_{mj}\V{w_m} = \sum_i a_{ij}\V{w_i} 
		\]
		so that the $j$th column of the matrix $A$ is the coordinate vector of $T(\V{v_j})$ with respect to the basis $B_W$.\\\\		
		Substituting ${ B_W = \{\V{w_1},\dots,\V{w_m}\} }$ we can obtain an expression for $A$,
		\begin{align*}
		&& \begin{bmatrix}
			\V{w_1} & \cdots & \V{w_m}
			\end{bmatrix}
			A &=
			\begin{bmatrix}
			T(\V{v_1}) & \cdots & T(\V{v_n})
			\end{bmatrix} \\
		&\iff & [B_W]A &= \begin{bmatrix}
					T(\V{v_1}) & \cdots & T(\V{v_n})
					\end{bmatrix} \\
		&\iff & A &= \inv{[B_W]}\begin{bmatrix}
					T(\V{v_1}) & \cdots & T(\V{v_n})
					\end{bmatrix}. &\sidecomment{} \\
		\end{align*}		
		The matrix $A$ is referred to as the \textbf{matrix of $T$ with respect to the bases $B_V$ and $B_W$} and conforms to,
		\[ A\x_{B_V} = \b_{B_W}. \]
		This can be seen as,
		\begin{align*}
		&& A\x_{B_V} &= \inv{[B_W]}\begin{bmatrix}
						T(\V{v_1}) & \cdots & T(\V{v_n})
						\end{bmatrix}\x_{B_V} \\
		&\iff & A\x_{B_V} &= \inv{[B_W]}\b_{B_V} &\sidecomment{} \\
		&\iff & A\x_{B_V} &= \b_{B_W} &\sidecomment{} \\
		\end{align*}
		so that ${ \x_{B_V} }$ --- the coordinate vector with respect to $B_V$ --- is first transformed by applying the coordinates to the transformed version of the basis $B_V$ and then these coordinates are converted to $B_W$ coordinates by left multiplication by $\inv{[B_W]}$.\\\\		
		\note{If we chose different bases for the spaces we would get a different matrix. If the bases are the standard bases then the matrix is the standard matrix for the transformation.}
		
		\medskip
		\subsubsection{Examples of Linear Transform Matrices \wrt Bases}
		\begin{exe}
			\ex{Let ${ T: \R{2} \longmapsto \R{2} }$ be a linear transform defined (against the standard basis) by,
				\[ T\left(\begin{bmatrix}1 \\ 0\end{bmatrix}\right) = \begin{bmatrix}2 \\ 3\end{bmatrix} \eqand
					T\left(\begin{bmatrix}0 \\ 1\end{bmatrix}\right) = \begin{bmatrix}3 \\ 2\end{bmatrix}.
				\]
				Then, if we define the matrix of $T$ with respect to the standard basis only then we have,
				\[ A = \begin{bmatrix}
						T\left(\begin{bmatrix}1 \\ 0\end{bmatrix}\right) & T\left(\begin{bmatrix}0 \\ 1\end{bmatrix}\right)
						\end{bmatrix} =
						\begin{bmatrix}
						2 & 3\\
						3 & 2
						\end{bmatrix}
				\]
				and, if we define a vector ${ \x = \langle 1,1 \rangle }$ against the standard basis we can see that,
				\[ T(\x) = 1 \cdot T\left(\begin{bmatrix}1 \\ 0\end{bmatrix}\right) + 1 \cdot T\left(\begin{bmatrix}0 \\ 1\end{bmatrix}\right) =
				   A\x =
				   \begin{bmatrix}
				   2 & 3\\
				   3 & 2
				   \end{bmatrix}
				   \begin{bmatrix}
				   1\\
				   1
				   \end{bmatrix} =
				   \begin{bmatrix}
				   5\\
				   5
				   \end{bmatrix}.
				\]
				If we now define $B$, an alternative basis of $\R{2}$, as
				\[ B = \left\{
							\begin{bmatrix}3\\0\end{bmatrix},
							\begin{bmatrix}0\\2\end{bmatrix}
					   \right\}
				\]
				then we have,
				\[
					[B] = 	\begin{bmatrix}
							3 & 0\\
							0 & 2
							\end{bmatrix}
					\eqand
					\inv{[B]} = \frac{1}{6}
								\begin{bmatrix}
								2 & 0\\
								0 & 3
								\end{bmatrix}
				 \]
				and the linear transform matrix \textbf{of coordinate vectors \wrt the basis $B$} is defined as,
				\[ A = \begin{bmatrix}
						T\left(\begin{bmatrix}3 \\ 0\end{bmatrix}\right) & T\left(\begin{bmatrix}0 \\ 2\end{bmatrix}\right)
						\end{bmatrix} =
						\begin{bmatrix}
						6 & 6\\
						9 & 4
						\end{bmatrix}.
				\]
				Now \textit{this} matrix $A$ expects coordinate vectors \wrt $B$ and so if we convert $\x$ to basis $B$ as follows,
				\[ \x_B = \inv{[B]}\x = \frac{1}{6}\begin{bmatrix}
													2 & 0\\
													0 & 3
													\end{bmatrix} 
													\begin{bmatrix}
													1\\
													1
													\end{bmatrix} =
													\begin{bmatrix}
													1/3\\
													1/2
													\end{bmatrix}									
				\]
				then we find that,
				\[ T(\x) = A\x_B = \begin{bmatrix}
									6 & 6\\
									9 & 4
									\end{bmatrix}
									\begin{bmatrix}
									1/3\\
									1/2
									\end{bmatrix} =
									\begin{bmatrix}
									6/3 + 6/2\\
									9/3 + 4/2
									\end{bmatrix} =
									\begin{bmatrix}
									5\\
									5
									\end{bmatrix}.
				\]
				\\If we were to define another basis of $\R{2}$ called $B'$ and construct the matrix of $T$ with respect to $B$ and $B'$ then the matrix $A$ would become,
				\[ A = \inv{[B']}\begin{bmatrix}
						T\left(\begin{bmatrix}3 \\ 0\end{bmatrix}\right) & T\left(\begin{bmatrix}0 \\ 2\end{bmatrix}\right)
						\end{bmatrix}
				\]
				and to get the result in standard coordinates we would need to apply the result to the basis vectors of $B'$,
				\[ T(\x) = [B']A\x_B. \]
				
				In the case where we want the result to be in the same basis as the argument vector $\x_B$ we still need to modify the matrix $A$. In this case $A$ becomes,
				\[ A = \inv{[B]}\begin{bmatrix}
						T\left(\begin{bmatrix}3 \\ 0\end{bmatrix}\right) & T\left(\begin{bmatrix}0 \\ 2\end{bmatrix}\right)
						\end{bmatrix}
				\]
				This $A$ still expects $\x$ to be in $B$ coordinates but outputs a result defined in $B$ coordinates rather than standard coordinates.
				\[ A\x_B = \frac{1}{6}\begin{bmatrix}
							2 & 0\\
							0 & 3
							\end{bmatrix}
							\begin{bmatrix}
							6 & 6\\
							9 & 4
							\end{bmatrix}
							\x_B =
							\begin{bmatrix}
							2 & 2\\
							9/2 & 2
							\end{bmatrix}
							\begin{bmatrix}
							1/3\\
							1/2
							\end{bmatrix} =
							\begin{bmatrix}
							5/3\\
							5/2
							\end{bmatrix}.
				\]
				
				So, to get the result in standard coordinates we need to apply the result to the basis vectors of $B'$,
				\[ T(\x) = [B](A\x_B) =
							\begin{bmatrix}
							3 & 0\\
							0 & 2
							\end{bmatrix}
				 			\begin{bmatrix}
				 			5/3\\
				 			5/2
				 			\end{bmatrix} =
				 			\begin{bmatrix}
				 			5\\
				 			5
				 			\end{bmatrix}.
				\]
			}
		\end{exe}
	
		\bigskip
		\labeledProposition{Let $A$ be the matrix of a linear transformation ${ T: V \longmapsto W }$ with respect to the bases ${ B_V, B_W }$ of dimension $n$ and $m$ respectively. The matrices $A'$ which represent $T$ with respect to other bases are those of the form,
			\[ A' = QA\inv{P} \]
			where ${ Q \in GL_m(\F{}), P \in GL_n(\F{}) }$.
		}{matrix_wrt_to_2_bases_wrt_to_other_bases}
		\begin{proof}
			Let ${ B_V' = \{\v_1',\dots,\v_n'\}, B_W' = \{\w_1',\dots,\w_n'\} }$ be alternative bases with respect to which we want to find $A'$, the matrix of $T$. Also let,
			\[ B_V = B_V'P \eqand B_W = B_W'Q. \]
			Then for ${ \v_i' \in B_V',\, T(\v') \in span\,B_W' }$ so there exists a matrix $A'$ such that, using the notation $T(B_V)$ to indicate the image of the set $B_V$ under $T$ and $[B_V]$ for the matrix whose columns are the elements of $B_V$,
			\begin{align*}
			&& \begin{bmatrix}
				\w_1' & \cdots & \w_m'
				\end{bmatrix}
				A'&=
				\begin{bmatrix}
				T(\v_1') & \cdots & T(\v_n')
				\end{bmatrix}  \\
			&\iff & [B_W']A' &= [T(B_V')] &\sidecomment{} \\
			&\iff & A' &= \inv{[B_W']}[T(B_V')] &\sidecomment{} \\
			&\iff & A' &= \inv{[B_W']}[T(B_V\inv{P})] &\sidecomment{} \\
			&\iff & A' &= Q\inv{[B_W]}[T(B_V)]\inv{P} &\sidecomment{P is coefficient matrix} \\
			&\iff & A' &= QA\inv{P}. &\qedhere \\
			\end{align*}
		\end{proof}
	
		\bigskip
		\subsubsection{Simplification of the Matrix of a Transformation}
		\medskip
		\labeledProposition{Let ${ T: V \longmapsto W }$ be a linear transformation of rank $r$. Bases ${ B_V,B_W }$ may be chosen so that the matrix of $T$ takes the form,
			\[ A = \begin{bmatrix}
					I_r & \vdots \\
					\cdots & 0 \\
					\end{bmatrix}.
			\]
		}{simplification_of_linear_transform_matrix}
		\begin{proof}
			Let ${ U = \{\u_1,\dots,\u_k\} }$ be a basis of the kernel of $T$ where ${ k = dim(ker\,T) }$. Then $U$ may be extended to a basis of $V$ (\autoref{prop:lin_ind_set_can_be_extended_to_basis}),
			\[ B_V = \{\v_1,\dots,\v_r,\u_1,\dots,\u_k\}. \]
			Then, let ${ T(\v_i) = \w_i }$ so that,
			\[ [T(B_V)] = \begin{bmatrix}
						\w_1 & \cdots & \w_r & \0 & \cdots & \0
						\end{bmatrix}. 
			\]
			As shown in \autoref{theo:linear_map_dimension_formula}, ${ \{ \w_1,\dots,\w_r \} }$ is a basis of the image of $T$ and can also be extended to a basis of $W$,
			\[ B_W = \{ \w_1,\dots,\w_r, \x_1,\dots, \x_{m-r} \}. \]
			So, the matrix $A$ of $T$ with respect to the bases ${ B_V,B_W }$ satisfies,
			\begin{align*}
			&& A &= \inv{[B_W]}[T(B_V)] \\
			&\iff & [B_W]A &= [T(B_V)] \\
			&\iff & \begin{bmatrix}\w_1 & \dots & \w_r & \x_1 & \dots & \x_{m-r}\end{bmatrix}A &= 
					\begin{bmatrix}\w_1 & \dots & \w_r & \0 & \dots & \0\end{bmatrix}. &\sidecomment{}
			\end{align*}
			If we look at the components of the matrices we see,
			\[  
				\begin{bmatrix}
				w_{11} \dots & w_{1r} & x_{11} \dots & x_{1(m-r)} \\
				\vdots &  &  &  \\
				w_{r1} \dots & w_{rr} & x_{r1} \dots & x_{r(m-r)} \\
				\vdots &  &  &  \\
				w_{m1} \dots & w_{mr} & x_{m1} \dots & x_{m(m-r)} \\
				\end{bmatrix}
				\begin{bmatrix}
				a_{11} & \dots & a_{1n} \\
				\vdots &  &  \\
				a_{r1} & \dots & a_{rn} \\
				\vdots &  &  \\
				a_{m1} & \dots & a_{mn} \\
				\end{bmatrix}
				= 
				\begin{bmatrix}
				w_{11} \dots & w_{1r} & 0 \dots & 0 \\
				\vdots &  &  &  \\
				w_{r1} \dots & w_{rr} & 0 \dots & 0 \\
				\vdots &  &  &  \\
				w_{m1} \dots & w_{mr} & 0 \dots & 0 \\
				\end{bmatrix}
			\]
			which makes it clear that $A$ has the form,
			\[ A =
				\begin{bmatrix}
				1 & \dots & 0 & 0 & \dots & 0 \\
				\vdots & \ddots & \vdots & &  \\
				0 & \dots & 1 & 0 & \dots & 0 \\
				\vdots &  & & &  \\
				0 & \dots & 0 & 0 & \dots & 0 \\
				\end{bmatrix}
			\]
			as required.
		\end{proof}
		\begin{corollary}
			By \autoref{prop:left_multiplication_by_matrix_are_lin_transforms}, left multiplication by any matrix is a linear transformation and so is equivalent to left multiplication by a matrix of the form
			\[
			\begin{bmatrix}
				I_r & \vdots \\
				\cdots & 0 \\
				\end{bmatrix}
			\]
			but with reference to different coordinate systems.
		\end{corollary}
	
		\bigskip
		\subsubsection{Example of Simplification by Selecting Bases}
		\begin{exe}
			\ex{Continuing the example \ref{ex:gaussian-elimination} of Gaussian Elimination for row reducing a matrix we have a matrix
			\[ A = \begin{bmatrix}
						3 & 1 & 1  \\
						0 & 2 & -4 \\
						3 & 2 & -1
					\end{bmatrix}
			\]
			whose rref form is
			\[ A' = \begin{bmatrix}
						1 & 0 & 1  \\
						0 & 1 & -2 \\
						0 & 0 & 0
					\end{bmatrix}.
			\]
			The rref tells us that the first two columns of $A$ are a basis of the image and the kernel is 
			\[ 	c \begin{bmatrix}-1\\2\\1\end{bmatrix} \hspace{20pt} c \in \R{}. \]	
			
			We can use this information to form a matrix ${ A_{PQ} }$ --- which is the matrix $A$ expressed with respect to the bases $P$ and $Q$ --- such that $A_{PQ}$ is maximally simplified. Following the procedure used in the proof of \autoref{prop:simplification_of_linear_transform_matrix}, we begin by finding a basis of the domain space by extending a basis of the kernel.\\
			
			Since the kernel is one-dimensional and the domain is three-dimensional, we can extend the basis of the kernel given above to a basis of the domain by adding two of the three standard basis vectors. So, the chosen basis of the domain is
			\[ P = \left\{ \begin{bmatrix}1\\0\\0\end{bmatrix}, \begin{bmatrix}0\\1\\0\end{bmatrix}, \begin{bmatrix}-1\\2\\1\end{bmatrix} \right\}. \]
			
			Again following the procedure from the same proof, we next form a basis of the codomain space that extends a basis of the image which, in this case, is the first two columns of the matrix $A$. So, we can choose the basis,
			\[ Q = \left\{ \begin{bmatrix}3\\0\\3\end{bmatrix}, \begin{bmatrix}1\\2\\2\end{bmatrix}, \begin{bmatrix}0\\0\\1\end{bmatrix} \right\}. \]
			\note{We could have chosen anything for the final column just so long as it is linearly independent of the first two columns.}
			
			Then,
			\begin{align*}
			A_{PQ} &= \inv{Q} A P \\
			&= 	\inv{\begin{bmatrix}
					3 & 1 & 0\\
					0 & 2 & 0\\
					3 & 2 & 1
				\end{bmatrix}}
				\begin{bmatrix}
					3 & 1 & 1  \\
					0 & 2 & -4 \\
					3 & 2 & -1
				\end{bmatrix}
				\begin{bmatrix}
					1 & 0 & -1  \\
					0 & 1 &  2 \\
					0 & 0 &  1
				\end{bmatrix} &\sidecomment{} \\[8pt]
			&= 	\frac{1}{6}
				\begin{bmatrix}
					2  & -1 & 0\\
					0  &  3 & 0\\
					-6 & -3 & 6
				\end{bmatrix}
				\begin{bmatrix}
					3 & 1 & 1  \\
					0 & 2 & -4 \\
					3 & 2 & -1
				\end{bmatrix}
				\begin{bmatrix}
					1 & 0 & -1  \\
					0 & 1 &  2 \\
					0 & 0 &  1
				\end{bmatrix} &\sidecomment{} \\[8pt]
			&= 	\frac{1}{6}
				\begin{bmatrix}
				2  & -1 & 0\\
				0  &  3 & 0\\
				-6 & -3 & 6
				\end{bmatrix}
				\begin{bmatrix}
				3 & 1 & 0 \\
				0 & 2 & 0 \\
				3 & 2 & 0
				\end{bmatrix} &\sidecomment{} \\[8pt]
			&= 	\begin{bmatrix}
					1 & 0 & 0\\
					0 & 1 & 0\\
					0 & 0 & 0
				\end{bmatrix}.
			\end{align*}
			}\label{ex:matrix-simplification-by-selecting-bases}
		\end{exe}
		
	}



% ---------------------


	\pagebreak
	\searchableSubsection{Linear Operators and Eigenvectors}{linear algebra}{
		
		\bigskip
		\boxeddefinition{A \textbf{linear operator} is a linear transformation from a space to itself. That's to say, the domain and codomain of the transformation are the same space and considered with respect to the same basis.}
		
		\medskip
		\boxeddefinition{A linear operator is called \textbf{singular} if it does not have an inverse and \textbf{nonsingular} if it has an inverse.}
		
		\bigskip
		\boxeddefinition{If $T$ is a linear operator on a vector space (finite or infinite) over a field $\F{}$ and $p(x)$ is a polynomial over $\F{}$, 
			\[ p(x) = a_0 + a_1 x + \cdots + a_n x^n \]
		then we can define the \textbf{polynomial of the linear operator} $T$ as
			\[ p(T) = a_0 I + a_1 T + \cdots + a_n T^n. \]
		Since any power of the operator $T^n$ is also a linear operator and the polynomial is a linear combination of these, by \autoref{prop:linear-combination-of-linear-maps-is-linear-map}, \textbf{the polynomial $p(T)$ is also a linear operator}.\\
		
		Similarly for a square matrix $A$,
			\[ p(A) = a_0 I + a_1 A + \cdots + a_n A^n. \]
		}\label{defn:polynomials-of-linear-operators}
		
		\medskip
		\note{To show the validity of polynomials of linear operators:\\\\ Let $T$ be an arbitrary linear map over a vector space $V$ defined over a field $\F{}$. Define ${ T + a }$ as the linear map ${ T + aI }$ and similarly ${ T - b = T - bI }$. Lastly, define ${ (T + a)(T - b) }$ as the composition of the linear maps,
			\[ (T + aI) \circ (T - bI). \]
		Then,
		\[\begin{aligned}
			(T + a)(T - b) &= T(T - b) + a(T - b) &\sidecomment{pointwise definition} \\
			&= TT - Tb + aT - ab &\sidecomment{linearity} \\
			&= T^2 + (a-b)T - ab. \\
		\end{aligned}\]
		In fact, polynomials over a field such as the reals can be viewed as linear combinations of a linear map -- the simplest linear map: scalar multiplication.\\\\ If we regard the real variable $x$ as the function "multiply by $x$ for any $x$ in the field" ${ f_1(t) = xt }$, then ${ x + a }$ is the linear combination of this function with the function "multiply by the constant $a$" (defined pointwise) which results in: ${ f_2(t) = (x + a)t = xt + at }$. Then 
		\[\begin{aligned}
			(x + a)^2 &= (x + a)(x + a) \\
			&= (x + a)f_2(t) \\
			&= (x + a)(xt + at) \\
			&= x^2 t + axt + axt + a^2 t \\
			&= x^2 t + 2axt + a^2 t \\
			&= (x^2 + 2ax + a^2)t.
		\end{aligned}\]
		A full treatment of this topic requires Modules (\href{https://en.wikipedia.org/wiki/Module_(mathematics)}{wikipedia}).
		}
		
		\bigskip
		\labeledProposition{A linear operator ${ T: V \longmapsto V }$ is bijective iff it has a trivial kernel.}{injective_lin_operator_is_bijective}
		\begin{proof}
			By \autoref{theo:homomorphism_injective_iff_kernel_is_trivial} of homomorphisms, if $T$ has a trivial kernel then it is injective which implies that ${ \cardinality{im\,T} = \cardinality{V} }$ so that it is also surjective (because the domain is equal to the codomain). Therefore, it is bijective.\\
			Conversely, if it is bijective then it is injective and so it has a trivial kernel.
		\end{proof}
		\begin{corollary}
			A linear operator ${ T: V \longmapsto V }$ is isomorphic iff it has a trivial kernel.
		\end{corollary}
		\begin{corollary}
			A linear operator is nonsingular iff it has a trivial kernel.
		\end{corollary}
	
		\medskip\note{Note that these are not true of linear transformations in general because, for a transformation between different vector spaces of different dimensions, injectivity does not imply bijectivity and invertibility.
		}
	
		\medskip
		\labeledProposition{The following conditions on a linear operator ${ T: V \longmapsto V }$ on a finite-dimensional vector space are equivalent:
			\begin{enumerate}[label=(\roman*)]
				\item{${ ker\,T > 0 }$.}
				\item{${ im\,T < V }$.}
				\item{If $A$ is the matrix of the operator with respect to an arbitrary basis, then ${ det\,A = 0 }$.}
				\item{$T$ is singular}
			\end{enumerate}
		}{properties_of_non_bijective_lin_operator}
		\begin{proof}
			The first two of these properties follow from the dimension formula for finite-dimensional vector spaces. The third follows since an operator with a non-trivial kernel is noninvertible (\autoref{prop:injective_lin_operator_is_bijective}) and so its matrix will also be noninvertible; noninvertible matrices have determinant 0. The last property follows from the definition of singular and the fact that $T$ is noninvertible.
		\end{proof}
	
		\medskip\note{Again it's worth noting the contrast with transformations in general: for a transformation whose domain vector space is lower dimension than its codomain, the transformation may be injective but not surjective (kernel is trivial but ${ im\,T < V }$), and conversely, if the domain is higher dimension than the codomain then the transformation may be surjective while not injective (${ ker\,T > 0 }$ but image covers codomain). Furthermore, the third condition relating to the determinant, only applies to operators because the determinant is a property of square matrices.
		}
		
		\note{Note that the first two properties do not hold for infinite-dimensional vector spaces. For example, let ${ V = \R{\infty} }$ be the space of sequences ${ a_1,a_2,\dots }$. Then the "shift operator" is defined as,
			\[ T(a_1,a_2,\dots) = (0,a_1,a_2,\dots). \]
			This is a linear operator because,
			\begin{align*}
			\alpha T(a_1,a_2,\dots) + \beta T(b_1,b_2,\dots) &= \alpha (0,a_1,a_2,\dots) + \beta (0,b_1,b_2,\dots) \\
			&= (0, \alpha a_1, \alpha a_2,\dots) + (0, \beta b_1, \beta b_2,\dots) &\sidecomment{} \\
			&= (0, \alpha a_1 + \beta b_1, \alpha a_2 + \beta b_2,\dots) &\sidecomment{} \\
			&= T(\alpha a_1 + \beta b_1, \alpha a_2 + \beta b_2,\dots). &\sidecomment{} \\
			\end{align*}
			However, while clearly for this operator we have ${ im\,T < V }$, nevertheless the kernel of this operator is the trivial kernel ${ \{\0\} }$. This just shows further that the dimension formula (\autoref{theo:linear_map_dimension_formula}) for finite-dimensional vector spaces does not apply for infinite-dimensional spaces.\\
			The explanation of this is that infinite sets can have proper subsets that have equal cardinality. So we can have an image whose dimensions are a proper subset of the dimensions of the space but the image, nevertheless, has the same dimensionality as the space. This can only happen with infinite-dimensional spaces.
		}
	
		
		
		
		
		
		\bigskip
		\subsubsection{Invariant Subspaces}
		\medskip
		\boxeddefinition{Let ${ T: V \longmapsto V }$ be a linear operator on a vector space. A subspace $W$ of $V$ is called an \textbf{invariant subspace} or a \textbf{$T$-invariant subspace} if it is carried to itself by the operator,
			\[ TW \subseteq W. \]
			In other words, $W$ is $T$-invariant if ${ T(\w) \in W }$ for all ${ \w \in W }$. In this case, $T$ may be referred to as defining an operator on $W$ that is the \textbf{restriction of $T$ to $W$}.
		}
		\notation{Denote the restriction of $T$ to $W$ by $T_w$.}
		
		\bigskip
		\begin{tcolorbox}[breakable,enhanced jigsaw,colframe=white,colback=white,boxrule=0pt,arc=0pt,left=0pt,right=0pt,top=0pt,bottom=0pt]
			\labeledProposition{If $T$ is a linear operator over a vector space $V$ with range $R(T)$ and kernel/nullspace $N(T)$ then the following subspaces of $V$ are all $T$-invariant:
				\begin{enumerate}[label=(\roman*)]
					\item{$V$}
					\item{$ \{ \0 \} $}
					\item{$R(T)$}
					\item{$N(T)$}
				\end{enumerate}
			}{range-nullspace-origin-and-whole-space-are-all-T-invariant}
			\begin{proof}$ $
				\begin{enumerate}[label=(\roman*)]
					\item{$V$}\\
						Since $T$ is a linear operator, ${ T: V \longmapsto V }$, we have, for all ${ \v \in V }$, ${ T\v \in V }$.
					\item{$ \{ \0 \} $}\\
						Any linear transformation, by homogeneity (\autoref{coro:linear-maps-map-the-origin-to-itself}), maps the origin to itself.
					\item{$R(T)$}\\
						By the definition of the range of $T$ we must have for all ${ \v \in V }$, ${ T\v \in R(T) }$ and since $T$ is a linear operator, ${ T: V \longmapsto V }$, 
						\[ R(T) \subseteq V \implies \forall \V{r} \in R(T), \; T\V{r} \in R(T). \]
					\item{$N(T)$}\\
						By the definition of the nullspace/kernel of $T$ we have: for all ${ \v \in N(T) }$, ${ T\v = \0 }$. Since the origin is a member of all vector spaces, this implies that also for all ${ \v \in N(T) }$, ${ T\v \in N(T) }$.
				\end{enumerate}
			\end{proof}
		\end{tcolorbox}
	
		\bigskip		
		\labeledProposition{Let $T$ be an operator over a vector space $V$ and $W$ a subspace of $V$ such that ${ V = R(T) \oplus W }$ where $R(T)$ denotes the range of $T$. If $W$ is $T$-invariant then ${ W \subseteq N(T) }$ where $N(T)$ denotes the nullspace of $T$. Furthermore, if $V$ is finite-dimensional then ${ W = N(T) }$.
		}{invariant-subspace-linearly-independent-from-range-is-in-nullspace}
		\begin{proof}
			Let ${ U = R(T) }$ so that ${ V = U \oplus W }$ and for all ${ \v \in V }$, there exists ${ \u \in U, \w \in W }$ such that
			\[ \v = \u + \w \eqand \u + \w = \0 \implies \u = \w = \0. \]
			Since $W$ is $T$-invariant, for any ${ \w \in W }$
			\[ T\w \in W. \]
			But ${ U = R(T) }$ is the range of $T$, so it must also be that
			\[ T\w \in U. \]
			Therefore, there exists some ${ \w_1 \in W, \, \u_1 \in U }$ such that
			\[ T\w = \w_1 = \u_1. \]
			Since,
			\[ \w_1 = \u_1 \iff \u_1 - \w_1 = \0 \implies \w_1 = \u_1 = \0 \]
			we obtain the result that, for all ${ \w \in W }$,
			\[ T\w = \0. \]
			This implies that ${ W \subseteq N(T) }$.\\
			
			Furthermore, if $V$ is finite-dimensional, the dimension formula for linear operators over finite-dimensional vector spaces (\autoref{theo:linear_map_dimension_formula}) tells us that,
			\[ \dim R(T) + \dim N(T) = \dim V \iff \dim N(T) = \dim V - \dim R(T).  \]
			So we have,
			\[ \dim N(T) = \dim V - \dim U \]
			but also
			\[ V = U \oplus W \implies \dim V = \dim U + \dim W \iff \dim V - \dim U = \dim W \]
			which gives us
			\[ \dim W = \dim N(T). \]
			We can therefore deduce,
			\[ [\, W \subseteq N(T) \,] \land [\, \dim W = \dim N(T) \,] \implies W = N(T). \]
		\end{proof}
	
		\biggerskip
		\subsubsubsection{Examples of Invariant Subspaces}
		\begin{exe}
			\ex{Let ${ T: \R{3} \longmapsto \R{3} }$ be defined by,
				\[ T(a, b, c) = (a + b, \, b + c, \, 0). \]
				Then the $xy$-plane ${ \setc{(x,y,0)}{x,y \in \R{}} }$ and the $x$-axis ${ \setc{(x,0,0)}{x \in \R{}} }$ are $T$-invariant subspaces of $\R{3}$.
			}
		\end{exe}
		
		\biggerskip\biggerskip
		\subsubsubsection{Invariant Subspaces as Matrix Blocks}
		Let ${ W \subseteq V }$ be a $T$-invariant subspace of $V$ with basis ${ B_W = \w_1,\dots,\w_k }$. Then, by \autoref{prop:lin_ind_set_can_be_extended_to_basis}, $B_W$ can be extended to a basis of $V$,
		\[ B_V = \{ \w_1,\dots,\w_k, \v_1,\dots,\v_{n-k} \}. \]
		If we look at the matrix $A$ of $T$ with respect to this basis $B_V$ we find a characteristic pattern.
		\[ A = \inv{[B_V]}[T(B_V)] = \inv{[B_V]}[T(\w_1) \cdots T(\w_k) \; T(\v_1) \cdots T(\v_{n-k})] \]
		with ${ T(\w_1), \dots, T(\w_k) \in W }$ so that,
		\[ T(\w_i) = \alpha_1\w_1 + \cdots + \alpha_k\w_k. \] 
		This means that when we express $T(\w_i)$ with respect to the basis $B_V$ the coordinate vectors will take the form,
		\[ (\alpha_1,\dots,\alpha_k,0,\dots,0)^T. \]
		As a result, the matrix $A$ will take the form,
		\[ A = \begin{bmatrix}
				C & D\\
				0 & E
				\end{bmatrix}\label{eq:invariant_space_block_decomposition}
		\]
		where $C$ is a ${ k \times k }$ matrix that represents the restriction of $T$ to $W$.\\
		
		\note{A description of a matrix of the form of the description of $A$ here is known as a \textbf{block decomposition}.} 
		
		On the other hand, if ${ V = W_1 \oplus W_2 }$ where \textit{both} $W_1$ \textit{and} $W_2$ are $T$-invariant subspaces then $A$ takes the form,
		\[ A = \begin{bmatrix}
				C & 0\\
				0 & E
				\end{bmatrix} 
		\]
		where, as before, $C$ is a ${ k \times k }$ matrix that represents the restriction of $T$ to $W_1$ but, this time, also $E$ is a ${ (n-k) \times (n-k) }$ matrix that represents the restriction of $T$ to $W_2$.
		
		\smallskip\note{Matrices with the form of the matrix,
			\[ \begin{bmatrix}
				C & 0\\
				0 & E
				\end{bmatrix} \]
				are known as \textbf{block diagonal matrices} or \textbf{diagonal block matrices}.
		}
	
		\bigskip
		\subsubsubsection{Cyclic Subspaces}
		\boxeddefinition{Let $T$ be a linear operator on a vector space $V$ and let ${ \v \neq \0 \in V }$. Then the subspace,
			\[ W = \operatorname{span} \{ \v, \, T\v, \, T^2\,\v, \dots \} \]
			is called the \textbf{$T$-cyclic subspace of $V$ generated by $\v$}.\\
			The $T$-cyclic subspace of $V$ generated by any element of $V$, is $T$-invariant by construction.
			\note{refer: cyclic subgroups in Group Theory \ref{sssection:cyclic-subgroups}}
		}
	
		\bigskip
		\labeledProposition{The $T$-cyclic subspace of $V$ generated by ${ \v \in V }$ is the smallest $T$-invariant subspace of $V$ containing $\v$.}{cyclic-subspace-generated-by-v-is-smallest-invariant-space-containing-v}
		\begin{proof}
			Let $W$ be the $T$-cyclic subspace of $V$ generated by ${ \v \in V }$.\\
			Clearly, from the definition, $W$ contains $\v$ so $W$ is a subspace containing $\v$.\\
			Furthermore, if $W$ is to be $T$-invariant then $T\v$ must also be in $W$. But then for $W$ to be $T$-invariant we also need that 
			\[ T(T\v) = T^2\,\v \]
			is in $W$ too. In fact, we need the closure of the composition of $T$ applied to $\v$.\\
			Therefore, $W$ is the smallest $T$-cyclic subspace of $V$ containing $\v$.
		\end{proof}
	
		\paragraph{Examples}
		\begin{exe}
			\ex{Let ${ T: \R{3} \longmapsto \R{3} }$ be defined by,
				\[ T(a, b, c) = (-b + c, \, a + c, \, 3c). \]
				To obtain the $T$-cyclic subspace generated by ${ \e{1} = (1,0,0) }$ we compose $T$ repeatedly:
				\[\begin{aligned}
					T\e{1} &= (0,1,0) = \e{2} \\
					T^2\,\e{1} &= T(0,1,0) = (-1,0,0) = -\e{1} \\
					T^3\,\e{1} &= T(-1,0,0) = (0,-1,0) = -\e{2} \\
					T^4\,\e{1} &= T(0,-1,0) = (1,0,0) = \e{1}
				\end{aligned}\]
				which produces the cycle of elements:
				\[ \e{1} \to \e{2} \to -\e{1} \to -\e{2} \to \e{1}. \]
				Therefore the subspace generated is
				\[ \operatorname{span} \{ \e{1}, \e{2} \} = \setc{(x,y,0)}{x,y \in \R{}}. \]
			}\label{ex:simple-T-cyclic-subspace}
			\ex{Let $T$ be the linear operator on $P(\R{})$, the polynomials with real coefficients, defined by differentiation,
				\[ T(p(x)) = \Dif_x p. \]
				Then the $T$-cyclic subspace generated by $x^2$ is ${ \operatorname{span} \{ x^2, 2x, 2 \} }$.
			}
		\end{exe}
		
		\biggerskip
		\subsubsection{Eigenvectors}
		\boxeddefinition{An \textbf{eigenvector} of a linear operator $T$ is a nonzero vector $\v$ with the property under $T$ that,
			\[ T(\v) = c\v \]
			for some constant ${ c \in \F{} }$. The constant $c$ is called an \textbf{eigenvalue}.\\\\
			Eigenvectors may also be referred to as \textbf{characteristic vectors} and eigenvalues as \textbf{characteristic values}.\\\\
			The \textbf{eigenspace} of an eigenvalue is the subspace formed by the eigenvectors associated with the eigenvalue and the zero vector.
		}
		\medskip\note{Note:
			\begin{itemize}
				\item{To have eigenvectors, $T$ must be an operator as the image of the eigenvector ${ \v \in V }$ under $T$,
					\[ T(\v) = c\v \in V \]
					 is, by definition, in the same space as the eigenvector itself (if there were a change of basis then we would not consider it to be the same vector). In fact, this is the key of the relationship between eigenvectors and invariant subspaces: The space spanned by eigenvectors of $T$ is $T$-invariant. The bases of $T$-invariant subspaces, however, need not be eigenvectors. For example, a rotation of a plane in a 3d space has the plane as a T-invariant subspace but the only eigenvector is the axis of rotation, perpendicular to the plane.
				 }
				\item{An eigenvector may not be $\0$ but an eigenvalue may be 0. As a consequence of this, every nonzero vector in the kernel is an eigenvector (with eigenvalue 0). As a consequence of this, if $W$ is a 2-dimensional $T$-invariant subspace, for example, and its image under $T$ is one-dimensional (a line inside the plane of $W$) then vectors in $W$ that are not in the line of the image will be in the kernel of $T$ and so also eigenvectors but with the eigenvalue 0. It is also worth noting that they are still considered to be parallel to their image under $T$ because, by convention, the zero vector $\0$ is considered to be parallel to all vectors. 
				}
				\item{If we speak of an eigenvector of a square matrix then we are referring to an eigenvector of \textbf{left multiplication} by the matrix, i.e. a nonzero vector $\x$ such that,
					\[ A\x = c\x. \]
					Clearly if $\x_B$ is the coordinate vector of ${ \v \in V }$ with respect to $B$ --- a basis of $V$ --- and $A$ is the matrix of $T$ with respect to the basis $B$, then
					\[ A\x_B = c\x_B \iff T(\v) = c\v. \]
					As a result, all similar matrices have the same eigenvalues.	
				}
			\end{itemize}
		 }
	 
	 	\bigskip
	 	\labeledProposition{Let $T$ be a linear operator on a finite-dimensional vector space $V$.%
	 		\begin{enumerate}[label=(\roman*)]
	 			\item{If $V$ has dimension $n$, then $T$ has at most $n$ eigenvalues.}
	 			\item{If $\F{}$ is the field of complex numbers and ${ V \neq 0 }$, then $T$ has at least one eigenvalue, and hence it has an eigenvector.}
	 		\end{enumerate}
	 	}{num-eigenvalues}
	 	\begin{proof}\nl
	 		\begin{enumerate}[label=(\roman*)]
	 			\item{For any field $\F{}$, a polynomial of degree $n$ can have at most $n$ different roots (see Artin[373]). Since $T$ is defined over a vector space of dimension $n$, the degree of the characteristic polynomial of $T$ is $n$. Then, by \autoref{theo:eigenvalues_are_roots_of_characteristic_poly} we can have a maximum of $n$ eigenvalues.}
	 			\item{Every polynomial of positive degree with complex coefficients has at least one complex root. This fact is called the Fundamental Theorem of Algebra (\href{https://en.wikipedia.org/wiki/Fundamental_theorem_of_algebra}{wikipedia}).}
	 		\end{enumerate}
	 	\end{proof}
	 
	 	\bigskip
	 	\labeledTheorem{The eigenspace of an eigenvalue is a vector subspace.}{eigenspaces-are-vector-subspaces}
	 	\begin{proof}
	 		Let $S$ be the set of all eigenvectors of a linear operator $T$ corresponding to a particular eigenvalue $c$. Then the eigenspace is defined as,
	 		\[ E = S \cup \{\0\}. \]
	 		Then $E$ is a subspace because it contains the zero vector and
	 		\[ \v,\w \in E \implies T(\alpha\v + \beta\w) = \alpha(c\v) + \beta(c\w) = c(\alpha\v + \beta\w) \implies \alpha\v + \beta\w \in E. \qedhere \]
	 	\end{proof}
 		\begin{corollary}
 			Any linear combination of eigenvectors with eigenvalue $c$ is also an eigenvector with eigenvalue $c$.
 		\end{corollary}
 	

 		\bigskip
 		\labeledProposition{Eigenvectors corresponding to different eigenvalues are linearly independent. That's to say: Let ${ \v_1,\dots,\v_r \in V }$ be eigenvectors for a linear operator $T$, with distinct eigenvalues ${ c_1,\dots,c_r }$. Then the set ${ \{\v_1,\dots,\v_r\} }$ is linearly independent.}{distinct-eigenvals-means-independent-eigenvecs}
 		\begin{proof}
 			Assume for contradiction that there exists a linear relation between the set of eigenvectors,
 			\[ \alpha_1\v_1 + \cdots + \alpha_r\v_r = \0. \]
 			Linearity of $T$ gives us,
 			\[ T(\alpha_1\v_1 + \cdots + \alpha_r\v_r) = \alpha_1T(\v_1) + \cdots + \alpha_rT(\v_r) = T(\0) = \0 \]
 			while the eigenvector property gives us,
 			\[ \alpha_1T(\v_1) + \cdots + \alpha_rT(\v_r) = \alpha_1c_1\v_1 + \cdots + \alpha_rc_r\v_r \]
 			so we have the simultaneous equations,
 			\begin{align*}
 				&& \alpha_1\v_1 + \cdots + \alpha_r\v_r &= \0  \\
 				&& \alpha_1c_1\v_1 + \cdots + \alpha_rc_r\v_r &= \0.
 			\end{align*}
 			If we multiply the first equation by $c_r$ and subtract the second equation from it we get,
 			\[ \alpha_1(c_r - c_1)\v_1 + \cdots \alpha_{r-1}(c_r - c_{r-1})\v_{r-1} = \0. \]
 			Since all the eigenvalues are distinct, for ${ i \neq j,\; c_i - c_j \neq 0 }$ and the eigenvectors $\v_i$, by definition, are nonzero. So, this equation implies that either ${ \alpha_1,\dots,\alpha_{r-1} = 0 }$ or there is a linear relation between the vectors ${ \v_1,\dots,\v_{r-1} }$.\\
 			This dependence of the properties of the $r$-length list on the properties of the ${ (r-1) }$-length list signals that we can set up a proof by induction using the hypothesis that a $k$-length list is linearly independent.\\
 			If we use ${ k = 2 }$ as the base case, set up the linear relation and use the eigenvector property as before then this results in,
 			\[ \alpha_1(c_2 - c_1)\v_1 = \0. \]
 			As before, both ${ c_2 - c_1 }$ and $\v_1$ are nonzero so this implies that ${ \alpha_1 = 0 }$. This, in turn, implies that ${ \alpha_2 = 0 }$ also and so, the list of length ${ k = 2 }$ is linearly independent.\\
 			Then the induction step is to assume that the list of length ${ k = r-1 }$ is linearly independent and show that this implies that the list of length ${ k = r }$ is linearly independent. We have already shown that if we set up a linear relation on a list of eigenvectors of length ${ k = r }$ then the eigenvector property implies that,
 			\[ \alpha_1(c_r - c_1)\v_1 + \cdots \alpha_{r-1}(c_r - c_{r-1})\v_{r-1} = \0. \]
 			Now we can use the induction hypothesis to assert that ${ \v_1,\dots,\v_{r-1} }$ are linearly independent implying that ${ \alpha_1,\dots,\alpha_{r-1} = 0 }$. This, in turn, implies that ${ \alpha_r = 0 }$ meaning that ${ \v_1,\dots,\v_r }$ is linearly independent.
 		\end{proof}
 	
 	
 		\bigskip
 		\labeledProposition{If we consider an $n \times n$ matrix $A$ with real entries as a matrix in $\C{n \times n}$ and it has a complex eigenvalue $\lambda$ with a corresponding eigenvector $\v$, then the complex conjugate $\conj{\lambda}$ is also an eigenvalue and has a corresponding eigenvector $\conj{\v}$, the complex conjugate of $\v$.}{complex-eigenvalues-and-eigenvectors-come-in-pairs-with-their-conjugates}
 		\begin{proof}
 			\[\begin{aligned}
 				&& A\v &= \lambda\v \\
 				&\iff & \conj{A\v} &= \conj{\lambda\v} \\
 				&\iff & \conj{A} \, \conj{\v} &= \conj{\lambda} \, \conj{\v} &\sidecomment{by \ref{prop:conjugate-of-matrix-times-vector-is-conjugate-matrix-times-conjugate-vector}}.
 			\end{aligned}\]
 			But $A$ is a matrix with only real entries and so ${ \conj{A} = A }$. So the previous result implies that
 			\[ A \conj{\v} = \conj{\lambda} \, \conj{\v} \]
 			which means that $\conj{\lambda}$ is an eigenvalue of $A$ with a corresponding eigenvector $\conj{\v}$. 
 		\end{proof}
 	
 	
 	
 		\bigskip
 		\labeledProposition{Let ${ A \in \C{n \times n} }$ be a matrix in complex space but that \textit{has only real entries} and let $A$ have a real eigenvalue $\lambda$. Then we can find a basis of the eigenspace of $\lambda$ containing only real vectors.}{for-real-matrix-in-complex-space-a-real-eigenvalue-has-real-eigenvectors}
 		\begin{proof}
 			Let ${ A \in \C{n \times n} }$ have only real entries and a real eigenvalue $\lambda$. Suppose $\v$ is a complex eigenvector of $A$ corresponding to $\lambda$. Then,
 			\[\begin{aligned}
 				&& A\v &= \lambda \v \\
 				&\iff & \conj{A\v} &= \conj{\lambda \v} &\sidecomment{} \\
 				&\iff & A \conj{\v} &= \lambda \conj{\v}. &\sidecomment{$A$ and $\lambda$ are real} \\
 			\end{aligned}\]
 			So $\conj{\v}$ is also an eigenvector of $\lambda$. Then, by \autoref{prop:linear-span-of-conjugate-pair-vectors-is-span-of-real-and-imaginary-parts} the two real vectors $\RePart{\v}$ and $\ImPart{\v}$ span the same space as $\v$ and $\conj\v$ and so can also be chosen as eigenvectors for $\lambda$.
 		\end{proof}
 		\note{It may also be possible to prove this using the definition of the eigenspace as the nullspace of the matrix $(A - \lambda I)$ but the above proof is constructive.}
 	
 	
 		\bigskip
 		\labeledProposition{If two matrices $A$ and $B$ have the same eigenvalues (with the same multiplicity) and corresponding eigenvectors and one of the matrices is diagonalisable, then ${ A = B }$.}{diagonalisable-matrices-with-same-eigenvals-and-eigenvecs-are-equal}
 		\begin{proof}
 			\WLOG assume that $A$ is diagonalisable. Since $B$ has the same eigenvectors as $A$, it must have the same dimension and since it has the same eigenvalues with the same multiplicity as $A$, it must also be diagonalisable.\\
 			
 			By diagonalisability,
 			\[ D_1 = \inv{P_1} A P_1 \eqand D_2 = \inv{P_2} B P_2. \]
 			Since $B$ has the same eigenvalues as $A$, the diagonal entries must be the same upto order (\addref). We can choose them to be in the same order and then we have,
 			\[ D_1 = D_2. \]
 			Each of these eigenvalues in the diagonal entries has an associated eigenvector in the matrices $P_1$ and $P_2$ and, since these eigenvectors are the same in both $A$ and $B$, and we have chosen the order of the eigenvalues in the diagonalisation to be the same, then the order of the eigenvectors in $P_1$ and $P_2$ also must be the same. So we have,
 			\[ P_1 = P_2. \]
 			Therefore,
 			\[ A = P_1 D_1 \inv{P_1} = P_2 D_2 \inv{P_2} = B. \]	
 		\end{proof}
 	
 	
	 	\biggerskip
	 	\subsubsubsection{Examples of Eigenvectors and Eigenvalues}
	 	\begin{exe}
	 		\item{If we take the matrix,
	 			\[ 
		 			\begin{bmatrix}
		 			3 & 1\\
		 			0 & 2
		 			\end{bmatrix}
	 			\]
	 			we can determine its eigenvectors by finding the solutions to the equation,
	 			\[
	 			 	\begin{bmatrix}3 & 1\\0 & 2\end{bmatrix}\begin{bmatrix}x_1\\x_2\end{bmatrix} = c\begin{bmatrix}x_1\\x_2\end{bmatrix}.	 			 	
	 			\]
	 			This gives us two simultaneous equations in three unknowns: the two vector dimensions ${ x_1,x_2 }$ and the eigenvalue $c$,
	 			\begin{align*}
	 			&& 3x_1 + x_2 &= cx_1 \\
	 			&& 2x_2 &= cx_2 &\sidecomment{} \\
	 			\end{align*}
	 			which imply that,
	 			\[ 6x_1 = 2cx_1 - cx_2 \iff (6/c - 2)x_1 = -x_2 \iff x_2 = (2 - 6/c)x_1. \]
	 			So, if ${ x_1 = 1 }$ then ${ x_2 = 2 - (6/c) }$ and we have the follwing eigenvector/eigenvalue pairs.
	 			\[
	 				c=3: \begin{bmatrix}1\\0\end{bmatrix}, \; c=1: \begin{bmatrix}1\\-4\end{bmatrix} \wrong
	 			\]
	 			\subparagraph{Why does this not work?} There is an additional constraint not expressed in the linear system here: eigenvectors are nonzero by definition. This means that we have the additional constraint,
	 			\[ x_1x_2 \neq 0 \]
	 			which cannot be expressed in a linear system of equations. When ${ c \not\in \{2,3\} }$ both $x_1$ and $x_2$ are zero and the vector is not an eigenvector.\\
	 			So, how can we systematically restrict the values of $c$ to only those that produce valid eigenvectors? If we follow the alternative logic:
	 			\begin{align*}
	 			&& \begin{bmatrix}3 & 1\\0 & 2\end{bmatrix}\begin{bmatrix}x_1\\x_2\end{bmatrix} &= c\begin{bmatrix}x_1\\x_2\end{bmatrix}  \\
	 			&\iff & \begin{bmatrix}3 & 1\\0 & 2\end{bmatrix}\begin{bmatrix}x_1\\x_2\end{bmatrix} - c\begin{bmatrix}x_1\\x_2\end{bmatrix} &= \0 &\sidecomment{} \\
	 			&\iff & \left( \begin{bmatrix}3 & 1\\0 & 2\end{bmatrix}I - cI \right)\begin{bmatrix}x_1\\x_2\end{bmatrix} &= \0 &\sidecomment{} \\
	 			&\iff & \begin{bmatrix}3 - c & 1\\0 & 2 - c\end{bmatrix}\begin{bmatrix}x_1\\x_2\end{bmatrix} &= \0 &\sidecomment{} \\
	 			\end{align*}
	 			Now if we let
	 			\[ A = \begin{bmatrix}3 - c & 1\\0 & 2 - c\end{bmatrix} \]
	 			then the nullspace of $A$ is the space of vectors ${ (x_1,x_2)^T }$ that satisfy this equation. If $A$ is invertible and nonsingular then this space is the trivial space ${ \{\0\} }$ but, if $A$ is singular however, then there exist nonzero vectors that satisfy this equation. Therefore, it is precisely the nonzero vectors that are in the nullspace of this matrix $A$ when it is singular that are the eigenvectors we are looking for. So, if we first determine the values of $c$ for which $A$ is singular, we can then determine the eigenvectors. Since, by \autoref{prop:properties_of_non_bijective_lin_operator}, $A$ is singular if and only if ${ det\,A = 0 }$, we are looking for \textit{precisely the values of $c$ which make ${ det\,A = 0 }$}.
	 		}
 			\item{The minimal linear transformation seen in \ref{ex:minimal-linear-transformation} --- which is just scalar multiplication --- has every vector as its eigenvectors because
 				\[ T(\v) = A\v = a\v \]
 				for all ${ \v \in V }$. So every vector in $V$ is an eigenvector with eigenvalue $a$.
 			}\label{ex:eigenvectors-of-minimal-linear-transformations}
	 	\end{exe}
 	
 		\bigskip
 		\subsubsection{Matrices of Eigenvectors}
 		\bigskip
 		If ${ \v_1 \in V }$ is a eigenvector of a linear transformation $T$ and we extend the set ${ \{\v_1\} }$ (by \autoref{prop:lin_ind_set_can_be_extended_to_basis}) to a basis of $V$, say ${ \{\v_1,\dots,\v_n\} }$, then the matrix of $T$ will have the block form,
 		\[
 			\begin{bmatrix}
 			c & B\\
 			0 & D
 			\end{bmatrix} =
 			\begin{bmatrix}
 			c & \dots & \dots\\
 			0 & \dots & \dots\\
 			\vdots & \dots & \dots\\
 			0 & \dots & \dots
 			\end{bmatrix}
 		\]
 		where $c$ is the eigenvalue of $\v_1$. This is the same block decomposition as that shown for $T$-invariant spaces in \ref{eq:invariant_space_block_decomposition} with the case of a 1-dimensional invariant subspace.
 		
 		\medskip
 		\labeledProposition{If $A$ is the matrix of a linear operator $T$ with respect to a basis $B$ then the matrix $A$ is diagonal iff every basis vector in $B$ is an eigenvector of $T$.}{diagonal_iff_every_basis_vector_eigenvector}
 		\begin{proof}
 			The defining property of the matrix $A$ is that the $j$-th column is the coordinates of the image of the $j$-th basis vector in $B$ under $T$,
 			\[ A(:j) = T(\v_j) = a_{1j}\v_1 + \cdots + a_{nj}\v_n. \]
 			For an eigenvector $\v_j$, ${ T(\v_j) = c\v_j = a_{jj}\v_j }$ so that ${ a_{jj} = c }$ the eigenvalue and for all ${ a_{ij} }$ such that ${ i \neq j }$, ${ a_{ij} = 0 }$. 
 		\end{proof}
 		\begin{corollary}
 			\label{coro:diagonal-similar-matrix-iff-exists-basis-of-eigenvectors}
 			The matrix of a linear operator $T$ over a vector space $V$ is similar to a diagonal matrix iff there exists some basis of $V$ solely comprised of eigenvectors of $T$. 
 		\end{corollary}
 	
 		\bigskip
 		\labeledProposition{Similar matrices have the same eigenvalues.}{similar_matrices_same_eigenvalues}
 		\begin{proof}
 			Similar matrices represent the same transformation with respect to different bases and for a matrix $A$ representing a transformation $T$ with respect to an arbitrary basis $B$,
 			\[ T(\v) = c\v \iff A\v_B = c\v_B. \]
 			That's to say, the eigenvalues are not dependent on the basis with respect to which a coordinate vector is defined.
 		\end{proof}
	}


% --------------------


	\pagebreak
	\searchableSubsection{Diagonalisation}{linear algebra}{
		\biggerskip
		\boxeddefinition{\textbf{(Diagonalisation)} The process of determining a diagonal matrix that is similar to a given matrix of a linear operator is known as \textbf{diagonalisation}.}
		\boxeddefinition{\textbf{(Eigenbasis)} If a linear operator $T$ is defined over a vector space $V$, an \textit{eigenbasis} of $T$ is a basis of $V$ consisting solely of eigenvectors of $T$.\\
			
		If $A$ is the matrix of $T$ \wrt the standard basis and $P$ is the matrix whose columns are the elements of the eigenbasis of $T$ then, by \autoref{prop:diagonal_iff_every_basis_vector_eigenvector},
			\[ \inv{P} A P = D \]
		is diagonal.
		}\label{def:eigenbasis}
		
		\bigskip
		\subsubsection{Existence of Eigenvectors}
		\begin{itemize}
			\item{Every linear operator on a complex vector space has at least one eigenvector and, in most cases, these form a basis.}
			\item{Linear operators over real vector spaces need not have eigenvectors (e.g. rotation of the plane $\R{2}$ by an angle $\theta$ has no eigenvector unless ${ \theta = 0 \text{ or } \pi }$).}
			\item{Real matrices that are \textit{positive} (having only positive components) are guaranteed to have at least one positive eigenvector.}
		\end{itemize}
	
		\bigskip
		\subsubsection{The Effect of Multiplication by a Positive Matrix}
		\TODO{Artin[134]}
		
		\bigskip
		\subsubsection{Determining the Eigenvectors}
		The process of finding eigenvectors is to first determine the eigenvalues and then calculate the eigenvectors that correspond to those eigenvalues. Let $I$ be the identity operator. Then,
		\[ T(\v) = c\v \iff T(\v) - c\v = \0 \iff [T - cI](\v) = \0 \]
		where the expression ${ T - cI }$ is a linear combination of linear transformations and so is also a linear transformation. Furthermore, it is an operator as both its terms are operators (in fact they need to have the same dimensions in order for the expression to make sense).\\
		Two things are clear from this expression:
		\begin{enumerate}[label=(\roman*)]
			\item{The matrix of the linear operator ${ T - cI }$ is ${ A - cI }$ where $I$ is the identity matrix.}
			\item{The eigenvector $\v$ is in the kernel of ${ T - cI }$ and so is in the nullspace of ${ A - cI }$.}
		\end{enumerate}
	
		\medskip
		\labeledProposition{A linear operator $T$ has a nontrivial kernel iff 0 is an eigenvalue of $T$.}{lin_op_singular_iff_0_eigenvalue}
		\begin{proof}
			This follows from the fact that, if we let ${ \v \neq \0 \in ker\,T }$, then
			\[ T(\v) = \0 = 0\v = c\v \]
			for ${ c = 0 }$ so that a nontrivial kernel implies that 0 is an eigenvalue. Conversely, if 0 is an eigenvalue we must have ${ 0\v = \0 = T(\v) }$ and, since if a vector $\v$ is an eigenvector, by definition, ${ \v \neq \0 }$, this therefore implies that the kernel contains a nonzero vector.
		\end{proof}
		\begin{corollary}
			A linear operator $T$ has all the properties in \autoref{prop:properties_of_non_bijective_lin_operator} iff 0 is an eigenvalue of $T$.
		\end{corollary}
	
		\medskip
		\labeledProposition{The eigenvalues of a linear operator $T$ are the scalars ${ c \in \F{} }$ such that the linear operator ${ [T - cI] }$ is singular.}{eigenvals_are_c_such_that_T-cI_singular}
		\begin{proof}
			The eigenvalues of a linear operator $T$ are the scalars ${ c \in \F{} }$ such that there exists a nonzero vector $\v$ with ${ [T - cI](\v) = \0 }$. If such a vector exists then the kernel of ${ T - cI }$ is nontrivial and so, by \autoref{prop:properties_of_non_bijective_lin_operator}, ${ T - cI }$ is singular.
		\end{proof}
		\begin{corollary}
			If ${ A - cI }$ is the matrix of ${ T - cI }$, \textbf{the eigenvalues of $\bm{T}$ are the scalars ${\bm{ c \in \F{} }}$ such that ${\bm{ det(A - cI) = 0 }}$}.
		\end{corollary}
		\begin{corollary}
			The eigenvalues of ${ A - cI }$ are the same as the eigenvalues of ${ cI - A }$.
		\end{corollary}
		\begin{proof}
			If $A$ is a ${ n \times n }$ matrix representing the operator $T$ then,
			\[ det(-A) = (-1)^n det(A). \]
			So, if ${ det(A) = 0 }$ then ${ det(-A) = 0 }$ also. Therefore,
			\[ det(A - cI) = 0 \iff det(cI - A) = 0. \qedhere \]
		\end{proof}
	
		\bigskip
		\subsubsection{The Characteristic Polynomial}
		\notation{It is customary to use either the variable $t$ or $\lambda$ to denote the eigenvalue in the characteristic polynomial.}
		\boxeddefinition{The \textbf{characteristic polynomial} of a linear operator $T$ is the polynomial,
			\[ p(t) = det(tI - A) = \sum s(j_1,\cdots,j_n)a_{1j_1} \cdots a_{nj_n} \]
			where the sum is defined over all permutations ${ j_1, \cdots, j_n }$ of ${ \{1,\dots,n\} }$  and ${ s(j_1,\cdots,j_n) }$ is the sign of the permutation.
		}\label{def:characteristic-polynomial}
		\note{The determinant is an expression in which every term is the product of a component in every column and row of the matrix with no column or row appearing more than once in each term,
			\[
				tI - A = \begin{bmatrix}
						(t - a_{11}) & -a_{12} & \cdots & -a_{1n}\\
						-a_{21}	& (t - a_{22}) & \cdots & -a_{2n}\\
						\vdots &   &  & \vdots\\
						-a_{n1} & \cdots & \cdots & (t - a_{nn})
						\end{bmatrix}.
			\]
			It can be seen in the matrix of ${ tI - A }$ that the highest power of $t$ will be obtained in the term of the determinant that forms the product of all the diagonal terms ${ a_{11} \cdots a_{nn} }$ which occurs when ${ j_1, \cdots, j_n = 1,\dots,n }$. This term of the determinant will be the product of precisely $n$ terms containing the eigenvalue $t$. Therefore the result is a polynomial of degree $n$ in the eigenvalue $t$.
		}
	
		\medskip
		\labeledTheorem{The eigenvalues of a linear operator are the roots of its characteristic polynomial.}{eigenvalues_are_roots_of_characteristic_poly}
		\begin{proof}
			If $p(t)$ is the characteristic polynomial of a linear operator $T$ then the values of $t$ for which ${ p(t) = 0 }$ are the values of $t$ such that ${ det(tI - A) = 0 }$ and these are precisely the eigenvalues.
		\end{proof}
	
		\bigskip
		\labeledProposition{The eigenvalues of an upper or lower triangular matrix are its diagonal entries.}{triangular_matrix_eigenvalues_are_on_diagonal}
		\begin{proof}
			The determinant of a triangular matrix is equal to the product of its diagonal entries and if $A$ is a triangular matrix then ${ tI - A }$ is also triangular. Therefore, the characteristic polynomial is simply,
			\[ p(t) = (t - a_{11})\cdots(t - a_{nn}) \]
			and so the eigenvalues are the diagonal entries ${ a_{11},\dots,a_{nn} }$.
		\end{proof}
	
		\bigskip
		\labeledProposition{A positive matrix (a matrix whose entries are all positive) has at least one eigenvector with positive coordinates.}{positive_matrix_has_at_least_1_positive_eigenvector}
		\note{An abstract vector does not have coordinates so when we refer to a "positive" vector with positive-valued coordinates, this is with respect to a particular basis. In this context, the basis in question is the basis with respect to which the matrix outputs the transformed vectors.}
		\begin{proof}
			\TODO{review: this "proof" is not really a proof, more an example using a 2x2 matrix.}
		\end{proof}
	
		\bigskip
		\labeledProposition{The characteristic polynomial of a linear operator does not depend on the basis with respect to which the matrix of the operator is defined.}{characterstic_poly_independent_of_basis_of_matrix}
		\begin{proof}
			For two similar matrices representing the same linear operator $T$ we have,
			\[ A' = PA\inv{P} \]
			where $P$ is the matrix of change of basis between the bases of $A$ and $A'$. If we form the characteristic polynomial of $A'$,
			\begin{align*}
			&& tI - A' &= tI - PA\inv{P} \\
			&\iff &  &= PtI\inv{P} - PA\inv{P} &\sidecomment{\small{${ PtI\inv{P} = tP\inv{P} = tI }$}} \\
			&\iff &  &= P(tI - A)\inv{P} &\sidecomment{by distributivity of matrix multiplication}. \\
			\end{align*}
			Then,
			\begin{align*}
			&& det(tI - A') &= det(P(tI - A)\inv{P}) \\
			&\iff &  &= det\,P \cdot det(tI - A) \cdot det\,\inv{P} & \\
			&\iff &  &= det(tI - A). & \\
			\end{align*}
			This result, ${ det(tI - A') = det(tI - A) }$, must hold for all $t$ and therefore implies that, for $p,p'$ characteristic polynomials of $A$ and $A'$ respectively,
			\[ \forall t \in \F{} \logicsep p(t) = p'(t). \]
			This implies that the characteristic polynomials are equal.
		\end{proof}
	
		\bigskip
		\labeledProposition{The characteristic polynomial ${ p(t) }$ of a matrix $A$ has the form
			\[ p(t) = t^n - (tr\,A)t^{n-1} + \cdots + (-1)^n(det\,A), \]
			where ${ tr\,A }$ is the trace of $A$ (see: \ref{sssection:trace}):
			\[ tr\,A = a_{11} + \cdots + a_{nn}. \]
		}{characteristic_poly_coefficients}
		\begin{proof}
			Calculation of the characteristic polynomial of a matrix $A$ is calculation of ${ p(t) = det(tI - A) }$ the determinant of the matrix ${ tI - A }$ which takes the form,
			\[
				tI - A = \begin{bmatrix}
						(t - a_{11}) & \cdots & \cdots 			   & \vdots\\
						\vdots & (t - a_{22}) & \cdots 			   & \vdots\\
						\vdots &   			&  					   & \vdots\\
						\vdots & \cdots 	& (t - a_{(n-1)(n-1)}) & -a_{(n-1)n}\\
						\vdots & \cdots 	& -a_{n(n-1)} 		   & (t - a_{nn})
						\end{bmatrix}.
			\]
			This calculation proceeds with terms of products of elements from each row and a permutation of the column indices (see: \ref{sssection:determinant_formula}) of which we examine the first two terms.\\
			
			The first term is for the identity permutation ${ j_1,\dots,j_n = 1,\dots,n }$ along the diagonal:
			\begin{align*}
			&& a_{11} \cdots a_{nn} &= (t - a_{11})\cdots(t - a_{nn})  \\
			&& &= t^n - (a_{11} + \cdots + a_{nn})t^{n-1} \\ 
			&&&\hspace{20pt} + (a_{11}a_{22} + a_{11}a_{33} + \cdots + a_{(n-1)(n-1)}a_{nn})t^{n-2} \\
			&&&\hspace{20pt} \dots + (-1)^n(a_{11} \cdots a_{nn}) &\sidecomment{}
			\end{align*}
			The second term is the permutation one swap away from identity 
			\[ j_1,\dots,j_n = 1,\dots,n,(n-1) \]
			which is an odd permutation, so the sign is $-1$:
			\begin{align*}
			& \hspace{15pt} (-1)a_{11} \cdots a_{(n-2)(n-2)}a_{(n-1)n}a_{n(n-1)} &\\
			& =(-1)(t - a_{11}) \cdots (t - a_{(n-2)(n-2)})(-a_{(n-1)n})(-a_{n(n-1)}) & \\
			& = - a_{(n-1)n}a_{n(n-1)}t^{n-2} + a_{(n-1)n}a_{n(n-1)}(a_{11} + \cdots +  a_{(n-2)(n-2)})t^{n-3} & \\ 
			& \hspace{20pt} \dots - (-1)^n(a_{11} \cdots a_{(n-2)(n-2)}a_{(n-1)n}a_{n(n-1)}) &
			\end{align*}
			From these first two terms we can discern enough about the general pattern of the characteristic polynomial to see that the first two terms in $t^n$ and $t^{n-1}$ are produced by the first permutation and take the form
			\[ t^n - (a_{11} + \cdots + a_{nn})t^{n-1} = t^n - (tr\,A)t^{n-1} \]
			as required. We can also see that the final terms of each permutation --- those that involve no powers of $t$ --- are going to sum up to the value of ${ (-1)^n(det\,A) }$. Therefore the characteristic polynomial takes the form,
			\[ p(t) = t^n - (tr\,A)t^{n-1} + \cdots + (-1)^n(det\,A) \]
			as claimed.
		\end{proof}
	
		\bigskip
		\begin{corollary}
			\label{coro:matrix-trace-independent-of-basis}
			The trace of a matrix of a linear operator is independent of the basis with respect to which the matrix is defined.
		\end{corollary}
		\begin{proof}
			Let $T$ be a linear operator and $A$ be the matrix of $T$ with respect to a basis $B$. Then \autoref{prop:characterstic_poly_independent_of_basis_of_matrix} tells us that any matrix of the same linear operator defined with respect to some other basis (i.e. a similar matrix to $A$) has the same characteristic polynomial. \autoref{prop:characteristic_poly_coefficients} tells us that the coefficients of this characteristic polynomial include the trace of the matrix $A$. Therefore, if $A'$ is a matrix of $T$ defined against the basis $B'$ and $P$ is the change of basis matrix such that ${ B'P = B }$ then,
			\[ A' = PA\inv{P} \iff p'(t) = p(t) \iff tr\,A' = tr\,A.  \qedhere \]
		\end{proof}
	
		\medskip\note{As a result of this we can refer to the characteristic polynomial, determinant and trace of a linear operator $T$ without reference to a particular matrix or basis.}
		

	
		\bigskip
		\labeledProposition{Let $T$ be a linear operator on a finite-dimensional complex vector space $V$. There is a basis $B$ of $V$ such that the matrix $A$ of $T$ is upper triangular.}{complex_space_exists_basis_st_matrix_is_upper_triang}
		\begin{proof}
			By \autoref{prop:num-eigenvalues}, $T$ has at least one eigenvector. We can extend this eigenvector to a basis of $V$, say,
			\[ B' = \{\v_1',\dots,\v_n'\}. \]
			Then the first column of the matrix $A'$ of $T$ with respect to $B'$ will be
			\[ (c_1,0,\dots,0)^T \]
			where $c_1$ is the eigenvalue of $\v_1'$. Therefore $A'$ has the form
			\[
				A' = \begin{bmatrix}
					c_1 & \cdots \\
					0   &   D \\
					\end{bmatrix} 
			\]
			where $D$ is a ${ (n-1) \times (n-1) }$ matrix and, if ${ P = \inv{[B']}I = \inv{[B']} }$ is the change of basis matrix, then
			\[ A' = PA\inv{P}. \]
			Now we can use induction on the dimension of the matrix $n$ and the induction hypothesis will be that there exists some upper triangular
			\[ Q' = QD\inv{Q}. \]
			Define
			\[ Q_1 = \begin{bmatrix}
					1 & 0\\
					0 & Q
					\end{bmatrix}. 
			\]
			Then
			\[ (Q_1P)A\inv{(Q_1P)} = Q_1PA\inv{P}\inv{Q_1} = Q_1A'\inv{Q_1} \]
			takes the form
			\[
				\begin{bmatrix}
				c_1 & \cdots\\
				0   & QD\inv{Q}
				\end{bmatrix} 
			\]
			which is upper triangular. This proves the induction step.
		\end{proof}
	
		\medskip\note{Note that this proof is over the complex number field but the same proof would work over any field that contains all the roots of the characteristic polynomial.}

	
		\bigskip
		\labeledTheorem{Let $T$ be a linear operator on a vector space $V$ of dimension $n$ over a field $F$. Assume that its characteristic polynomial has $n$ \textbf{distinct} roots in $F$. Then there is a basis for $V$ with respect to which the matrix of $T$ is diagonal.}{maximum-distinct-roots-of-char-poly-implies-diagonal}
		\begin{proof}
			If the characteristic polynomial has $n$ distinct roots then there are $n$ distinct eigenvalues along with their associated eigenvectors. By \autoref{prop:distinct-eigenvals-means-independent-eigenvecs}, these eigenvectors form a linearly independent set. Since the dimension of the space $V$ is $n$, by \autoref{theo:lin_ind_set_of_dim_cardinality_is_basis}, these eigenvectors form a basis of $V$. Then, by \autoref{prop:diagonal_iff_every_basis_vector_eigenvector}, the matrix of $T$ with respect to this basis is diagonal.
		\end{proof}
	
		\bigskip\note{Note that:
			\begin{itemize}
				\item{The diagonal entries of the matrix of a linear operator with respect to a basis of eigenvectors are the eigenvalues. For this reason, the set of values is wholly determined by the linear operator although the order is determined on the order of the vectors in the basis set (which is not significant);
				}
				\item{If a matrix $A$ is found to be similar to a diagonal matrix $B$ via a change of basis expressed in the matrix $P$ then, by \autoref{theo:exponentiation-of-similar-matrices},
					\[ A^m = (\inv{P}BP)^m = \inv{P}B^mP. \]
				}
			\end{itemize}
		}
		\begin{corollary}\label{coro:matrix-with-eigenbasis-has-trace-equal-to-sum-of-eigenvalues-and-determinant-equal-to-product}
			If a linear operator on a vector space of dimension $n$ over a field $F$ has a characteristic polynomial with $n$ distinct roots then its determinant is equal to the product of its eigenvalues and its trace is equal to the sum of its eigenvalues.
		\end{corollary}
		\begin{proof}
			By \autoref{prop:similar-matrices-have-same-determinant} and \autoref{coro:matrix-trace-independent-of-basis}, the determinant and trace are independent of the basis with respect to which the matrix is defined and so the existence of a basis with respect to which the matrix is diagonal means that we can look at the determinant and trace of the diagonal matrix.
		\end{proof}
	
	
		\bigskip
		\labeledProposition{Let $T$ be a linear operator on a finite-dimensional vector space $V$ over a field $F$, $q(t)$ an arbitrary polynomial over $F$, and $\v$ an eigenvector of $T$ with eigenvalue $c$. Then
			\[ q(T)\v = q(c)\v. \]
		}{polynomial-of-linear-map-times-eigenvec-is-polynomial-of-eigenval-times-eigenvec}
		\begin{proof}
			The polynomial $q(t)$ has the general form,
			\[ q(t) = a_n t^n + \cdots + a_1 t + a_0. \]
			Since $T$ is an operator we can use \ref{defn:polynomials-of-linear-operators} to define,
			\[ q(T) = a_n T^n + \cdots + a_1 T + a_0 I. \]
			Then,
			\[\begin{aligned}
				q(T)\v &= a_n T^n \, \v + \cdots + a_1 T \v + a_0 \v \\
				&= a^n c^n \, \v + \cdots + a_1 c \v + a_0 \v \\
				&= (a^n c^n + \cdots + a_1 c + a_0) \v \\
				&= q(c)\v. \qedhere
			\end{aligned}\]
		\end{proof}
		\begin{corollary}
			If $p(t)$ is the characteristic polynomial of a \textbf{diagonalizable} linear operator $T$ over a finite-dimensional vector space, then $p(T)$ is 0, the zero operator $T_0$.
		\end{corollary}
		\begin{proof}
			As above, for an eigenvector $\v$ with eigenvalue $c$,
			\[ p(T)\v = p(c)\v. \]
			But now, $p(t)$ is the characteristic polynomial of $T$ and $c$, as an eigenvalue of $T$, \textit{is a root of the characteristic polynomial}. So, for any eigenvector of $T$ corresponding to the eigenvalue $c$,
			\[  p(T)\v = p(c)\v = 0\v = 0. \]
			Since $T$ is diagonalizable, there exists a basis of the space $V$ comprised solely of eigenvectors. Therefore, for any vector ${ \v \in V }$, we can express it as a linear combination of eigenvectors each of which will be mapped to the zero vector by $p(T)$.
		\end{proof}
	
		\bigskip
		\labeledProposition{Let $T$ be a linear operator on a finite-dimensional vector space $V$, and let $W$ be a $T$-invariant subspace of $V$. Then the characteristic polynomial of the restriction of $T$ to $W$, $T_w$, divides the characteristic polynomial of $T$.}{char-poly-of-restriction-to-invariant-space-divides-char-poly-of-T}
		\begin{proof}
			Let ${ B_w = \{ \w_1, \dots, \w_k \} }$ be a basis of $W$ and extend it to a basis of $V$, ${ B = \{ \w_1, \dots, \w_k, \v_1, \dots, \v_{n-k} \} }$. Then the matrix of $T$ with respect to the basis $B$ has the form,
			\[ A = 	\begin{bmatrix}
						A_w & C  \\
						0   & D
					\end{bmatrix}
			\]
			where ${ A_w = T_w(B_w) }$ is the matrix of $T_w$ \wrt to the basis $B_w$.\\
			So, the characteristic polynomial $p(t)$ of $A$ is
			\[\begin{aligned}
				p(t) = \det (A - tI_n) &=
									\begin{vmatrix}
										A_w - tI_k & C \\
										0               & D - tI_{n-k}
									\end{vmatrix} \nn
				&= \det (A_w - tI_k) \cdot \det (D - tI_{n-k}) \nn
				&= q(t) \cdot \det (D - tI_{n-k})
			\end{aligned}\]
			where $q(t)$ is the characterstic polynomial of $T_w$, the restriction of $T$ to $W$. 		
		\end{proof}
	
		\bigskip
		\labeledProposition{Let $T$ be a linear operator on a finite-dimensional vector space $V$, and let $W$ denote the $T$-cyclic subspace of $V$ generated by ${ \v \in V }$. Suppose that ${ \dim W = k \geq 1 }$ (and hence ${ \v \neq \0 }$). Then,
			\begin{enumerate}[label=(\roman*)]
				\item{${ \{ \v, T\v, T^2\,\v, \dots, T^{k-1}\,\v \} }$ is a basis for $W$.}
				\item{If 
					\[ T^k\,\v = -a_0\v - a_1 T\v - \cdots - a_{k-1} T^{k-1}\,\v, \]
					then the characteristic polynomial of $T_w$ is
					\[ p(t) = (-1)^k (t^k + a_{k-1} t^{k-1} + \cdots + a_1 t + a_0). \]
				}
			\end{enumerate}
		}{characteristic-polynomial-of-T-cyclic-subspace}
		\begin{proof}
			Let ${ B_x = \setc{ T^i\,\v }{ i \in \N{}, \; 0 \leq i \leq x } }$. Since $V$ is finite-dimensional, there exists some ${ m \in \N{} }$ such that
			\[ T^m\,\v \in \operatorname{span} B_{m-1}. \]
			Let ${ j \in \N{} }$ be the lowest such value so that ${ B_{j-1} = \setc{ T^i\,\v }{ i \in \N{}, \; 0 \leq i \leq j-1 } }$ is linearly independent. If we can show that $B_{j-1}$ spans $W$ then, by \autoref{theo:lin_ind_set_of_dim_cardinality_is_basis}, $B_{j-1}$ is a basis of $W$.\\
			We can show inductively that for any ${ m \in \N{} }$, ${ T^m\,\v \in \operatorname{span} B_{j-1} }$. Begin by observing that, clearly, ${ T^m\,\v }$ for any ${ 0 \leq m \leq j-1 }$ is a member of $B_{j-1}$ and is, therefore, trivially in the span. Then, for some ${ m > j-1 }$ we have that, by the induction hypothesis, $T^{m-1}\,\v$ is in the span of $B_{j-1}$ so,
			\[ T^{m-1}\,\v = a_0 \v + a_1 T\v + \cdots + a_{j-1} T^{j-1}\,\v \]
			for some ${ a_0, a_1, \dots, a_{j-1} \in \F{} }$. Then,
			\[\begin{aligned}
				T^m\,\v &= T(a_0 \v + a_1 T\v + \cdots + a_{j-1} T^{j-1}\,\v) \\
				&= a_0 T\v + a_1 T^2\,\v + \cdots + a_{j-1} T^j\,\v \\
			\end{aligned}\]
			which is in the span of $B_{j-1}$ because $T^j\,\v$ is in the span by construction. Therefore, $B_{j-1}$ is a basis of $W$ and it follows then that it must have length $k$ so that ${ j = k }$.
			
			\note{Note that we could also have attempted to prove this by invoking the division theorem (\autoref{theo:division-theorem}) to observe that, if ${ j \in \N{} }$ is the lowest natural number such that ${ T^j\,\v \in \operatorname{span} B_{j-1} }$, then, for any ${ m > j \in \N{} }$, there exist ${ q \in \Z{}, r \in \N{} }$ with ${ 0 \leq r < j }$, such that
				\[ m = qj + r \implies T^m\,\v = (T^j\,\v)^q \, (T^r\,\v). \]
				Here, $T^j\,\v$ is in the span of $B_{j-1}$ by hypothesis and $T^r\,\v$ is an element of $B_{j-1}$ and so, trivially, in the span. However, this still leaves the question of whether $(T^j\,\v)^q$ is in the span of $B_{j-1}$ -- which is precisely what we're trying to prove!
			}
			
			To build the characteristic polynomial of $T_w$, observe that the regular structure of the basis $B_{k-1}$ results in a regular structure of the matrix of the transformation \wrt this basis (remember that, by \autoref{prop:characterstic_poly_independent_of_basis_of_matrix}, the characterstic polynomial is the same regardless of the basis \wrt which we specify the matrix). If $A_w$ is the matrix of $T_w$ \wrt to the basis $B_{k-1}$ then (see: \ref{sssection:linear-trans-and-change-of-basis}),
			\[ A_w = \inv{[B_{k-1}]} [T(B_{k-1})] \]
			where $[B_{k-1}]$ denotes the matrix whose columns are the elements of $B_{k-1}$ and $[T(B_{k-1})]$ denotes the matrix whose columns are the elements of $B_{k-1}$ transformed by $T$. So, the columns of $[T(B_{k-1})]$ are ${ T\v, T^2\,\v, \dots, T^k\,\v }$ which, when expressed \wrt the basis $B_{k-1}$ are going to be,
			\[ (0,1,0,\dots,0), \, (0,0,1,0\dots,0), \dots, (-a_0, -a_1, \dots, -a_{k-1}). \]
			
			So, the matrix $A_w$ is going to take the form,
			\[ A_w = 	\begin{bmatrix}
							0 & 0 & \cdots & 0 & -a_0 \\
							1 & 0 & \cdots & 0 & -a_1 \\
							0 & 1 & \cdots & 0 & -a_2 \\
							\vdots & \vdots & &  & \vdots \\
							0 & 0 & \cdots & 0 & -a_{k-2} \\
							0 & 0 & \cdots & 1 & -a_{k-1} \\
						\end{bmatrix}.
			\]
			Note that $A_w$ is square ${ k \times k }$ which is as expected because $T_w$ is ${ W \longmapsto W }$.\\
			
			To show that this results in the desired characteristic polynomial we use induction on $k$, the dimension of $A_w$.\\
			
			For ${ k = 2 }$,
			\[\begin{aligned}
				A_w &= 	\begin{bmatrix}
							0 & -a_{k-2} \\
							1 & -a_{k-1}
						\end{bmatrix} \leadsto
						\begin{vmatrix}
							-t & -a_{k-2} \\
							1 & -a_{k-1} - t
						\end{vmatrix} \nn
				&= (-t)^k + (-t)^{k-1} (-a_{k-1}) + (-1)^{k-1} (-a_{k-2}) (1) \\
				&= (-1)^k (t^k + a_{k-1} t^{k-1} + a_0).
			\end{aligned}\]
			which has the desired form ${ (-1)^k (t^k + a_{k-1} t^{k-1} + \cdots + a_1 t + a_0) }$.\\
			
			For ${ k = n > 2 \in \N{} }$, we assume that matrices of size ${ (n-1) \times (n-1) }$ have the desired form of characteristic polynomial,
			\[ p_{n-1}(t) = (-1)^{n-1} (t^{n-1} + a_{n-2} t^{n-2} + \cdots + a_1 t + a_0). \]
			Then,
			\[\begin{aligned}
				\det A_w &= \begin{vmatrix}
								-t & 0 & \cdots & 0 & -a_0 \\
								1 & -t & \cdots & 0 & -a_1 \\
								0 & 1 & \cdots & 0 & -a_2 \\
								\vdots & \vdots & &  & \vdots \\
								0 & 0 & \cdots & -t & -a_{k-2} \\
								0 & 0 & \cdots & 1 & -a_{k-1} - t \\
							\end{vmatrix} \nn
				&= (-t) p_{n-1}(t) + (-1)^{k-1} (-a_0) (1) &\sidecomment{} \\
				&= (-t) [(-1)^{k-1} (t^{k-1} + a_{k-1} t^{k-1} + \cdots + a_2 t + a_1)] + (-1)^{k-1} (-a_0) &\sidecomment{} \\
				&= (-1)^k (t^k + a_{k-1} t^k + \cdots + a_2 t^2 + a_1 t) + (-1)^k a_0 &\sidecomment{} \\
				&= (-1)^k (t^k + a_{k-1} t^k + \cdots + a_2 t^2 + a_1 t + a_0). \qedhere
			\end{aligned}\]
		\end{proof}
	
	
		\biggerskip
		\labeledTheorem{\textbf{(Cayley-Hamilton Theorem.)} If $p(t)$ is the characteristic polynomial of \textbf{any} linear operator $T$ over a finite-dimensional vector space, then $p(T)$ is 0, the zero operator $T_0$.}{any-linear-operator-on-finite-vector-space-satisfies-its-characteristic-polynomial}
		\begin{proof}
			Let ${ T: V \longmapsto V }$ be a linear operator over a finite-dimensional vector space. Then for an arbitrary ${ \v \neq \0 \in V }$ there exists an associated $T$-cyclic subspace $W$ with basis ${ \{ \v, T\v, \dots, T^{k-1}\,\v \} }$ and we have
			\[ T^k\,\v = -a_0\v - a_1 T\v - \cdots - a_{k-1} T^{k-1}\,\v \]
			for some scalars ${ a_0,a_1,\dots,a_{k-1} }$.\\
			
			By \autoref{prop:characteristic-polynomial-of-T-cyclic-subspace} then, the characteristic polynomial of $W$ is
			\[ p_w(t) = (-1)^k (t^k + a_{k-1} t^{k-1} + \cdots + a_1 t + a_0) \]
			and then,
			\[ p_w(T)\v = (-1)^k (T^k\,\v + a_{k-1} T^{k-1}\,\v + \cdots + a_1 T\v + a_0\v) = \0. \]
			But also, \autoref{prop:char-poly-of-restriction-to-invariant-space-divides-char-poly-of-T} tells us that $p_w(t)$ divides $p(t)$, the characteristic polynomial of $T$. Therefore,
			\[ p(T)\v = (p_w(T) \cdot q(T))\v = (q(T) \cdot p_w(T))\v = q(T)(p_w(T)\v) = q(T)(\0) = \0. \]
			\note{Note the commutativity of the polynomial factors for polynomials in a single linear operator (see: \ref{defn:polynomials-of-linear-operators}).}
			In this way, $p(T)$ has been shown to map any arbitrary non-zero vector in the space to the zero vector and, since it is a linear operator (by the definition of a polynomial of a linear operator \ref{defn:polynomials-of-linear-operators}), it must also map the zero vector to the zero vector. It therefore follows that ${ p(T) = 0 = T_0 }$, the zero operator.
		\end{proof}
	
	
		\bigskip
		\labeledProposition{Let $T$ be a linear operator over a finite-dimensional vector space $V$ and let
			\[ V = W_1 \oplus W_2 \oplus \cdots \oplus W_n \]
			where each $W_i$, for ${ 1 \leq i \leq n }$, is a $T$-invariant subspace of $V$. Then, if $p(t)$ is the characteristic polynomial of $T$ while $p_i(t)$ is the characteristic polynomial of $T_{W_i}$ we have,
			\[ p(t) = p_1(t) p_2(t) \cdots p_n(t). \]
		}{}
		\begin{proof}
			This proof proceeds by induction on the number of invariant subspaces $n$ with base case ${ n = 2 }$.\\
			
			For ${ n = 2 }$, we have ${ W_1 \oplus W_2 = V }$ and if $B_1$ is a basis for $W_1$ and $B_2$ is a basis of $W_2$ then
			\[ B = B_1 \cup B_2 \]
			is a basis of $V$. If we form the matrix of $T$ \wrt $B$ then we get
			\[ 
				A = \begin{bmatrix}
						A_{W_1} & 0\\
						0 & A_{W_2}
					\end{bmatrix}.
			\]
			Let ${ k_1 = \dim W_1, k_2 = \dim W_2 }$, then
			\[\begin{aligned}
				&& \det (A - tI) &= \det (A - tI_{k_1}) \cdot \det (A - tI_{k_2}) \\
				&\iff & p(t) &= p_1(t) p_2(t).
			\end{aligned}\]
			So the proposition is proven for the base case ${ n = 2 }$.\\
			
			For the induction step, assume that the proposition holds for some ${ n - 1 \geq 2 \in \N{} }$. Then
			\[ V = W_1 \oplus W_2 \oplus \cdots \oplus W_n = (W_1 \oplus W_2 \oplus \cdots \oplus W_{n-1}) \oplus W_n. \]
			If we let
			\[ W' = W_1 \oplus W_2 \oplus \cdots \oplus W_{n-1} \]
			then $W'$ is a $T$-invariant subspace of $V$ whose characteristic polynomial, by the induction hypothesis, takes the form
			\[ p_{W'}(t) = p_1(t) p_2(t) \cdots p_{n-1}(t) \]
			but we also have 
			\[ W' \oplus W_n = V \]
			which, by the base case proof means that the characteristic polynomial of $T$ can be expressed as
			\[ p(t) = p_{W'}(t) p_n(t) = p_1(t) p_2(t) \cdots p_{n-1}(t) p_n(t). \]
		\end{proof}
	
			
		
		\sep
		\subsubsection{Examples of Diagonalization using the Characteristic Polynomial}
		\begin{exe}
			\ex{Let the real-valued matrix $A$ be,
				\[ A = \begin{bmatrix}
						4 & 0 & 4\\
						0 & 4 & 4\\
						4 & 4 & 8
						\end{bmatrix}.
				\]
				Constructing the characteristic polynomial,
				\begin{align*}
				\lvert{A - \lambda I}\rvert &= \begin{vmatrix}
													4 - \lambda & 0 & 4\\
													0 & 4 - \lambda & 4\\
													4 & 4 & 8 - \lambda
													\end{vmatrix} \\
				&= (4 - \lambda)\begin{vmatrix}4 - \lambda & 4\\4 & 8 - \lambda\end{vmatrix} + 
										4\begin{vmatrix}0 & 4 - \lambda\\4 & 4\end{vmatrix} \\
				&= (4 - \lambda)((4 - \lambda)(8 - \lambda) - 16 - 16)\\
				&= (4 - \lambda)(\lambda^2 - 12\lambda)\\
				&= (4 - \lambda)\lambda(\lambda - 12).
				\end{align*}
				So the eigenvalues are 4,0 and 12. To find an eigenvector for 4 we need to solve the equation ${ (A - 4I)\x = \0 }$ so we construct the matrix,
				\[ A - 4I = \begin{bmatrix}
							0 & 0 & 4\\
							0 & 0 & 4\\
							4 & 4 & 4
							\end{bmatrix}
				\]
				and then find the nullspace of this matrix using row reduction,
				
				\begin{align*}
				\begin{bmatrix}
				0 & 0 & 4\\
				0 & 0 & 4\\
				4 & 4 & 4
				\end{bmatrix} &=
				\begin{bmatrix}
				1 & 1 & 1\\
				0 & 0 & 1\\
				0 & 0 & 1
				\end{bmatrix} \\
				&= \begin{bmatrix}
				1 & 1 & 0\\
				0 & 0 & 1\\
				0 & 0 & 0
				\end{bmatrix}
				\end{align*}
				which gives ${ x_3 = 0 }$ and one free variable $x_2$. So
				\[ \x = \begin{bmatrix}-t \\ t \\ 0\end{bmatrix} = t\begin{bmatrix}-1 \\ 1 \\ 0\end{bmatrix} \hspace{30pt} t \neq 0 \in \R{}. \]
				\note{Note that ${ t \neq 0 }$ because eigenvectors are nonzero by definition.}
			}
			\biggerskip
			\ex{Let ${ T : \R{4} \longmapsto \R{4} }$ be defined as,
				\[ T(a,b,c,d) = (a + b + 2c - d, \, b + d, \, 2c - d, \, c + d). \]
				The subspace ${ W = \setc{(x,y,0,0)}{x,y \in \R{}} }$ is a $T$-invariant subspace of $\R{4}$ as,
				\[ T(x,y,0,0) = (x + y, \, y, \, 0, \, 0). \]
				Clearly, the standard basis of the $xy$-plane, ${ \{\e{1}, \e{2}\} }$ is a basis of $W$. This basis of $W$ can be extended to the standard basis of $\R{4}$ and the matrix of $T$ \wrt this basis is
				\[ A = 	\begin{bmatrix}
							1 & 1 & 2 & -1\\
							0 & 1 & 0 & 1 \\
							0 & 0 & 2 & -1\\
							0 & 0 & 1 & 1
						\end{bmatrix} 
				\]
				which contains the matrix for the restriction of $T$ to $W$ \wrt to the standard basis of the $xy$-plane in the top left corner,
				\[ 
					A_w =
					\begin{bmatrix}
						1 & 1 \\
						0 & 1
					\end{bmatrix}.
				\]
				So, we can see that the characteristic polynomial of $A_w$ divides that of $A$ as,
				\[ \det (A - tI_4) = \det (A_w - tI_2) \, \cdot \, 
														\begin{vmatrix}
															2 & -1\\
															1 & 1
														\end{vmatrix}.
				\]
			}
			\biggerskip
			\ex{Continuing the example \ref{ex:simple-T-cyclic-subspace}, we have ${ T: \R{3} \longmapsto \R{3} }$ defined by,
				\[ T(a, b, c) = (-b + c, \, a + c, \, 3c) \]
				and the $T$-cyclic subspace generated by $\e{1}$,
				\[ W = \operatorname{span} \{ \e{1}, \e{2} \} = \setc{(x,y,0)}{x,y \in \R{}}. \]
				The  restriction of $T$ to $W$ is
				\[ T_w = T(a, b) = (-b, a). \]
				If we form the matrix of $T_w$
				\[ A_w = [T_w(\{ \e{1}, \e{2} \})] = 
							\begin{bmatrix}
								0 & -1\\
								1 & 0
							\end{bmatrix},
				\]
				then the characteristic polynomial generated by the matrix method is
				\[
					\begin{vmatrix}
						-t & -1\\
						1 & -t
					\end{vmatrix} = t^2 + 1.
				\]
				Alternatively, we could apply \autoref{prop:characteristic-polynomial-of-T-cyclic-subspace} which tells us that, since in $W$ we have
				\[ T^2\,\e{1} = T(T\e{1}) = T\e{2} = -\e{1} \]
				then the characteristic polynomial of $T_w$ is
				\[ p(t) = (-1)^2 (t^2 + 1) = t^2 + 1. \]
			}
		\end{exe}
	
		
		\sep
		\subsubsection{Repeated Eigenvalues}
		\bigskip
		It's not every matrix that is diagonalizable because we can only form a diagonal matrix when there is a complete set of eigenvectors of the matrix that form a basis of the domain of the matrix. However, if we refer to the Fundamental Theorem of Algebra \autoref{theo:fundamental-theorem-of-algebra} we see that there will always be (when counted with multiplicity) $n$ roots of a degree-$n$ non-constant polynomial with complex coefficients. So, if we consider our matrix to be defined over the complex field, then we will always have, counted with multiplicity, $n$ roots of a characteristic polynomial of a ${ n \times n }$ matrix. The reason is that, when we don't have a complete eigenbasis, we have one or more roots of the characteristic polynomial with a multiplicity greater than one -- or, in other words, repeated roots.\\
		
		Consider, for example, the matrix
		\[
			A = \begin{bmatrix}
					a & 1\\
					0 & a
				\end{bmatrix} 
		\]
		that results in the characteristic polynomial
		\[ t^2 - 2at + a^2 = 0 \implies (t - a)(t - a) = (t - a)^2 = 0. \]
		
		This implies that we have a single eigenvalue ${ t = a }$, repeated twice. In this case, we cannot form an eigenbasis of the space and a diagonalized matrix of $A$. However, we can still find two eigenvectors, both of which will be in the eigenspace of the eigenvalue ${ t = a }$. The reasoning is as follows: If we find the eigenvector $\v_1$ for the eigenvalue in the usual way,
		\[ (A - aI)\v_1 = 
			\begin{bmatrix}
				0 & 1\\
				0 & 0
			\end{bmatrix}\v_1 = \0 \implies \v_1 = \begin{bmatrix}1\\ 0\end{bmatrix}. 
		\]
		But we also have a second eigenvector $\v_2$ such that,
		\[ (A - aI)^2 \,\v_2 = (A - aI)(A - aI)\v_2 = \0. \]
		We can leverage the first eigenvector to solve this:
		\[ (A - aI)\v_2 = \v_1 \implies (A - aI)[(A - aI)\v_2] = (A - aI)\v_1 = 0. \]
		So, if we can find a vector $\v_2$ such that,
		\[ (A - aI)\v_2 = \v_1 \iff A\v_2 = a\v_2 + \v_1 \]
		then $\v_2$ is also a kind of eigenvector with eigenvalue $a$. This kind of eigenvector is known as a \textit{generalized eigenvector}.\\
		
		In the example here it is easy to see that,
		\[
			\begin{bmatrix}
				0 & 1\\
				0 & 0
			\end{bmatrix}\v_2 = \begin{bmatrix}1\\ 0\end{bmatrix} \implies \v_2 = \begin{bmatrix}0\\ 1\end{bmatrix}
		\]
		and together, $\v_1$ and $\v_2$ form a basis of the two-dimensional domain of the matrix $A$. In fact,
		\[
			A = \begin{bmatrix}
					a & 1\\
					0 & a
				\end{bmatrix} = 
				\begin{bmatrix}
					0 & 1\\
					1 & 0
				\end{bmatrix} 
				\begin{bmatrix}
					a & 0\\
					1 & a
				\end{bmatrix}
				\begin{bmatrix}
					0 & 1\\
					1 & 0
				\end{bmatrix} 
		\]
		or, alternatively,
		\[
			A = \begin{bmatrix}
					a & 1\\
					0 & a
				\end{bmatrix} = 
				\begin{bmatrix}
					1 & 0\\
					0 & 1
				\end{bmatrix} 
				\begin{bmatrix}
					a & 1\\
					0 & a
				\end{bmatrix}
				\begin{bmatrix}
					1 & 0\\
					0 & 1
				\end{bmatrix}.
		\]
		
		\biggerskip
		\labeledProposition{The multiplicities of the eigenvalues of a linear operator sum to the dimension of the vector space over which it is defined.}{sum-of-eigenvalue-multiplicities-is-dimension-of-space}
		\begin{proof}
			It is easy to see that, if the characteristic polynomial of a linear operator factorizes to
			\[ p(t) = (t - \lambda_1)^{m_1}(t - \lambda_2)^{m_2}\cdots(t - \lambda_k)^{m_k} \]
			then the highest power of $t$ is ${ t^{m_1 + m_2 + \cdots + m_k} }$ and so the degree of the polynomial is ${ \sum_i m_i }$, the sum of the multiplicities of the eigenvalues.\\
			Furthermore, it is also clear from the definition of the characteristic polynomial (\ref{def:characteristic-polynomial}) that -- since the variable $t$ only appears in the diagonal elements of the matrix of the operator -- the degree of the characteristic polynomial is equal to the number of diagonal elements in the matrix; that's to say, the dimension of the space.
		\end{proof}
		
		\bigskip
		\labeledProposition{The dimension of an eigenspace is less than or equal to the multiplicity of the corresponding eigenvalue.}{eigenspace-dimension-leq-multiplicity-of-eigenvalue}
		\begin{proof}
			Let $T$ be a linear operator over a finite-dimensional vector space $V$ of dimension $n$ and let $\lambda$ be an eigenvalue of $T$. Further, let $E_\lambda$ be the eigenspace corresponding to the eigenvalue $\lambda$ and let ${ d = \dim E_\lambda }$ and $m$ be the multiplicity of $\lambda$.\\
			Then, there exists a $d$-length basis for $E_\lambda$. Let this basis be ${ \{ \x_1, \dots, \x_d \} }$ and extend it to a basis of $V$,
			\[ B = \{ \x_1, \dots, \x_d, \x_{d + 1}, \dots, \x_n \}. \]
			If we construct the matrix of $T$ \wrt to this basis of $V$, we see that the block form of the matrix comes out,
			\[
				[T]_B = [T(B)] = 	\begin{bmatrix}
										\lambda I_d & A\\
										0 & C
									\end{bmatrix}
			\]
			so that the characteristic polynomial of $T$ (which is the same against any basis by \autoref{prop:characterstic_poly_independent_of_basis_of_matrix}),
			\[\begin{aligned}
				p(t) &= \det ((\lambda - t)I_d) \, \cdot \, \det (C - t I_{n-d}) \\
				&= (t - \lambda)^d \, \cdot \, \det (C - t I_{n-d}).
			\end{aligned}\]
			Therefore, the multiplicity of $\lambda$ is at least $d$ and so we have ${ m \geq d }$ as required.
		\end{proof}
	
		\bigskip
		\labeledProposition{A linear operator over a finite-dimensional vector space such that the characteristic polynomial factorizes, is diagonalizable iff for each eigenvalue, the dimension of the eigenspace is equal to the multiplicity of the eigenvalue.}{lin-operator-diagonalizable-iff-all-eigenspace-dimensions-equal-to-multiplicity-of-eigenvalue}
		\begin{proof}
			Let $T$ be a linear operator over a finite-dimensional vector space $V$ of dimension $n$ such that the characteristic polynomial of $T$ contains the eigenvalue $\lambda$ with multiplicity $m$.\\
			
			\subheading{Matrix-based proof}
			Assume $T$ is diagonalizable so that there exists a basis such that the matrix of $T$ \wrt to this basis, is diagonal. Since the eigenvalue $\lambda$ in the characteristic polynomial has multiplicity $m$ then there must $m$ columns of the matrix that have $\lambda$ as the diagonal element. Therefore the basis vectors that are eigenvectors of $\lambda$ are the vectors corresponding to these $m$ columns. The eigenspace of $\lambda$ is therefore $m$-dimensional.\\
			
			Conversely, assume that, for every eigenvalue $\lambda$ with multiplicity $m$, the dimension of the eigenspace of $\lambda$ is $m$. Then the number of columns with the sole non-zero element being $\lambda$ in the diagonal position is $m$. Since this applies to every eigenvalue, the total number of columns of the matrix with the only non-zero element being the eigenvalue in the diagonal position is the sum of the multiplicities which, by the definition of the characteristic polynomial, is the number of columns in the matrix and hence, the dimension of $V$.\\
			
			\subheading{Non matrix-based proof}
			Suppose that $T$ is diagonalizable and let $B$ be a basis of $V$ consisting of eigenvectors of $T$. Let the distinct eigenvalues of $T$ be ${ \setc{\lambda_i}{1 \leq i \leq k} }$ and let ${ B_i = B \cap E_{\lambda_i} }$ be the subset of the basis $B$ consisting of eigenvectors of the eigenvalue $\lambda_i$.\\
			If we further let ${ n_i = \cardinality{B_i} }$ be the number of eigenvectors of $\lambda_i$ in the basis $B$ then, because $B_i$ is a linearly independent set in $E_{\lambda_i}$, it follows that
			\[ \dim E_{\lambda_i} \geq n_i. \]
			Also, by \autoref{prop:eigenspace-dimension-leq-multiplicity-of-eigenvalue}, if $m_i$ is the multiplicity of $\lambda_i$, then
			\[ \dim E_{\lambda_i} \leq m_i. \]
			Since $B$ is a basis for $V$,
			\[ \dim V = n \implies \cardinality{B} = n = \sum_i n_i \]
			and also, by \autoref{prop:sum-of-eigenvalue-multiplicities-is-dimension-of-space},
			\[ \sum_i m_i = \dim V = n. \]
			Thus we have,
			\[ n = \sum_i n_i \leq \sum_i \dim E_{\lambda_i} \leq \sum_i m_i = n. \]
			We can therefore conclude that
			\[ \sum_i \dim E_{\lambda_i} = n. \]
			Furthermore, if there were any eigenspace $E_{\lambda_i}$ such that ${ \dim E_{\lambda_i} < m_i }$ then there would have to be another such that the dimension is greater than its multiplicity so that the sum of all the dimensions of the eigenspaces equals the sum of the multiplicities. But, by \autoref{prop:eigenspace-dimension-leq-multiplicity-of-eigenvalue}, there cannot be an eigenspace with dimension greater than its multiplicity. That's to say,
			\[\begin{aligned}
				&& \left( \forall i \logicsep (m_i - \dim E_{\lambda_i}) \geq 0 \right) \, &\land \, \left( \sum_i (m_i - \dim E_{\lambda_i}) = 0 \right) \\
				&\implies & \forall i \logicsep (m_i - \dim E_{\lambda_i}) &= 0. &\sidecomment{} \\
			\end{aligned}\]			
			Therefore, we conclude that,
			\[ \forall i \logicsep \dim E_{\lambda_i} = m_i. \]
			
			\nl
			Conversely, suppose that ${ \forall i \logicsep \dim E_{\lambda_i} = m_i }$. Then, for each eigenspace $E_{\lambda_i}$, there exists a basis $B_i$ such that ${ \cardinality{B_i} = m_i }$. Furthermore, since the eigenspaces are linearly independent (\autoref{prop:distinct-eigenvals-means-independent-eigenvecs}), they form a direct sum
			\[ W = E_{\lambda_1} \oplus E_{\lambda_2} \oplus \cdots \oplus E_{\lambda_k} \]
			where ${ \dim W = \sum_i m_i = n }$.\\
			Therefore, the union of the bases
			\[ B = \bigcup_i B_i \]
			is a linearly independent set of length $n$ and is, therefore, a basis for $V$ consisting of eigenvectors of $T$. It follows, by the definition, that $T$ is diagonalizable.
		\end{proof}

		

% -------------------- break -------------------


		
		\pagebreak
		\subsubsection{Generalized Eigenvectors}\label{sssection:generalized-eigenvectors}
		\bigskip
		\boxeddefinition{The \textbf{algebraic multiplicity} of an eigenvalue is the number of times that it appears as a root of the characteristic polynomial.\\
		
			The \textbf{geometric multiplicity} of an eigenvalue is the dimension of its associated eigenspace.\\
			
			An eigenvalue is said to be \textbf{defective} if its geometric multiplicity is less than its algebraic multiplicity.
		}
		\boxeddefinition{A \textbf{generalized eigenvector of rank (or index) $\bm{r}$} of a matrix $A$ corresponding to an eigenvalue $\lambda$ is defined as a vector $\v$ such that,
			\[ (A - \lambda I)^r \, \v = \0 \eqand (A - \lambda I)^{r-1} \, \v \neq \0. \]
			
			\nl[3]
			An eigenvector is a particular case of a generalized eigenvector with rank 1 as,
			\[ (A - \lambda I)^1 \, \v = \0 \iff (A - \lambda I) \, \v = \0 \]
			and we have the restriction that an eigenvector cannot be $\0$ so also,
			\[ (A - \lambda I)^0 \, \v = \v \neq \0. \]
		}	
		\boxeddefinition{The generalized eigenspace of an eigenvalue $\lambda$ of a linear operator $T$ over a vector space $V$ is the set
			\[ K_\lambda = \setc{\v \in V}{\exists k \in \N{} \suchthat (T- \lambda I)^k \, \v = \0}. \]
		}\label{def:generalized-eigenspace}
	
		\bigskip
		\labeledProposition{The generalized eigenspace of an eigenvalue $\lambda$ of a linear operator $T$ over a vector space $V$ is a $T$-invariant subspace of $V$ that contains the eigenspace of $\lambda$.}{generalized-eigenspace-is-T-invariant-subspace}
		\begin{proof}
			Let $K_\lambda$ be the generalized eigenspace of an eigenvalue $\lambda$. $K_\lambda$ is a subspace of the vector space $V$ because:
			\begin{itemize}
				\item{It contains the zero vector because ${ (T- \lambda I)^k \, \0 = \0 }$ for any ${ k \in \N{} }$;}
				\item{If $\v_1,\v_2$ are members of $K_\lambda$ such that
					\[ (T- \lambda I)^p \, \v_1 = \0 = (T- \lambda I)^q \, \v_2 \]
					for some ${ p,q \in \N{} }$, then
					\[ (T- \lambda I)^{p+q} \, (\v_1 + \v_2) = (T- \lambda I)^q \, (\0) + (T- \lambda I)^p \, (\0) = \0. \]
				}
			\end{itemize}
			$K_\lambda$ is $T$-invariant because if $\v$ is a member of $K_\lambda$ with index $p$ then,
			\[ (T- \lambda I)^p \, \v = \0 \iff (T- \lambda I)^p \, T\v = T[(T- \lambda I)^p \, \v] = T\0 = \0. \]
			Therefore ${ \v \in K_\lambda \implies T\v \in K_\lambda }$.\\
			
			Finally, $K_\lambda$ clearly contains the eigenspace of $\lambda$ as this is just
			\[ \setc{\v \in V}{(T- \lambda I)^k \, \v = \0} \]
			where ${ k = 1 }$.
		\end{proof}
	
		\bigskip
		\labeledProposition{If $T$ is a linear operator on a vector space $V$ with eigenvalue $\lambda$ then $T$-invariance is equivalent to ${ (T - \lambda I) }$-invariance.}{T-invariance-is-equiv-to-T-minus-lambda-invariance}
		\begin{proof}
			Let $W$ be a $T$-invariant subspace of $V$. Then, for any ${ \w \in W }$,
			\[ T\w \in W, \; \w \in W \implies T\w - \lambda \w = (T - \lambda I)\w \in W.  \]
			Conversely, if $W$ is a $(T - \lambda I)$-invariant subspace of $V$, then, for any ${ \w \in W }$,
			\[ (T - \lambda I)\w \in W, \; \w \in W \implies T\w - \lambda \w + \lambda \w = T\w \in W. \]
		\end{proof}
	
		\bigskip
		\labeledProposition{If $T$ is a linear operator on a vector space $V$ and $p(t)$ is a polynomial with coefficients in the same field as $V$, then $T$-invariance is equivalent to $p(T)$-invariance.}{T-invariance-is-equiv-to-any-polynomial-invariance}
		\begin{proof}
			\TODO{This is easily shown by expanding the polynomial to monomial terms that are a linear combination of the basis vectors of a $T$-cyclic space.}
		\end{proof}
	
		\bigskip
		\labeledProposition{If a vector $\v_r$ is a generalized eigenvector of rank $r$ corresponding to the eigenvalue $\lambda$ of the linear transformation $T$ then,
			\[ \v = (T - \lambda I)^{r-1} \, \v_r \]
			is an eigenvector of $T$ with eigenvalue $\lambda$.
		}{gen-eigenvec-chain-initial-vector-is-eigenvector}
		\begin{proof}
			By the definition of a generalized eigenvector of rank $r$ we have,
			\[ (T - \lambda I)^r \, \v_r = \0 \iff (T - \lambda I)(T - \lambda I)^{r-1} \, \v_r = \0 \]
			but also
			\[ (T - \lambda I)^{r-1} \, \v_r \neq \0 \]
			which also implies that
			\[ (T - \lambda I)^m \, \v_r \neq \0 \]
			for any ${ m < r-1 }$. It therefore follows that also,
			\[ (T - \lambda I) \, \v_r \neq \0. \]
			So we must have,
			\[ (T - \lambda I)^{r-1} \, \v_r = \v \suchthat (T - \lambda I)\v = \0. \]
			Therefore ${ \v = (T - \lambda I)^{r-1} \, \v_r }$ is an eigenvector of the transformation $T$.
		\end{proof}
	
		\bigskip
		\labeledProposition{The generalized eigenspaces of distinct eigenvalues are linearly independent spaces.}{distinct-eigenvalue-gen-eigenspaces-are-lin-independent}
		\begin{proof}
			Let $T$ be a linear operator on a (not necessarily finite) vector space defined over a field $\F{}$, and let ${ \setc{\lambda_i}{1 \leq i \leq k} }$ be a (not necessarily exhaustive) set of distinct eigenvalues of $T$. Let $K_{\lambda_i}$ be the generalized eigenspace corresponding to the eigenvalue $\lambda_i$ and let ${ \v_i \in K_{\lambda_i} }$ be an arbitrary vector in the generalized eigenspace of $\lambda_i$. The proposition is proven if it is shown that
			\[ \v_1 + \v_2 + \cdots + \v_k = \0 \implies \v_1, \v_2, \dots, \v_k = \0. \]
			
			Following a proof by induction on the number of distinct eigenvalues $k$, if we take ${ k = 1 }$ as the base case, the proposition holds trivially as
			\[ \v_1 = \0 \implies \v_1 = \0. \]
			
			\nl[2]
			For the induction step, assume that the proposition holds for some ${ k - 1 \geq 1 \in \N{} }$. Now identify two possible cases: ${ \v_k = \0 }$ and ${ \v_k \neq \0 }$.\\
			
			\nl[2]
			Assume ${ \v_k = \0 }$. Then we have,
			\[ \v_1 + \v_2 + \cdots + \v_{k-1} + \v_k = \v_1 + \v_2 + \cdots + \v_{k-1} = \0 \]
			which satisfies the proposition by the induction hypothesis so that
			\[ \v_1 + \v_2 + \cdots + \v_{k-1} = \0 \implies \v_1, \v_2, \dots, \v_{k-1} = \0. \]
			Since, also, ${ \v_k = \0 }$ by assumption, the proposition holds.\\
			
			\nl[3]
			Conversely, assume that ${ \v_k \neq \0 }$ and that we have a linear relation so that,
			\[ \v_1 + \v_2 + \cdots + \v_k = \0 \implies \v_k = -(\v_1 + \v_2 + \cdots + \v_{k-1}). \]
			Let ${ r_i \in \N{} }$ be the rank of each $\v_i$ so that,
			\[ (T - \lambda_i I)^{r_i} \, \v_i = \0 \]
			and, using \autoref{prop:gen-eigenvec-chain-initial-vector-is-eigenvector}, define the eigenvectors $\x_i$,
			\[ \x_i = (T - \lambda_i I)^{r_i - 1} \, \v_i. \]

			The linear relation implies,
			\[\begin{aligned}
				&& \v_1 + \v_2 + \cdots + \v_k &= \0 \\
				&\implies & (T - \lambda_1 I)^{r_1} \, \v_1 + (T - \lambda_1 I)^{r_1} \, \v_2 + \cdots + (T - \lambda_1 I)^{r_1} \, \v_k &= \0 \\
				&\implies & \0 + (T - \lambda_1 I)^{r_1} \, \v_2 + \cdots + (T - \lambda_1 I)^{r_1} \, \v_k &= \0 \\
				&\implies & (T - \lambda_1 I)^{r_1} \, (T - \lambda_2 I)^{r_2} \, \v_2 + \cdots + (T - \lambda_1 I)^{r_1} \, (T - \lambda_2 I)^{r_2} \, \v_k &= \0 \\
				&\implies & (T - \lambda_1 I)^{r_1} \, (T - \lambda_2 I)^{r_2} \, \cdots (T - \lambda_k I)^{r_k} \, \v_k &= \0. \\
			\end{aligned}\]
		
			If we define the polynomial with coefficients in $\F{}$,
			\[ p(t) = (t - \lambda_1)^{r_1}(t - \lambda_2)^{r_2}\cdots(t - \lambda_{k-1})^{r_{k-1}} \]
			then, using \ref{defn:polynomials-of-linear-operators}, the linear relation implies that
			\[ (T - \lambda_1)^{r_1} \, (T - \lambda_2)^{r_2} \, \cdots (T - \lambda_k)^{r_k} \, \v_k = \0 \implies p(T)\v_k = \0. \]
			
			However, we also have the eigenvector $\x_k$,
			\[ \x_k = (T - \lambda_k I)^{r_k - 1} \v_k \]
			and by \autoref{prop:polynomial-of-linear-map-times-eigenvec-is-polynomial-of-eigenval-times-eigenvec}, 
			\[ p(T)\x_k = (\lambda_k - \lambda_1)(\lambda_k - \lambda_2)\cdots(\lambda_k - \lambda_{k-1}) \neq \0. \]
			And so, for the eigenvector $\x_k$,
			\[\begin{aligned}
				&& p(T)\x_k = p(T)(T - \lambda_k I)^{r_k - 1} \, \v_k &\neq \0 \\
				&\iff & (T - \lambda_k I)^{r_k - 1} \, p(T)\v_k &\neq \0 &\sidecomment{commutativity by \ref{defn:polynomials-of-linear-operators}} \\
				&\implies & p(T)\v_k &\neq \0 \\
			\end{aligned}\]
			which contradicts the implications of the linear relation.
		\end{proof}
	
	
		\bigskip
		\labeledProposition{Let $T$ be a linear operator on a finite-dimensional vector space $V$ with distinct eigenvalues ${ \setc{\lambda_i}{1 \leq i \leq k} }$. For each $i$, let $S_i$ be a linearly independent subset of the generalized eigenspace $K_{\lambda_i}$. Then,
			\[ S_i \cap S_j = \emptyset \eqword{for} i \neq j \]
			and ${ S = S_1 \cup S_2 \cup \cdots \cup S_k }$ is a linearly independent subset of $V$.
		}{union-of-lin-ind-subsets-of-gen-eigenspaces-is-lin-ind}
		\begin{proof}
			Suppose that ${ \x \in S_{i} \cap S_{j} }$ for some ${ i \neq j }$. Let ${ \y = -\x }$. Then ${ \x \in K_{\lambda_i}, \, \y \in K_{\lambda_j} }$, and ${ \x + \y = \0 }$. By \autoref{prop:distinct-eigenvalue-gen-eigenspaces-are-lin-independent}, ${ \x = \0 }$, contrary to the fact that $\x$ lies in a linearly independent set. Thus ${ S_i \cap S_j = \emptyset }$.\\
			
			Now suppose that for each $i$
			\[ S_i = \{ \x_{i 1}, \x_{i 2}, \dots, \x_{i n_i} \}. \]
			Then 
			\[ S = \setc{ \x_{i j} }{ 1 \leq j \leq n_i, 1 \leq i \leq k }. \] 
			Consider any scalars $a_{i j}$ such that
			\[ \sum_{i=1}^k \sum_{j=1}^{n_i} a_{i j} \x_{i j} = \0. \]
			For each $i$ let
			\[ \y_i = \sum_{j=1}^{n_i} a_{i j} \x_{i j}. \]
			Then ${ \y_i \in K_{\lambda_i} }$ for each $i$ and ${ \y_1 + \y_2 + \cdots + \y_k = \0 }$. Therefore, by \autoref{prop:distinct-eigenvalue-gen-eigenspaces-are-lin-independent}, ${ \y_i = \0 }$ for all $i$. But for all $i$, $S_i$ is linearly independent by hypothesis. Thus, for each $i$, it follows that ${ a_{i j} = 0 }$ for all $j$. We conclude that $S$ is linearly independent.
		\end{proof}
	
		
	
		\biggerskip
		\group{
		\subsubsubsection{Chains/Cycles of Generalized Eigenvectors}
		\boxeddefinition{Let $T$ be a linear transformation of which $\lambda$ is an eigenvalue and $\v$ is a generalized eigenvector of rank $r$. Then the set
			\[ \{\, (T - \lambda I)^{r-1} \, \v, \, (T - \lambda I)^{r-2} \, \v, \, \dots, \, \v \,\} \]
		is known as a \textbf{chain} or \textbf{cycle} of generalized eigenvectors of length $r$. The elements $(T - \lambda I)^{r-1} \, \v$ and $\v$ are known as the \textbf{initial~vector} and \textbf{end~vector} respectively of the chain or cycle.
		}}
	
		\note{Note that a chain or cycle of generalized eigenvectors is a subset of a generalized eigenspace. There may be more than one chain/cycle in a given generalized eigenspace.}
	
		\biggerskip
		\labeledTheorem{The generalized eigenvectors in a chain are linearly independent.}{gen-eigenvecs-in-a-chain-are-linearly-independent}
		\begin{proof}
			Prove by induction on the length of the chain. 
			
			\subheading{Base case: length ${ \bm{= 2} }$}
			A chain of length 2 corresponding to the eigenvector $\lambda$ of a linear transformation $T$, has the form
			\[ C_2 = \{ (T - \lambda I) \v, \, \v \} \]
			where $\v$ is a generalized eigenvector of rank 2. Then also, by the definition of the generalized eigenvector of rank 2,
			\[ (T - \lambda I)^2 \, \v = \0 \eqand (T - \lambda I) \v \neq \0. \]
			
			Assume, for contradiction, that the chain is not linearly independent. Then there exists a linear relation between the elements of the chain,
			\[ \alpha_1 (T - \lambda I) \, \v + \alpha_2 \v = \0 \]
			where ${ \alpha_1, \alpha_2 \neq 0 }$ are constant scalars. This further implies, defining ${ \alpha = -\frac{\alpha_1}{\alpha_2} }$, that
			\[ \v = \alpha (T - \lambda I) \, \v \implies (T - \lambda I) \v = \alpha (T - \lambda I)^2 \, \v = \0. \]
			But ${ (T - \lambda I) \v = \0 }$ contradicts the definition of $\v$ as a generalized eigenvector of rank 2. It follows then, that a chain of length 2 is linearly independent.
			
			\subheading{Induction step: length ${ \bm{= k} }$}
			A chain of length $k$ corresponding to the eigenvector $\lambda$ of a linear transformation $T$, has the form
			\[ C_k = \{ (T - \lambda I)^{k-1} \, \v, \, (T - \lambda I)^{k-2} \, \v, \, \dots, \, (T - \lambda I) \v, \, \v \} \]
			where $\v$ is a generalized eigenvector of rank $k$. Then also, by the definition of the generalized eigenvector of rank $k$,
			\[ (T - \lambda I)^k \, \v = \0 \eqand (T - \lambda I)^{k-1} \, \v \neq \0. \]
			It follows from the fact that $\v$ is a generalized eigenvector of rank $k$ that ${ \w = (T - \lambda I) \v }$ is a generalized eigenvector of rank ${ k - 1 }$ because,
			\[ (T - \lambda I)^{k-1} \, \w = (T - \lambda I)^k \, \v = \0 \eqand (T - \lambda I)^{k-2} \, \w = (T - \lambda I)^{k-1} \, \v \neq \0. \]
			Therefore
			\[ C_{k-1} = \{ (T - \lambda I)^{k-1} \, \v, \, (T - \lambda I)^{k-2} \, \v, \, \dots, \, (T - \lambda I) \v \} \]
			is a $(k-1)$-length chain and is, by the induction hypothesis, linearly independent.\\
			
			It follows that $C_k$ will be linearly independent iff $\v$ is not in the span of $C_{k-1}$. Assume for contradiction, that $\v$ is indeed in the span of $C_{k-1}$. Then $\v$ can be expressed as a linear combination of the elements of $C_{k-1}$,
			\[ \v = \alpha_1 (T - \lambda I)^{k-1} \, \v + \alpha_2 (T - \lambda I)^{k-2} \, \v + \cdots + \alpha_{k-1} (T - \lambda I) \v. \]
			But this implies that,
			\[\begin{aligned}
				(T - \lambda I)^{k-1} \, \v &= \alpha_1 (T - \lambda I)^{k-2} ((T - \lambda I)^k \, \v) \\
				&\hspace{14pt} + \alpha_2 (T - \lambda I)^{k-3} ((T - \lambda I)^k \, \v) + \cdots + \alpha_{k-1} ((T - \lambda I)^k \, \v) \nn
				&= \alpha_1 (T - \lambda I)^{k-2} \, (\0) + \alpha_2 (T - \lambda I)^{k-3} (\0) + \cdots + \alpha_{k-1} (\0) \nn
				&= \0.
			\end{aligned}\]
			Since ${ (T - \lambda I)^{k-1} \, \v = \0 }$ contradicts the definition of $\v$ as a generalized eigenvector of rank $k$, we can deduce that $\v$ cannot be in the span of $C_{k-1}$.
		\end{proof}
	
		\bigskip
		\labeledTheorem{The generalized eigenvectors in a chain form the basis of a $T$-invariant space.}{chain-of-gen-eigenvecs-is-basis-of-invariant-space}
		\begin{proof}
			Let $C_k$ be a chain/cycle of length $k$ corresponding to the eigenvector $\lambda$ of a linear transformation $T$ so that,
			\[ C_k = \{ (T - \lambda I)^{k-1} \, \v, \, (T - \lambda I)^{k-2} \, \v, \, \dots, \, (T - \lambda I) \v, \, \v \}. \]
			By \autoref{theo:gen-eigenvecs-in-a-chain-are-linearly-independent} we have that $C_k$ is linearly independent so the proposition will be proven if we can show that the space spanned by $C_k$ is $T$-invariant.\\
			
			\note{Note that it may be tempting to use the proof used in \autoref{prop:generalized-eigenspace-is-T-invariant-subspace}:
				For any ${ \x \in C_k }$ we have some ${ m \leq k \in \N{} }$ such that
				\[ (T - \lambda I)^m\,\x = \0. \]
				Therefore, for any ${ n \in \N{} }$,
				\[ (T - \lambda I)^m \, T^n \, \x = T^n \, (T - \lambda I)^m \, \x = T^n \, (\0) = \0. \]
				But this only proves that $T^n \,\x$ is a generalized eigenvector with eigenvalue $\lambda$, it does \textbf{not} prove that it is in the span of the chain/cycle $C_k$. Remember, there may be more than one chain/cycle in a generalized eigenspace.
			}
			
			\[\begin{aligned}
				T\v &= (T - \lambda I) \v + \lambda \v, \\
				T(T - \lambda I) \v &= (T - \lambda I)^2 \, \v + \lambda T\v - \lambda^2 \v \\
									&= (T - \lambda I)^2 \, \v + \lambda (T - \lambda I) \v. \\
			\end{aligned}\]
			For any ${ m \in \N{} }$ such that ${ 1 \leq m \leq k-1 }$,
			\[\begin{aligned}
				T(T - \lambda I)^m\,\v &= (T - \lambda I)(T - \lambda I)^m\,\v + \lambda (T - \lambda I)^m\,\v \\
				&= (T - \lambda I)^{m+1}\,\v + \lambda (T - \lambda I)^m\,\v.\\
			\end{aligned}\]
			When ${ m = k-1 }$ we have the special case that,
			\[\begin{aligned}
				T(T - \lambda I)^{k-1}\,\v &= (T - \lambda I)^k\,\v + \lambda (T - \lambda I)^{k-1}\,\v \\
				&= \0 + \lambda (T - \lambda I)^{k-1}\,\v \\
			\end{aligned}\]
			which reflects the fact that ${ (T - \lambda I)^{k-1}\,\v }$ is an eigenvector.\\
			
			So, we have shown that $T\x$ for any ${ \x \in C_k }$ is in the span of $C_k$. For any ${ n > 1 \in \N{} }$,
			\[\begin{aligned}
				&& T^n\,(T - \lambda I)^m\,\v &= (T - \lambda I)T^{n-1}\,(T - \lambda I)^m\,\v + \lambda (T - \lambda I)^m\,\v \\
				&\iff &  &= T^{n-1}\,(T - \lambda I)^{m+1}\,\v + \lambda (T - \lambda I)^m\,\v &\sidecomment{} \\
			\end{aligned}\]
			In the case that ${ m + 1 = k }$,
			\[ T^{n-1}\,(T - \lambda I)^{m+1}\,\v + \lambda (T - \lambda I)^m\,\v = \0 + \lambda (T - \lambda I)^m\,\v \]
			which is clearly in the span of $C_k$. Otherwise, 
			\[\begin{aligned}
				&& T^{n-1}\,(T - \lambda I)^{m+1}\,\v + \lambda (T - \lambda I)^m\,\v &\in \operatorname{span} C_k \\
				&\iff &  T^{n-1}\,(T - \lambda I)^{m+1}\,\v &\in \operatorname{span} C_k. &\sidecomment{} \\
			\end{aligned}\]
			Therefore, by induction, the space spanned by $C_k$ is shown to be $T$-invariant.
		\end{proof}

		
		\bigskip
		\labeledProposition{Let $T$ be a linear operator on a finite-dimensional vector space $V$ and let $Z_i$ for ${ 1 \leq i \leq q }$ be cycles of generalized eigenvectors of $T$ corresponding to a single common eigenvalue $\lambda$.\\
		If $\y_i$ is the initial vector in cycle $i$ then, if the set ${ \setc{ \y_i }{ 1 \leq i \leq q } }$ is linearly independent then the sets $Z_i$ are disjoint and their union
			\[ Z = \bigcup_{i=1}^q Z_i \]
		is linearly independent.
		}{for-given-eigenvalue-the-union-of-cycles-of-linearly-independent-eigenvectors-is-a-linearly-independent-set}
		\begin{proof}
			To show that the cycles $Z_i$ are disjoint, assume for contradiction that, for ${ i \neq j }$,
			\[\begin{aligned}
				&& &\exists \x \in Z_i \cap Z_j \\
				&\implies & &\exists r_i,r_j \in \N{} \logicsep [ \y_i = (T - \lambda I)^{r_i - 1}\,\x ] \land [ \y_j = (T - \lambda I)^{r_j - 1}\,\x ].
			\end{aligned}\]
			If we now assume \WLOG that ${ r_j \geq r_i }$ so that ${ r = r_j - r_i \geq 0 }$, we have
			\[ \y_j = (T - \lambda I)^r\,\y_i = (T - \lambda I)^{r-1}\,(T - \lambda I) \y_i = \0 \]
			where ${ (T - \lambda I) \y_i = \0 }$ because $\y_i$ is an eigenvector by hypothesis. But $\y_j$ is also an eigenvector by hypothesis and therefore, by definition, cannot be $\0$. It therefore follows that
			\[ \centernot\exists  \x \in Z_i \cap Z_j \implies Z_i \cap Z_j = \emptyset \]
			and the cycles $Z_i$ are disjoint.\\
			
			To show that $Z$ is linearly independent, we will use induction on ${ n = \cardinality{Z} }$, the cardinality of the set $Z$. If ${ n = 1 }$ then the proposition holds trivially because the cycles, by definition, contain only non-zero vectors.\\
			
			Assume that, for some ${ n > 1 }$, the proposition holds for any number less than $n$. We will show that $Z$ is a basis of $W$, the space that it generates. Since $W$ is clearly $(T - \lambda I)$-invariant, we can define the restriction of $(T - \lambda I)$ to $W$ and denote it $(T - \lambda I)_W$.\\
			
			Firstly, note that the image under $(T - \lambda I)$ of $Z_i$,
			is equal to $Z_i$ but with the end vector swapped for $\0$. So, if we define,
			\[ Z_i' = (T - \lambda I) Z_i \setminus \{\0\} \]
			then their union
			\[ Z' = \bigcup_{i=1}^q Z_i' \]
			contains all the non-zero images under $(T - \lambda I)$ of the vectors in $Z$ and therefore spans the range of $(T - \lambda I)_W$. Furthermore, this is also a disjoint union because, for each $i$, ${ Z_i' \subset Z_i }$ and the sets $Z_i$ are disjoint. So we can deduce that
			\[\begin{aligned}
				\cardinality{Z'} &= \sum_{i=1}^q \cardinality{Z_i'} \nn
				&= \sum_{i=1}^q (\cardinality{Z_i} - 1) &\sidecomment{} \nn
				&= \left( \sum_{i=1}^q \cardinality{Z_i} \right) - q \nn
				&= \cardinality{Z} - q \\
				&= n - q.
			\end{aligned}\]
			Since the cardinality of $Z'$ is less than $n$ and it consists of cycles of generalized eigenvectors with the same set of linearly independent eigenvector initial vectors as $Z$, we can use the induction hypothesis to deduce that $Z'$ is linearly independent. Since $Z'$ spans the range of $(T - \lambda I)_W$ and is linearly independent, it is therefore a basis of the range of $(T - \lambda I)_W$ which, in turn, must therefore be of dimension ${ n - q }$. Meanwhile, the kernel of $(T - \lambda I)_W$ is the set of eigenvectors ${ \setc{ \y_i }{ 1 \leq i \leq q } }$ and so has dimension $q$.\\
			
			The space $W$ is generated by $Z$ -- a finite set of vectors -- and is thus finite-dimensional. So we can employ the dimension formula (\autoref{theo:linear_map_dimension_formula}) to deduce that,
			\[ \dim W = \operatorname{rank} (T - \lambda I)_W + \operatorname{nullity} (T - \lambda I)_W = (n - q) + q = n. \]
			
			Since, by the induction hypothesis, ${ \cardinality{Z} = n }$ and this is equal to the dimension of $W$, the space that $Z$ generates, it follows that $Z$ is a basis of the space $W$. Therefore $Z$ is linearly independent.
		\end{proof}	
		
		
		
		
		
		\biggerskip
		\subsubsection{Jordan Canonical Form}
		\bigskip
		\boxeddefinition{A \textbf{Jordan block} is a matrix (appearing as a block within a larger matrix) with the form,
			\[
				\begin{bmatrix}
					\lambda & 1 &  &  &  &  & \\
					& \lambda & 1 &  &  &  &  \\
					& & \ddots & \ddots &  & &  \\
					& & & \ddots & \ddots & & \\
					& & & & \ddots & \ddots & \\
					& & & & &  \lambda & 1
				\end{bmatrix} 
			\]
			or its transpose (depending on the ordering of the basis vectors).
		}
		\boxeddefinition{A \textbf{Jordan} matrix or \textbf{Jordan form} or \textbf{Jordan Canonical form} matrix, is a matrix consisting solely of jordan blocks.}
		\boxeddefinition{Let $T$ be a linear operator on a vector space $V$ such that the characteristic polynomial of $T$ factorizes. A \textbf{Jordan Canonical Basis} is a basis for $V$ that is a disjoint union of cycles of generalized eigenvectors of $T$.}\label{def:jordan-canonical-basis}
		
		\biggerskip
		\note{\textbf{Preliminary to Proof of Existence Jordan Canonical Basis}\\\\
			If ${ T(x, y) = (ax + y, ay) }$ then, \wrt the standard basis, $T$ is represented in matrix form as,
			\[
			\begin{bmatrix}
				a & 1\\
				0 & a
			\end{bmatrix}.
			\]
			The $x$-axis is $T$-invariant because ${ T(x, 0) = (ax, 0) }$. So, the characteristic polynomial of the restriction of $T$ to the $x$-axis divides the characteristic polynomial of $T$. The characteristic polynomial of $T$ is ${ p(t) = (a - t)^2 }$ so that ${ (T - a I)^2\,\v = \0 }$ for any $\v$ in the space.\\\\
			Let ${ W = R(T - a I) }$ and define the restriction of $T$ to $W$ as $T_W$ so that, for ${ \w \in W }$,
			\[ (T - a I)\w = \0. \]
			What this looks like in matrix form is
			\[
			(T - a I) = \begin{bmatrix}
				0 & 1\\
				0 & 0
			\end{bmatrix}  \eqand
			W = R(T - a I) = \alpha \begin{bmatrix}1\\0\end{bmatrix} \eqword{for} \alpha \in \F{}.
			\]
			So, clearly, the jordan basis for $T_W$ is the single vector ${ (1,0)^T = \e{1} }$ and this is equal to the nullspace ${ N(T - a I) }$ meaning that we have a cycle of length 2 associated with the eigenvalue $a$.\\\\
			If, on the other hand, we had ${ T(x, y) = (ax, ay) }$ with matrix \wrt the standard basis,
			\[
			\begin{bmatrix}
				a & 0\\
				0 & a
			\end{bmatrix},
			\]
			then ${ W = R(T - a I) = \0 }$ with basis ${ \emptyset = \{\} }$ and the nullspace ${ N(T - a I) }$ is the whole space (in this case the $xy$-plane). So, in this case, we have no cycles of length > 1 associated with the eigenvalue $a$ and, instead, we have two lone eigenvectors.\\\\
			The converse case is ${ T(x, y) = (ax, by) }$ with matrix \wrt the standard basis,
			\[
			\begin{bmatrix}
				a & 0\\
				0 & b
			\end{bmatrix}.
			\]
			Then 
			\[ 
			(T - a I) = \begin{bmatrix}
				0 & 0\\
				0 & b - a
			\end{bmatrix}   \eqand
			W = R(T - a I) = \alpha \begin{bmatrix}0\\1\end{bmatrix} \eqword{for} \alpha \in \F{}.
			\] 
			In this case, the nullspace ${ N(T - a I) \centernot\subseteq R(T - a I) }$ and there are no cycles of length > 1; just one lone eigenvector associated with the eigenvalue $a$. A basis of the space is completed by another eigenvector for a different eigenvalue $b$.
		}
		
		\bigskip
		\labeledTheorem{(\textbf{Jordan Canonical Basis.}) Let $T$ be a linear operator on a vector space $V$ such that the characteristic polynomial of $T$ factorizes. Then there exists a Jordan canonical basis for $T$.}{jordan-basis-existence}
		\begin{proof}
			Let $V$ have dimension $n$. The proof will follow an induction on $n$.\\
			
			For ${ n = 1 }$, the proposition holds trivially as any vector in the space is an eigenvector that spans the space.\\
			
			Assume that the proposition holds for any vector space of dimension less than ${ n = \dim V > 1 }$. Let ${ \setc{\lambda_i}{1 \leq i \leq k} }$ be the set of distinct eigenvalues of $T$. We arbitrarily select the first eigenvalue $\lambda_1$ and define (where $R$ denotes the range),
			\[ W = R(T - \lambda_1 I), \; r = \operatorname{rank} (T - \lambda_1 I) = \dim W. \]
			By \autoref{prop:range-nullspace-origin-and-whole-space-are-all-T-invariant}, $W$ is $(T - \lambda_1 I)$-invariant and therefore, by \autoref{prop:T-invariance-is-equiv-to-T-minus-lambda-invariance}, $W$ is also $T$-invariant. So we can define $T_W$, the restriction of $T$ to $W$.
			
			\note{For all ${ \v \in V }$ we have,
				\[ (T - \lambda_1 I)^{m_1}(T - \lambda_2 I)^{m_2}\cdots(T - \lambda_k I)^{m_k}\,\v = \0 \]
				and, since
				\[ \forall \w \in W \logicsep \exists \v \in V \suchthat \w = (T - \lambda_1 I)\v, \] 
				for all ${ \w \in W }$ we have,
				\[ (T - \lambda_1 I)^{m_1 - 1}(T - \lambda_2 I)^{m_2}\cdots(T - \lambda_k I)^{m_k}\,\w = \0. \]
			}
			
			Then, by \autoref{prop:char-poly-of-restriction-to-invariant-space-divides-char-poly-of-T}, the characteristic polynomial of $T_W$ divides that of $T$ and so, we can deduce, also factorizes. Furthermore, $T_W$ is a linear operator over an $r$-dimensional vector space where ${ r < n }$ because, by the definition of $\lambda_1$ as an eigenvalue of $T$, ${ (T - \lambda_1 I) }$ must have a non-trivial nullspace. So $T_W$ is a linear operator over a vector space of dimension less than $n$ such that the characteristic polynomial of $T_W$ factorizes. Therefore, we can use the induction hypothesis to assert the existence of a jordan canonical basis for $T_W$ -- which we shall denote $J_W$.\\
			
			Let $S_i$ for ${ 1 \leq i \leq k }$ be the generalized eigenvectors in $J_W$ corresponding to the eigenvalue $\lambda_i$. Since $S_i$ is a subset of a jordan basis, it is therefore a linearly independent set of disjoint cycles of generalized eigenvectors. Since it is the subset corresponding to $\lambda_i$, it is therefore a linearly independent set of disjoint cycles of generalized eigenvectors corresponding to $\lambda_i$.\\
			
			Let ${ \setc{Z_j}{1 \leq j \leq p} }$ be the set of cycles of generalized eigenvectors whose disjoint union is equal to $S_1$. For each cycle, let
			\[ Z_j' = \{\y_j\} \cup Z_j \eqword{for some} \y_j \in V \]
			such that ${ (T - \lambda_1 I)\y_j }$ is the end vector of $Z_j$. Such a $\y_j$ is guaranteed to exist because 
			\[ Z_j \subseteq W = R(T - \lambda_1 I). \]
			Then $Z_j'$ is also a cycle of generalized eigenvectors of $T$ corresponding to $\lambda_1$.\\
			%\autoref{prop:distinct-eigenvalue-gen-eigenspaces-are-lin-independent}
			Now, let ${ I = \setc{z_j}{1 \leq j \leq p} }$ be the set of initial vectors of the cycles $Z_j$. Since ${ I \subseteq J_W }$ and $J_W$ is a basis and hence linearly independent, $I$ is linearly independent. Furthermore, $I$ is a subset of the nullspace ${ N(T - \lambda_1 I) }$.
			\note{The nullspace ${ N(T - \lambda_1 I) }$ comprises the eigenvectors of $T$ for the eigenvalue $\lambda_1$ but these are only in the space $W$ -- the range of ${ T - \lambda_1 I }$ -- if they are the initial vectors in a cycle of length > 1. Hence ${ I \subseteq N(T - \lambda_1 I) }$.}
			
			Next, we extend $I$ to a basis for the nullspace which, by the dimension formula for finite-dimensional vector spaces (\autoref{theo:linear_map_dimension_formula}), has dimension ${ n - r }$,
			\[ I_N = \{ z_1, \dots, z_p, z_{p+1}, \dots, z_{n-r} \}. \]
			\note{Here,
				\[ p = \dim (R(T - \lambda_1 I) \cap N(T - \lambda_1 I)). \]
				If there are no cycles (of length > 1) for the eigenvalue $\lambda_1$, then ${ p = 0 }$ and the basis of the nullspace of $N(T - \lambda_1 I)$ is comprised solely of ${ n - r }$ eigenvectors. Otherwise ${ z_1, \dots, z_p }$ are the eigenvectors that are initial vectors of cycles of generalized eigenvectors for $\lambda_1$ and they are extended to a basis for $N(T - \lambda_1 I)$ by adding the eigenvectors that are not in cycles (of length > 1).
			}
			If ${ p < n - r }$ (i.e. if there are eigenvectors that are not in cycles of length > 1), then ${ z_{p+1}, \dots, z_{n-r} }$ consists of eigenvectors not in $W$. These eigenvectors can be considered as cycles of length 1 and used to extend the cycles $Z_j'$ such that, 
			\[ Z_j' = \{z_j\} \eqword{for} p+1 \leq j \leq n-r. \]
			So, ${ Z' = \setc{Z_j'}{1 \leq j \leq n-r} }$ is a collection of disjoint cycles of generalized eigenvectors corresponding to $\lambda_1$. Let
			\[ S_1' = \bigcup_{j=1}^{n-r} Z_j' = Z_1' \cup Z_2' \cup \cdots \cup Z_{n-r}'. \]
			Then, since the initial vectors of the cycles in $Z'$ form a linearly independent set, by \autoref{prop:for-given-eigenvalue-the-union-of-cycles-of-linearly-independent-eigenvectors-is-a-linearly-independent-set}, $S_1'$ is a linearly independent disjoint union of the cycles. Furthermore,
			\[ \cardinality{S_1'} = \cardinality{S_1} + (n - r). \]
			If we define,
			\[ J = S_1' \cup S_2 \cup \cdots \cup S_k \]
			then $J$ is linearly independent by \autoref{prop:union-of-lin-ind-subsets-of-gen-eigenspaces-is-lin-ind} and
			\[ \cardinality{J} = \cardinality{J_W} + (n - r) = r + (n - r) = n. \]
			Therefore $J$ is a basis of $V$ comprising disjoint cycles of generalized eigenvectors of $T$ and, as such, is a jordan canonical basis for $T$.
		\end{proof}
	
	
		\bigskip
		\labeledProposition{Let $T$ be a linear operator on a vector space $V$ such that the characteristic polynomial of $T$ factorizes. Then, for each eigenvalue, the dimension of its generalized eigenspace is equal to its multiplicity.
		}{dim-of-gen-eigenspace-equal-to-multiplicity}
		\begin{proof}
			Let ${ \setc{\lambda_i}{1 \leq k \leq k} }$ be the distinct eigenvalues of $T$ with the corresponding multiplicities ${ \setc{m_i}{1 \leq k \leq k} }$. By \autoref{theo:jordan-basis-existence}, there exists a jordan basis for $T$. Let this basis be $J$ and the matrix of $T$ \wrt $J$,
			\[ [T]_J = \inv{[J]}[T(J)]. \]
			Since $[T]_J$ is in Jordan Normal Form, it is upper-triangular and, therefore, the multiplicity of any eigenvalue $\lambda_i$ is equal to the number of columns of the matrix that have $\lambda_i$ as the diagonal element. This, in turn, is equal to the number of vectors in the jordan basis $J$ that are generalized eigenvectors corresponding to the eigenvalue $\lambda_i$.\\
			
			So if, for each $i$, 
			\[ U_i = K_{\lambda_i} \cap J \]
			is the subset of the jordan basis that comprises generalized eigenvectors corresponding to the eigenvalue $\lambda_i$, then
			\[ \forall i \logicsep \cardinality{U_i} = m_i. \]
			
			Since $U_i$ is a linearly independent set in $K_{\lambda_i}$ we must have,
			\[ \cardinality{U_i} \leq \dim K_{\lambda_i} \implies m_i \leq \dim K_{\lambda_i} \]
			and by \autoref{prop:sum-of-eigenvalue-multiplicities-is-dimension-of-space}, we have that
			\[ \sum_i m_i = \dim V. \]
			Furthermore, since the spaces $K_{\lambda_i}$ are linearly independent and the direct sum of the spaces is a subspace of $V$, we must have,
			\[ \sum_i \dim K_{\lambda_i} \leq \dim V. \]
			Therefore,
			\[\begin{aligned}
				&& \dim V = \sum_i m_i \leq \sum_i \dim K_{\lambda_i} &\leq \dim V \\
				&\implies & \sum_i m_i = \sum_i \dim K_{\lambda_i} &= \dim V.
			\end{aligned}\]
			So we can deduce,
			\[\begin{aligned}
				&& \forall i (\dim K_{\lambda_i} - m_i \geq 0) \; &\land \; \sum_i \dim K_{\lambda_i} - m_i = 0 \\
				&\implies & \forall i (\dim K_{\lambda_i} &- m_i = 0).
			\end{aligned}\]
			Therefore, for each $i$, ${ \dim K_{\lambda_i} = m_i }$ as required.	
		\end{proof}
		\begin{corollary}
			If $T$ is a linear operator on a vector space $V$ such that the characteristic polynomial of $T$ factorizes, then $T$ is diagonalizable iff, for each eigenvalue, its eigenspace is equal to its generalized eigenspace.
		\end{corollary}
		\begin{proof}
			For any linear operator such as $T$, by \autoref{prop:dim-of-gen-eigenspace-equal-to-multiplicity}, for each distinct eigenvalue $\lambda$ of $T$ with multiplicity $m$,
			\[ \dim K_\lambda = m. \]
			Also, by \autoref{prop:lin-operator-diagonalizable-iff-all-eigenspace-dimensions-equal-to-multiplicity-of-eigenvalue}, $T$ is diagonalizable iff, 
			\[ \dim E_\lambda = m. \]
			So we can put these two together to say that $T$ is diagonalizable iff
			\[ \dim E_\lambda = \dim K_\lambda. \]
			Furthermore, since $E_\lambda$ is a subspace of $K_\lambda$ (\autoref{prop:generalized-eigenspace-is-T-invariant-subspace}),
			\[ \dim E_\lambda = \dim K_\lambda \implies E_\lambda = K_\lambda. \]
		\end{proof}
		\begin{corollary}
			If $T$ is a linear operator on a vector space $V$ such that the characteristic polynomial of $T$ factorizes, then for every eigenvalue $\lambda$ with multiplicity $m$,
			\[ K_\lambda = N((T - \lambda I)^m) \]
			where $N$ denotes the nullspace.
		\end{corollary}
		\begin{proof}
			If ${ \v \in N((T - \lambda I)^m) }$ then, by the definition of the generalized eigenspace (\ref{def:generalized-eigenspace}), ${ \v \in K_\lambda }$. Therefore ${ N((T - \lambda I)^m) \subseteq K_\lambda }$.\\
			
			Conversely, if ${ \v \in K_\lambda }$ then there exists some minimial ${ p \in \N{} }$ such that,
			\[ (T - \lambda I)^p\,\v = \0. \]
			If we consider the $p$-length cycle whose end vector is this $\v$,
			\[ C = \{ (T - \lambda I)^{p-1}\,\v, \dots, (T - \lambda I)\v, \v \}, \]
			then, by \autoref{theo:gen-eigenvecs-in-a-chain-are-linearly-independent}, the vectors in the cycle $C$ are linearly independent. By \autoref{prop:dim-of-gen-eigenspace-equal-to-multiplicity}, the dimension of $K_\lambda$ is $m$ and, since the dimension is an upper bound on the length of a linearly independent set in the space, we must have ${ p \leq m }$ and so,
			\[ (T - \lambda I)^m\,\v = (T - \lambda I)^{m-p}\,(T - \lambda I)^p\,\v = (T - \lambda I)^{m-p}\,\0 = \0. \]
			It follows then, that ${ \v \in N((T - \lambda I)^m) }$ and so ${ K_\lambda \subseteq N((T - \lambda I)^m) }$.
		\end{proof}
		\begin{corollary}
			If $T$ is a linear operator on a vector space $V$ such that the characteristic polynomial of $T$ factorizes, then $V$ is a direct sum of the generalized eigenspaces of $T$.
		\end{corollary}
		\begin{proof}
			By \autoref{prop:dim-of-gen-eigenspace-equal-to-multiplicity}, for each distinct eigenvalue $\lambda_i$ with multiplicity $m_i$,
			\[ \dim K_{\lambda_i} = m_i. \]
			By \autoref{prop:distinct-eigenvalue-gen-eigenspaces-are-lin-independent}, the spaces $K_{\lambda_i}$ for the different $\lambda_i$, are linearly independent. Therefore if we let $B_i$ be a basis for $K_{\lambda_i}$ then
			\[ B = \bigcup_i B_i \]
			is a disjoint union and the set $B$ is linearly independent because, for ${ \b_i \in B_i }$,
			\[\begin{aligned}
				&& \b_1 + \b_2 + \cdots + \b_k &= \0 \\
				&\iff & \b_1 = \b_2 = \cdots = \b_k &= \0
			\end{aligned}\]
			and for each $\b_i$,
			\[ \b_i = \alpha_1 \b_{i1} + \cdots + \alpha_k \b_{ik} = \0 \implies \alpha_1,\dots\alpha_k = 0. \]
			So $B$ is a linearly independent set of vectors in the space $V$ with
			\[ \cardinality{B} = \sum_i \cardinality{B_i} = \sum_i \dim K_{\lambda_i} = \sum_i m_i. \]
			By \autoref{prop:sum-of-eigenvalue-multiplicities-is-dimension-of-space}, we have,
			\[ \sum_i m_i = \dim V \]
			and so, $B$ is a basis for $V$.\\
			
			Therefore, by \autoref{prop:bases-of-direct-summands-have-disjoint-union-equal-to-basis-of-space}, we have,
			\[ K_{\lambda_1} \oplus K_{\lambda_2} \oplus \cdots \oplus K_{\lambda_k} = V. \qedhere\]
		\end{proof}
		
		
		\sep
		\subsubsubsection{Generalized Eigenvectors and Jordan Blocks}
		Consider a simple case of repeated eigenvalues: a matrix $A$ that simply represents a uniform scaling by $a$, for some constant $a$,
		\[ 
		A = \begin{bmatrix}
			a & 0\\
			0 & a
		\end{bmatrix}.
		\]
		In this case, clearly, the only eigenvalue is ${ \lambda = a }$. From which we obtain,
		\[ (A - aI) = 	\begin{bmatrix}
			0 & 0\\
			0 & 0
		\end{bmatrix}
		\]
		which has a 2-dimensional nullspace equal to the entire vector space over which it operates. So we can use the standard basis as the two eigenvectors. If the diagonalized matrix is $D$ and the change of basis matrix to the eigenbasis is $P$ (which in this case is the identity) then ${ AP = PD }$ which is the matrix equation,
		\[ 
		\begin{bmatrix}
			a & 0\\
			0 & a
		\end{bmatrix}
		\begin{bmatrix}
			1 & 0\\
			0 & 1
		\end{bmatrix} =
		\begin{bmatrix}
			1 & 0\\
			0 & 1
		\end{bmatrix}
		\begin{bmatrix}
			a & 0\\
			0 & a
		\end{bmatrix}.
		\]
		
		\bigskip
		On the other hand, if the diagonal entries of $A$ are distinct so that the matrix becomes a non-uniform, rectangular scaling,
		\[ 
		A = \begin{bmatrix}
			a & 0\\
			0 & b
		\end{bmatrix}
		\]
		we have, in this case, eigenvalues: ${ \lambda_1 = a, \, \lambda_2 = b }$. From which we obtain,
		\[ (A - aI) = 	\begin{bmatrix}
			0 & 0\\
			0 & b-a
		\end{bmatrix} \eqand
		(A - bI) = 	\begin{bmatrix}
			a-b & 0\\
			0 & 0
		\end{bmatrix}	
		\]
		which, each have a 1-dimensional nullspace, the direct sum of them forming the entire vector space. In fact, the eigenvectors are the two standard basis vectors and, similarly to the previous case, we have the following matrix equation,
		\[ 
		\begin{bmatrix}
			a & 0\\
			0 & b
		\end{bmatrix}
		\begin{bmatrix}
			1 & 0\\
			0 & 1
		\end{bmatrix} =
		\begin{bmatrix}
			1 & 0\\
			0 & 1
		\end{bmatrix}
		\begin{bmatrix}
			a & 0\\
			0 & b
		\end{bmatrix}.
		\]
		
		\bigskip
		However, in the case,
		\[ 
		A = \begin{bmatrix}
			a & 1\\
			0 & b
		\end{bmatrix}
		\]
		we have the same eigenvalues ${ \lambda_1 = a, \, \lambda_2 = b }$, but the eigenvector of ${ \lambda_2 = b }$ is not the same.
		\[ (A - aI) = 	\begin{bmatrix}
			0 & 1\\
			0 & b-a
		\end{bmatrix} \eqand
		(A - bI) = 	\begin{bmatrix}
			a-b & 1\\
			0 & 0
		\end{bmatrix}	
		\]
		In this case, the eigenvector of ${ \lambda_2 = b }$ is
		\[ \begin{bmatrix}\frac{1}{b-a}\nn 1\end{bmatrix}. \]
		The reason is that, here, we have a stretching of $b$ in the second dimension (say, the $y$-direction) that also has a component of 1 in the $x$-direction -- which is being stretched by $a$. So, in order to have a vector whose components are \textit{both} stretched by $b$ we need the $x$ component to obey,
		\[ 1 + ax = bx \iff x = 1/(b-a). \] 
		\TODO{relate this to modular arithmetic and resonance}\\
		
		Here the matrix equation is,
		\[ 
		\begin{bmatrix}
			a & 1\\
			0 & b
		\end{bmatrix}
		\begin{bmatrix}
			1 & \frac{1}{b-a}\\
			0 & 1
		\end{bmatrix} =
		\begin{bmatrix}
			1 & \frac{1}{b-a}\\
			0 & 1
		\end{bmatrix}
		\begin{bmatrix}
			a & 0\\
			0 & b
		\end{bmatrix}.
		\]
		
		\bigskip
		Now, if we allow $b$ to go to $a$ then the first component of the eigenvector is going to go to infinity but we are also approaching,
		\[ 
		\begin{bmatrix}
			a & 1\\
			0 & b
		\end{bmatrix} \to
		\begin{bmatrix}
			a & 1\\
			0 & a
		\end{bmatrix} \leadsto
		(A - aI) = (A - bI) =
		\begin{bmatrix}
			0 & 1\\
			0 & 0
		\end{bmatrix}	
		\]
		which is a repeated eigenvalue with a generalized eigenvector. Specifically, the eigenvector is ${ (1,0)^T }$ and the generalized eigenvector satisfies,
		\[
		(A - aI)\v = (1,0)^T \implies \v = (0,1)^T
		\]
		so that 
		\[ (A - aI)^2 \, \v = (A - aI) (1,0)^T = \0. \]
		\note{Notice also, that since $A$ is a ${ 2 \times 2 }$ matrix, in accordance with \autoref{prop:n-eigenvalues-are-roots-of-n-ary-matrix-polynomial}, we have,
			\[ (A - aI)^2 = 0. \]}
		
		So, in matrix equations, we have
		\[ 
		\begin{bmatrix}
			0 & 1\\
			0 & 0
		\end{bmatrix}\begin{bmatrix}0\\ 1\end{bmatrix} = \begin{bmatrix}1\\ 0\end{bmatrix}.
		\]
		
		If we try to use the generalized eigenvector in the same way as a "normal" eigenvector and form the diagonalized matrix in the normal way then it doesn't work as intended because, clearly
		\[ 
		\begin{bmatrix}
			a & 1\\
			0 & a
		\end{bmatrix}
		\begin{bmatrix}
			1 & 0\\
			0 & 1
		\end{bmatrix} \neq
		\begin{bmatrix}
			1 & 0\\
			0 & 1
		\end{bmatrix}
		\begin{bmatrix}
			a & 0\\
			0 & a
		\end{bmatrix}.
		\]
		but, in fact,
		\[ 
		\begin{bmatrix}
			a & 1\\
			0 & a
		\end{bmatrix}
		\begin{bmatrix}
			1 & 0\\
			0 & 1
		\end{bmatrix} =
		\begin{bmatrix}
			1 & 0\\
			0 & 1
		\end{bmatrix}
		\begin{bmatrix}
			a & 1\\
			0 & a
		\end{bmatrix}.
		\]
		
		\smallskip
		The modification to the diagonal matrix reflects the fact that, for the generalized eigenvector,
		\[
		\begin{bmatrix}
			a & 1\\
			0 & a
		\end{bmatrix}\begin{bmatrix}0\\ 1\end{bmatrix} = \begin{bmatrix}1\\ 0\end{bmatrix} + a \begin{bmatrix}0\\ 1\end{bmatrix} =
		\begin{bmatrix}
			1 & 0\\
			0 & 1
		\end{bmatrix}\begin{bmatrix}1\\ a\end{bmatrix}.
		\]
		The matrix,
		\[ 
		\begin{bmatrix}
			a & 1\\
			0 & a
		\end{bmatrix}
		\]
		is a \textit{Jordan block} for the eigenvalue $a$.
	}

	\sep
	\subsubsubsection{Examples of Jordan Canonical Basis}
	\begin{exe}
		\ex{Let ${ T : P_2(\C{}) \longmapsto P_2(\C{}) }$ be a linear operator over the degree-2 complex polynomials and let $T$ be defined by
			\[ T(p) = -p - p'. \]
			If we use the standard basis for $P_2(\C{})$, ${ \{ 1, z, z^2 \} }$ then, the matrix of $T$ \wrt this basis,
			\[ A = 	\begin{bmatrix}
						-1  &  0 &  0\\
						 0  & -1 &  0\\
						 0  &  0 & -1
					\end{bmatrix} -
					\begin{bmatrix}
						0  &  1 &  0\\
						0  &  0 &  2\\
						0  &  0 &  0
					\end{bmatrix} =
					\begin{bmatrix}
						-1  & -1 &  0\\
						 0  & -1 & -2\\
						 0  &  0 & -1
					\end{bmatrix}.
			\]
			Clearly, the characteristic polynomial is ${ p(t) = -(t+1)^3 }$ and there is a single eigenvalue, -1, with multiplicity 3. The generalized eigenspace is the whole space of dimension 3, equal to the multiplicity (as predicted by \autoref{prop:dim-of-gen-eigenspace-equal-to-multiplicity}), and any basis of the space $P_2(\C{})$ is also a basis of the generalized eigenspace.
			\note{Note, though, that using, for example, the standard basis of $P_2(\C{})$ would not produce a Jordan Canonical Form matrix.}
			
			The eigenspace, meanwhile, is the nullspace ${ N(T + I) }$ where
			\[ A + I = 	\begin{bmatrix}
							0  & -1 &  0\\
							0  &  0 & -2\\
							0  &  0 &  0
						\end{bmatrix}
			\]
			and, in terms of the linear map,
			\[ (T + I)(p) = -p - p' + p = -p'. \]
			So the nullspace is
			\[ N(A + I) = t\begin{bmatrix}1\\0\\0\end{bmatrix} \eqword{for} t \in \C{} \]
			and
			\[ N(T + I) = \setc{ p(z) \in P_2(\C{}) }{ p' = 0 } = \setc{ p(z) = \alpha_0 }{ \alpha_0 \in \C{} }. \]
			That's to say, the eigenspace is the space of constant polynomials.\\
			
			If we look for a cycle with initial vector $(1,0,0)^T$ then we want,
			\[ (A + I)^2\,\v = \begin{bmatrix}1\\0\\0\end{bmatrix} = 
				\begin{bmatrix}
					0 & 0 & 2\\
					0 & 0 & 0\\
					0 & 0 & 0
				\end{bmatrix}\v \implies
				\v = \begin{bmatrix}0\\0\\\frac{1}{2}\end{bmatrix}
			\]
			and
			\[\begin{aligned}
				&& (T + I)^2\,(p) &= (T + I)(-p') = p'' = 1 \\
				&\implies & p(z) &= \frac{1}{2}z^2 + \alpha z + \beta \eqword{for} \alpha,\beta \in \C{}.
			\end{aligned}\]
			So the simplest cycle is: ${ \frac{1}{2}z^2, -z, 1 }$ and we can choose as a jordan basis,
			\[ J = \{ 2, -2z, z^2 \}. \]
			Then the matrix of $T$ \wrt this basis is in Jordan Canonical Form,
			\[ [T]_J = \inv{[J]}[T(J)] = 	
				\begin{bmatrix}
					\frac{1}{2} & 0 & 0\\
					0 & -\frac{1}{2} & 0\\
					0 & 0 & 1
				\end{bmatrix}
				\begin{bmatrix}
					-2 & 2 & 0\\
					0 & 2 & -2\\
					0 & 0 & -1
				\end{bmatrix} =
				\begin{bmatrix}
					-1 & 1 & 0\\
					0 & -1 & 1\\
					0 & 0 & -1
				\end{bmatrix}.
			\]
		}
	\end{exe}





% ---------------- break -------------
\pagebreak




\searchableSubsection{Projections}{linear transformations, projections}{
	\biggerskip
	\subsubsection{Linear Operators as Projections}
	\boxeddefinition{\textbf{(Projection)} A linear operator ${ T: V \longmapsto V }$ is a \textit{projection} on ${ W_1 \subseteq V }$ if
		\begin{enumerate}[label=(\roman*)]
			\item{there exists a subspace ${ W_2 \subseteq V }$ such that ${ W_1 \oplus W_2 = V }$;}
			\item{for ${ \v = \w_1 + \w_2 \in V }$ where ${ \w_1 \in W_1, \, \w_2 \in W_2 }$, we have
				\[ T\v = T(\w_1 + \w_2) = \w_1. \]
			}
		\end{enumerate}
		The projection of $V$ onto $W_1$ --- often denoted $P_{W_1}$ ---  is described as \textit{parallel} to $W_2$ because the displacement vector, from the original vector to its projected image,
		\[ (\w_1 + \w_2) - T(\w_1 + \w_2) = (\w_1 + \w_2) - \w_1 = \w_2 \]
		is in $W_2$.\\
		
		If we have,
		\[ W_1^\perp = W_2 \]
		then the projection is further described as an \textit{orthogonal projection}.
	}

	\medskip
	\labeledProposition{If $T$ is a projection in a vector space $V$ then
		\[ V = R(T) \oplus N(T) \]
		where $R(T)$ and $N(T)$ denote the range and nullspace of $T$ respectively.
	}{for-a-projection-the-direct-sum-of-the-range-and-nullspace-equals-the-space}
	\begin{proof}
		If $T$ is a projection then, by the definition,
		\[ W_1 \oplus W_2 = V \eqand \forall \w_1 \in W_1,\w_2 \in W_2 \logicsep T(\w_1 + \w_2) = \w_1. \]
		Then ${ R(T) \subseteq W_1 }$ and because, for every ${ \w_1 \in W_1 }$ there exists a ${ \v = \w_1 + \w_2 \in V }$ such that ${ T(\v) = \w_1 }$, also ${ W_1 \subseteq R(T) }$. Therefore, ${ R(T) = W_1 }$.\\
		
		Furthermore,
		\[ T(\v) = T(\w_1 + \w_2) = \0 = \w_1 \implies \v = \w_1 + \w_2 = \0 + \w_2 = \w_2 \in W_2 \]
		which implies that ${ N(T) \subseteq W_2 }$. Since also ${ T(\w_2) = T(\0 + \w_2) = \0 }$ then we also have
		\[ W_2 \subseteq N(T) \]
		and so ${ W_2 = N(T) }$.\\
		
		Therefore ${ V = R(T) \oplus N(T) }$ as required.
	\end{proof}

	\bigskip	
	\labeledProposition{If $T$ is a linear operator over a vector space $V$ and $T$ is a projection of $V$ onto $W$, then $W$ is $T$-invariant and the restriction of $T$ to $W$ is the identity operator on $W$,
		\[ T_W = I_W. \]
	}{restriction-to-subspace-of-projection-onto-same-subspace-is-identity-operator}
	\begin{proof}
		If $T$ is a projection of $V$ onto $W$ then, for all ${ \v \in V }$ there is some ${ \w \in W }$ such that,
		\[ T\v = \w. \]
		Therefore, since ${ W \subseteq V }$ we also have, for all ${ \w_1 \in W }$ and some ${ \w_2 \in W }$,
		\[ T\w_1 = \w_2 \in W \implies TW \subseteq W. \]
		So $W$ is shown to be $T$-invariant.\\
		
		Since $W$ is $T$-invariant, we can define the restriction of $T$ to $W$,
		\[ T_w : W \longmapsto W \suchthat T_w\w_1 = T\w_1 = \w_2. \]
		But also, since $T$ is a projection onto $W$, it is also idempotent,
		\[ T^2 = T \implies T^2\,\v = T\v \implies {T_w}^2\,\w = T^2\,\w = T\w = T_w\w. \]
		Putting these two together we obtain, for any ${ \w_1,\w_2 \in W }$ such that ${ T_w\w_1 = \w_2 }$,
		\[ {T_w}^2\,\w_1 = T_w\w_1 = \w_2 = T_w(T_w\w_1) = T_w\w_2 = {T_w}^2\,\w_2. \]
		We have obtained 
		\[ T_w\w_2 = \w_2 \]
		which implies that ${ T_w = I_w }$, the identity operator on $W$.
	\end{proof}


	\bigskip
	\labeledProposition{A linear operator $T$ on a finite-dimensional vector space $V$ is a projection iff it is idempotent. That's to say,
		\[ T^2 = T. \]
	}{linear-operator-is-projection-iff-idempotent}
	\begin{proof}
		Let ${ W_1 \oplus W_2 = V }$ and $T$ be a projection such that for ${ \w_1 \in W_1, \w_2 \in W_2 }$,
		\[ T(\w_1 + \w_2) = \w_1. \]
		Then $T$ is clearly idempotent because
		\[ T(T(\w_1 + \w_2)) = T(\w_1) = \w_1 = T(\w_1 + \w_2) \implies T^2 = T. \]
		
		\nl
		Conversely, let $T$ be a linear operator on $V$ such that ${ T^2 = T }$. Then, for any ${ \v \in V }$,
		\[\begin{aligned}
			&& T(T\v) &= T\v \\
			&\iff & T(T\v) - T\v &= \0 &\sidecomment{} \\
			&\iff & T(T\v - \v) &= \0 &\sidecomment{} \\
			&\iff & T\v - \v &\in N(T) &\sidecomment{} \\
			&\iff & \exists \u \in N(T) \suchthat \v &= T\v + \u.
		\end{aligned}\]
		Since ${ T\v \in R(T) }$, this shows that
		\[ R(T) + N(T) = V. \]
		For any ${ \u \in R(T) \cap N(T) }$, since ${ \u \in R(T) }$ then there exists some ${ \v \in V }$ such that ${ \u = T\v }$ and also, since ${ \u \in N(T) }$, we have ${ T\u = \0 }$. Putting these together with the idempotence of $T$,
		\[ \0 = T\u = T(T\u) = T(T\v) = T\v = \u. \]
		Therefore, we can deduce that ${ R(T) \cap N(T) = \{\0\} }$ and so,
		\[ R(T) \oplus N(T) = V. \]	
		We can also deduce, using the idempotence, that for any ${ \w = T\v \in R(T) }$,
		\[ T(\w) = T(T\v) = T^2\v = T\v = \w \implies \forall \w \in R(T) \logicsep T\w = \w. \]
		So, if we let ${ W_1 = R(T), W_2 = N(T) }$ so that
		\[ W_1 \oplus W_2 = V \]
		and, for any ${ \v \in V }$, there exists ${ \w_1 \in W_1, \w_2 \in W_2 }$ such that $ \v = \w_1 + \w_2 $, then
		\[ T\v = T(\w_1 + \w_2) = T(\w_1) + T(\w_2) = \w_1 + \0 = \w_1. \] 
		Therefore, $T$ is a projection as required.
	\end{proof}


	\bigskip
	\labeledProposition{A projection on a finite-dimensional inner product space on the reals with the standard inner product is orthogonal iff its matrix is symmetric.}{projection-on-finite-inner-prod-space-is-orthogonal-iff-matrix-is-symmetric}
	\begin{proof}
		Let $P$ be a projection on $V$, a finite-dimensional inner product space over the reals with the standard inner product. For convenience, let $P$ denote both the projection as a linear transformation and the matrix representing it.\\
		
		If $P$ is symmetric then
		\[\begin{aligned}
			&& P &= P^T &\sidecomment{by symmetry}\\
			&\implies & N(P) &= N(P^T) = R(P)^\perp.  &\sidecomment{by \autoref{prop:real-matrix-nullspace-is-orthogonal-complement-of-rowspace}}
		\end{aligned}\]
		Since $P$ is a projection, by \autoref{prop:for-a-projection-the-direct-sum-of-the-range-and-nullspace-equals-the-space}, we also have
		\[ V = R(P) \oplus N(P) = R(P) \oplus R(P)^\perp \]
		and so symmetry of matrix $P$ implies that $P$ is an orthogonal projection.\\
		
		\nl
		Conversely, if $P$ is an orthogonal projection then
		\[ V = R(P) \oplus N(P) = R(P) \oplus R(P)^\perp \]
		so define 
		\[ W_1 = R(P) \eqand W_2 = N(P) = R(P)^\perp \]
		and then, for any ${ \v,\v' \in V }$ there exists some 
		\[ \w_1,\w_2,\w_1',\w_2' \suchthat \v = \w_1 + \w_2 \eqand \v' = \w_1' + \w_2'. \]
		Then,
		\[\begin{aligned}
			\inner{P\v}{\v'} &= \inner{P(\w_1 + \w_2)}{\w_1' + \w_2'} \nn
			&= \inner{P(\w_1)}{\w_1'} + \inner{P(\w_1)}{\w_2'} +\\
			&\hspace{50pt} \inner{P(\w_2)}{\w_1'} + \inner{P(\w_2)}{\w_2'} &\sidecomment{} \nn
			&= \inner{\w_1}{\w_1'} + \inner{\w_1}{\w_2'} + \inner{\0}{\w_1'} + \inner{\0}{\w_2'} &\sidecomment{${ \because P }$ is projection onto $W_1$} \\
			&= \inner{\w_1}{\w_1'} + 0 + \inner{\0}{\w_1'} + \inner{\0}{\w_2'} &\sidecomment{${ \because W_1 \perp W_2 }$} \\
			&= \inner{\w_1}{\w_1'} + 0 + 0 + 0 &\sidecomment{} \\
			&= \inner{\w_1}{\w_1'} \\
			&= \inner{\w_1 + \w_2}{P(\w_1' + \w_2')} = \inner{\v}{P\v'}.
		\end{aligned}\]
		By \autoref{prop:} then, $P$ is symmetric.\\
		
		We could also have said,
		\[\begin{aligned}
			&&(I - P)\v &= \v - P\v = \w_1 + \w_2 - \w_1 = \w_2 \\
			&\implies & \inner{P\v}{(I - P)\v'} &= 0 \\
			&\implies & v^T P^T (I - P) \v' &= 0
		\end{aligned}\]
		which, since it applies to any ${ \v,\v' \in V }$, by \autoref{prop:quadratic-forms-equal-for-all-vectors-in-space-implies-matrices-are-equal}, implies that ${ P^T (I - P) = 0 }$ and so
		\[\begin{aligned}
			&& P^T (I - P) = P^T - P^TP &= 0 \\
			&\iff & P^T &= P^T P &\sidecomment{} \\
			&\iff & P &= (P^T P)^T &\sidecomment{} \\
			&\iff & P &= P^T P &\sidecomment{} \\
			&\iff & P &= P^T.
		\end{aligned}\]
	\end{proof}


	\bigskip
	\labeledProposition{Let $V$ be a finite-dimensional inner product space over the reals with the standard inner product. Let $U$ be a subspace of $V$, $P$ the orthogonal projection of $V$ onto $U$, and ${ \v \in V }$ is an arbitrary vector.\\
	
	Then the closest point in $U$ to $\v$ is given by the orthogonal projection $P\v$ of the vector onto the space $U$.\\
		
	That's to say, for all ${ \u \in U }$,
		\[ \norm{\v - \u} \geq \norm{\v - P\v}. \]
	}{closest-point-in-subspace-to-a-vector-is-orthogonal-projection-of-vector-onto-subspace}
	\begin{proof}
		Since $P$ is the orthogonal projection of $V$ onto $U$, by \autoref{prop:for-a-projection-the-direct-sum-of-the-range-and-nullspace-equals-the-space}, we have
		\[ R(P) \oplus N(P) = U + U^\perp = V. \]
		So, for any ${ \v \in V }$,
		\[ \v = P\v + (\v - P\v) \]
		where ${ P\v \in U }$ and ${ \v - P\v \in U^\perp }$. 
		\note{We can see that ${ \v - P\v \in U^\perp }$ by observing that, because of the idempotence of $P$, we have
			\[ P(\v - P\v) = P\v - P^2\v = P\v - P\v = \0 \implies \v - P\v \in N(P) = U^\perp. \]
		}
		Then,
		\[ \norm{\v - \u} = \norm{P\v + (\v - P\v) - \u} = \norm{(\v - P\v) + (P\v - \u)}  \]
		where ${ P\v - \u \in U }$ because ${ P\v \in R(P) = U }$. Since the vectors ${ \v - P\v }$ and ${ P\v - \u }$ are orthogonal, by the Generalised Pythagoras Theorem (\autoref{theo:generalized-pythagoras-theorem}), we have
		\[ \norm{\v - \u}^2 = \norm{(\v - P\v)}^2 + \norm{(P\v - \u)}^2. \]
		By positive definiteness of the norm (\ref{def:vector-norm}),
		\[\begin{aligned}
			&&\norm{(P\v - \u)}^2 &\geq 0 \nn
			&\implies & \norm{\v - \u}^2 &\geq \norm{(\v - P\v)}^2 &\sidecomment{} \nn
			&\implies & \norm{\v - \u} &\geq \norm{(\v - P\v)}.  \qedhere
		\end{aligned}\]
	\end{proof}


	\bigskip
	\labeledProposition{Let ${ A \in \R{m \times n} }$ be a matrix of rank $n$ (i.e. full column rank). Then the matrix
		\[ P = A \inv{(A^T A)} A^T \]
		represents the orthogonal projection of $\R{m}$ onto the range of $A$.
	}{expression-for-orthogonal-projection-of-codomain-of-any-real-matrix-onto-its-range}
	\begin{proof}
		\[\begin{aligned}
			P^2 &= (A \inv{(A^T A)} A^T) (A \inv{(A^T A)} A^T) \\
			&= A \inv{(A^T A)} (A^T A) \inv{(A^T A)} A^T &\sidecomment{} \\
			&= A \inv{(A^T A)} A^T = P.
		\end{aligned}\]
		Therefore $P$ is idempotent and, by \autoref{prop:linear-operator-is-projection-iff-idempotent} then, $P$ is a projection.\\
		
		\[\begin{aligned}
			P^T &= (A \inv{(A^T A)} A^T)^T \\
			&= A (\inv{(A^T A)})^T A^T &\sidecomment{} \\
			&= A \inv{(A^T A)^T} A^T &\sidecomment{by \autoref{prop:matrix-inverse-of-transpose-is-transpose-of-inverse}} \\
			&= A \inv{(A^T A)} A^T = P.
		\end{aligned}\]
		Therefore $P$ is symmetric and, since it is also a projection, by \autoref{prop:projection-on-finite-inner-prod-space-is-orthogonal-iff-matrix-is-symmetric}, $P$ is an orthogonal projection.\\
		
		\[ P\x =  A \inv{(A^T A)} A^T \x = A (\inv{(A^T A)} A^T \x) = A\v \]
		for some ${ \v \in V }$. Therefore ${ R(P) \subseteq R(A) }$.\\
		
		\[ A\x = A \inv{(A^T A)} (A^T A) \x = A \inv{(A^T A)} A^T (A\x) = A \inv{(A^T A)} A^T \v \]
		for some ${ \v \in V }$. Therefore ${ R(A) \subseteq R(P) }$.\\
		
		So ${ R(P) = R(A) }$ and $P$ is an orthogonal projection onto the range of $A$ as required.
	\end{proof}

	\bigskip
	\begin{corollary}\label{coro:normal-equation}
		\textbf{(Normal Equation)} If a design matrix $X$ contains the samples of a dataset to which a model $\V{\theta}$ is to be fitted using least squares with the labels being the matrix $Y$, then the best fit model ${ \V{\hat\theta} }$ is given by
			\[ \V{\hat\theta} = \inv{(X^T X)} X^T Y. \]
	\end{corollary}
	\begin{proof}
		Fitting the model using the least squares method means finding the value of $\V{\theta}$ that minimises the squared error. That is
		\[ \argmin_{\V{\theta}} \norm{Y - X\V{\theta}}^2. \]
		By \autoref{prop:closest-point-in-subspace-to-a-vector-is-orthogonal-projection-of-vector-onto-subspace}, the value of $X\V{\theta}$ that miminises this cost function is obtained by the orthogonal projection of $Y$ onto the range of $X$, which by \autoref{prop:expression-for-orthogonal-projection-of-codomain-of-any-real-matrix-onto-its-range}, is 
		\[ X\V{\hat\theta} = P_X Y = X \inv{(X^T X)} X^T Y \]
		and so, resolving for the argument $\V{\hat\theta}$ we have
		\[ \V{\hat\theta} = \inv{X} P_X Y = \inv{X} (X \inv{(X^T X)} X^T Y) = \inv{(X^T X)} X^T Y.   \qedhere \]
	\end{proof}


	\bigskip
	\subsubsection{Reduced-Row Echelon Form as Projection}	
	\TODO{the reduced row echelon form of a matrix $A$ is a projection onto the range of $A$}

	\sep
	\begin{exe}{% extra braces to scope \newcommand
		\newcommand{\thet}{\V{\theta}} \newcommand{\theth}{\V{\hat\theta}}
		\ex{If we attempt to project the $x$-axis of $\F{3}$ onto the $z$-axis,
			\[ 
			T\left( \begin{bmatrix}1\\0\\0\end{bmatrix} \right) = \begin{bmatrix}0\\0\\1\end{bmatrix}, \;  
			T\left( \begin{bmatrix}0\\1\\0\end{bmatrix} \right) = \begin{bmatrix}0\\0\\0\end{bmatrix}, \;  
			T\left( \begin{bmatrix}0\\0\\1\end{bmatrix} \right) = \begin{bmatrix}0\\0\\0\end{bmatrix}
			\]
			then this leads to the matrix,
			\[ A = 	\begin{bmatrix}
				0 & 0 & 0\\
				0 & 0 & 0\\
				1 & 0 & 0
			\end{bmatrix}.
			\]
			But this is \textbf{not} a projection because ${ A^2 = 0 \neq A }$. The $z$-axis is not invariant under $A$; in fact, $A$ maps the $z$-axis to the origin.\\
			
			If we try to fix this by also satisfying,
			\[ T\left( \begin{bmatrix}0\\0\\1\end{bmatrix} \right) = \begin{bmatrix}0\\0\\1\end{bmatrix} \]
			we get the matrix,
			\[ A = 	\begin{bmatrix}
				0 & 0 & 0\\
				0 & 0 & 0\\
				1 & 0 & 1
			\end{bmatrix}.
			\]
			\textit{This} matrix $A$, \textit{does} satisfy ${ A^2 = A }$ and \textit{is} therefore a projection of the $x$-axis onto the $z$-axis.
		}
		\biggerskip
		\ex{Following on from example \ref{ex:orthogonal-complement-of-plane-in-R3}: Suppose
			\[ U = \operatorname{Lin} \left\{ \begin{bmatrix}1\\ 2\\ -1\end{bmatrix}, \begin{bmatrix}1\\ 0\\ 1\end{bmatrix} \right\} \eqand 
				W = U^\perp = \operatorname{Lin} \left\{ \begin{bmatrix}-1\\ 1\\ 1\end{bmatrix} \right\}. \]
			Then, by \autoref{prop:vector-orthogonal-complement-is-complement-space},
			\[ U \oplus W = \R{3} \]
			so we can define a function $P_U$, the projection of $\R{3}$ onto $U$ parallel to $W$.\\
			
			For any ${ \x = (x,y,z)^T \in \R{3} }$, we can express it \wrt a basis that is the disjoint union of bases of $U$ and $W$,
			\[
				\begin{bmatrix}x\\ y\\ z\end{bmatrix} = \alpha_1 \begin{bmatrix}1\\ 2\\ -1\end{bmatrix} + 
					\alpha_2 \begin{bmatrix}1\\ 0\\ 1\end{bmatrix} + \alpha_3 \begin{bmatrix}-1\\ 1\\ 1\end{bmatrix}.
			\]
			So we have,
			\[
				\begin{bmatrix}
					1 & 1 & -1\\
					2 & 0 & 1\\
					-1 & 1 & 1
				\end{bmatrix} \begin{bmatrix}\alpha_1\\ \alpha_2\\ \alpha_3\end{bmatrix} =
				\begin{bmatrix}x\\ y\\ z\end{bmatrix}
			\]
			which gives
			\[
				 \begin{bmatrix}\alpha_1\\ \alpha_2\\ \alpha_3\end{bmatrix} = 
				 \frac{1}{6}
				 \begin{bmatrix}
				 	x + 2y - z\\
				 	3x + 3z\\
				 	-2x + 2y + 2z
				 \end{bmatrix}
			\]
			and so the expression of the vector in $\R{3}$ in this basis is
			\[ 
				\begin{bmatrix}x\\ y\\ z\end{bmatrix} = \frac{1}{6}\left( 
									(x + 2y - z) \begin{bmatrix}1\\ 2\\ -1\end{bmatrix} + 
				(3x + 3z) \begin{bmatrix}1\\ 0\\ 1\end{bmatrix} + (-2x + 2y + 2z) \begin{bmatrix}-1\\ 1\\ 1\end{bmatrix} 
														\right)
			\]
			and the projection onto $U$ is therefore
			\[\begin{aligned}
				P_U((x,y,z)^T) &=  \frac{1}{6}\left(
										\begin{bmatrix}(x + 2y - z)\\ 2(x + 2y - z)\\ -(x + 2y - z)\end{bmatrix} +  
										\begin{bmatrix}(3x + 3z)\\ 0\\ (3x + 3z)\end{bmatrix}
									\right) \nn
				&= \frac{1}{6}\left( 
						\begin{bmatrix}2(2x + y + z)\\ 2(x + 2y - z)\\ 2(x - y + 2z)\end{bmatrix}
					\right) \nn
				&= \frac{1}{3}\left( 
						\begin{bmatrix}2x + y + z\\ x + 2y - z\\ x - y + 2z\end{bmatrix}
					\right).
			\end{aligned}\]
		
			We can instead calculate this by taking the matrix whose columns are the basis of $U$,
			\[
				A = [U] = 	\begin{bmatrix}
								1 & 1\\
								2 & 0\\
								-1 & 1
							\end{bmatrix}
			\]
			and using \autoref{prop:expression-for-orthogonal-projection-of-codomain-of-any-real-matrix-onto-its-range} to determine the orthogonal projection onto $U$ as
			\[ P_U = A \inv{(A^T A)} A^T. \]
			Since
			\[ 
				A^T A = \begin{bmatrix}
							6 & 0\\
							0 & 2
						\end{bmatrix} \implies
						\inv{A^T A} = 
						\begin{bmatrix}
							\frac{1}{6} & 0\\
							0 & \frac{1}{2}
						\end{bmatrix}
			\]
			we then have,
			\[\begin{aligned}
				P_U &= 	\begin{bmatrix}
							1 & 1\\
							2 & 0\\
							-1 & 1
						\end{bmatrix}
						\frac{1}{6}
						\begin{bmatrix}
							1 & 0\\
							0 & 3
						\end{bmatrix}
						\begin{bmatrix}
							1 & 2 & -1\\
							1 & 0 & 1
						\end{bmatrix}
						&\sidecomment{} \nn
				&= 	\frac{1}{6}
					\begin{bmatrix}
						1 & 3\\
						2 & 0\\
						-1 & 3
					\end{bmatrix} 
					\begin{bmatrix}
						1 & 2 & -1\\
						1 & 0 & 1
					\end{bmatrix}
					&\sidecomment{} \nn
				&= 	\frac{1}{6}
					\begin{bmatrix}
						4 & 2 & 2 \\
						2 & 4 & -2 \\
						2 & -2 & 4
					\end{bmatrix}
					&\sidecomment{} \nn
				&= 	\frac{1}{3}
					\begin{bmatrix}
						2 & 1 & 1 \\
						1 & 2 & -1 \\
						1 & -1 & 2
					\end{bmatrix}.
			\end{aligned}\]
		}
		\biggerskip
		\ex{Suppose we are trying to fit a linear model ${ Y = \theta_0 + \theta_1 X }$ to the following dataset.
			\[
				\begin{tabular}{ccc}
					\toprule%
					$X$ & $Y$ \\ \toprule%
					0 & 1 \\
					3 & 4 \\
					6 & 5 \\ \bottomrule
				\end{tabular} 
			\]
			We are trying to find a least squares solution (values of ${ \theta_0, \theta_1 }$ that minimise the squared error) to the system
			\[\begin{aligned}
				\theta_0 &= 1 \\
				\theta_0 + 3\theta_1 &= 4 \\
				\theta_0 + 6\theta_1 &= 5.
			\end{aligned}\]
			It's easy to see there is no exact solution (i.e. no solution with 0 squared error).\\
			
			If we describe this system in matrix form we have
			\[
				X\thet = Y \iff 
				\begin{bmatrix}
					1 & 0\\
					1 & 3\\
					1 & 6
				\end{bmatrix}
				\begin{bmatrix}
					\theta_0\\
					\theta_1
				\end{bmatrix} = 
				\begin{bmatrix}
					1\\
					4\\
					5
				\end{bmatrix}.
			\]
			Using \autoref{coro:normal-equation} to get the best fit value of $\thet$ we have,
			\[ 
				\theth = \inv{(X^T X)} X^T Y = \begin{bmatrix} \frac{4}{3}\\ \frac{2}{3}\end{bmatrix}.
			\]
			So the best fit model found is
			\[ Y = \frac{4}{3} + \frac{2}{3} X. \]
		}
		\bigskip
		\ex{The least squares fitting method can be used to fit non-linear models also. In this case, the non-linear model becomes linearized by the encoding of the features. For example, \TODO{LSE Further Linear Algebra [76]}
		}
	}% extra braces to scope \newcommand
	\end{exe}
}






% ---------------- break -------------
\pagebreak




\searchableSubsection{Generalised Inverses}{linear transformations}{
	\biggerskip
	\subsubsection{Left and Right Inverses}
	\boxeddefinition{Let $A$ be an ${ m \times n }$ matrix. Then the ${ m \times n }$ matrix $L$ is a \textit{left inverse} for $A$ iff
		\[ LA = I_n \]
		while the ${ n \times m }$ matrix $R$ is a \textit{right inverse} for $A$ iff
		\[ AR = I_m. \]
	}

	\bigskip
	\labeledProposition{Let $A$ be an ${ m \times n }$ matrix. Then $A$ has a \textbf{left} inverse iff:
		\begin{enumerate}[label=(\roman*)]
			\item{The nullspace of $A$ is the trivial nullspace $\{\0\}$;}
			\item{$A$ is an injection: The equation ${ A\x = \b }$ has either no solution or one unique solution;}
			\item{$A$ has rank $n$.}
		\end{enumerate}
	}{properties-of-matrix-with-left-inverse}
	\begin{proof}\nl[4]
		Let $L$ be a left inverse of $A$.
		\begin{enumerate}[label=(\roman*)]
			\item{Since, for any ${ \x \in N(A) }$,
				\[ A\x =\0 \iff LA\x = L\0 = \0 \iff I_n\x = \x = \0, \]
				we deduce that ${ N(A) = \{\0\} }$.
			}
			\item{It follows from the fact that the nullspace of $A$ is the trivial kernel that $A$ is an injection (\autoref{coro:linear-transformation-is-injective-iff-kernel-is-trivial}).}
			\item{It also follows from the fact that the nullspace of $A$ is the trivial kernel that the nullity of $A$ is 0 and so the dimension formula for finite-dimensional linear transformations (\autoref{theo:linear_map_dimension_formula}) tells us that
				\[ n = \operatorname{rank} A + \operatorname{nullity} A = \operatorname{rank} A + 0 \implies \operatorname{rank} A = n. \]
			}
		\end{enumerate}
		It follows from the fact that $A$ has full column rank, by \autoref{prop:properties-of-full-column-rank-matrix-product-with-hermitian-conjugate}, ${ A^T A }$ is invertible and so
		\[ L = (A^T A) A^T \]
		is a left inverse of $A$.
	\end{proof}
	\begin{corollary}
		If $A$ is an ${ m \times n }$ matrix with ${ m < n }$ then $A$ cannot have a left inverse.
	\end{corollary}
	\begin{corollary}
		If $A$ is an ${ n \times n }$ square matrix with rank $n$ then there is a single unique left inverse equal to the right inverse. That's to say,
		\[ L = \inv{A} = R. \]
	\end{corollary}

	
	\bigskip
	\labeledProposition{Let $A$ be an ${ m \times n }$ matrix. Then $A$ has a \textbf{right} inverse iff:
		\begin{enumerate}[label=(\roman*)]
			\item{${ A\x = \b }$ has at least one solution for every ${ \b \in \F{m} }$;}
			\item{$A$ is a surjection: The range of $A$ is $\F{m}$;}
			\item{$A$ has rank $m$.}
		\end{enumerate}
	}{properties-of-matrix-with-right-inverse}
	\begin{proof}\nl[4]
		Let $R$ be a right inverse of $A$.
		\begin{enumerate}[label=(\roman*)]
			\item{Since, for every ${ \b \in \F{m} }$
				\[ A\x =\b \iff A\x = I_m\b = (AR)\b \iff A\x = A(R\b), \]
				we can deduce that there exists at least one solution, namely ${ \x = R\b }$. (Note that we can't deduce that this is the only solution because $A$ is not, in general, invertible.)
			}
			\item{It follows from the fact that, for every ${ \b \in \F{m} }$ there exists at least one ${ \x = R\b \in \F{n} }$ such that ${ A\x = \b }$ that $A$ is surjective and its range is all of $\F{m}$.
			}
			\item{Since the rank of $A$ is, by definition, the dimension of its range, it follows then, from the fact that the range of $A$ is $\F{m}$ that the rank of $A$ is $m$.
			}
		\end{enumerate}
		It follows from the fact that $A$ has full row rank that $A^T$ has full column rank and so, by \autoref{prop:properties-of-full-column-rank-matrix-product-with-hermitian-conjugate}, ${ (A^T)^T A^T = A A^T }$ is invertible and so
		\[ R = (A A^T) A \]
		is a right inverse of $A$.
	\end{proof}
	\begin{corollary}
		If $A$ is an ${ m \times n }$ matrix with ${ m > n }$ then $A$ cannot have a right inverse.
	\end{corollary}
	\begin{corollary}
		If $A$ is an ${ n \times n }$ square matrix with rank $n$ then there is a single unique right inverse equal to the left inverse. That's to say,
		\[ R = \inv{A} = L. \]
	\end{corollary}

	\sep
	\begin{exe}
		\ex{\TODO{LSE FLA Example 6.2}}
	\end{exe}



	\biggerskip
	\subsubsection{Weak and Strong Generalised Inverses}	
	\boxeddefinition{\textbf{(Weak Generalised Inverse)} Let $A$ be an ${ m \times n }$ matrix. A \textit{weak generalised inverse} (WGI) of $A$, denoted by $A^g$, is any ${ n \times m }$ matrix such that
		\[ A A^g A = A. \]
	}

	\medskip
	\labeledProposition{Both left and right inverses are WGIs.}{left-and-right-inverses-are-WGIs}
	\begin{proof}
		Let $A$ be an ${ m \times n }$ matrix.\\
		
		If $L$ is a left inverse of $A$ then
		\[ A L A = A I_n = A. \]
		If $R$ is a right inverse of $A$ then
		\[ A R A = I_m A = A. \]
	\end{proof}
}

\end{document}