\documentclass[../MathsNotesBase.tex]{subfiles}




\date{\vspace{-6ex}}


\begin{document}	
	\searchableSection{Differentiation}{analysis, differentiation}
	\bigskip\bigskip
	
	\searchableSubsection{\texorpdfstring{Differentiation as a Linear\\Transformation}{Differentiation as a Linear Transformation}}{analysis, differentiation}{\bigskip\bigskip
		
		\newcommand{\vtheta}{\V{\theta}}
		\newcommand{\valpha}{\V{\alpha}}
		\notation{The derivative of $y(x)$ with respect to $x$ will be denoted $\Dif_x y$ and of $f(x)$ with $\Dif_x f$.}
		\boxeddefinition{The set ${ P_n \subset \R{\R{}} }$ of all univariate real-valued polynomials ${ p: \R{} \longmapsto \R{} }$ of degree $n$ is defined as,
			\[ P_n = \setc{p \in \R{\R{}}}{p(x) = \sum_{i=0}^n \theta_i x^i, \hspace{10pt} \theta_i \in \R{}}. \]
			Or alternatively, in vector notation,
			\[ P_n = \setc{p \in \R{\R{}}}{p(x) = \vtheta^T \x, \hspace{10pt} \vtheta \in \R{n+1}} \]
			where $\x$ is the standard basis of the space of degree-$n$ polynomials,
			\[ (1,x,\dots,x^n)^T. \]
		}
		
		\bigskip
		\subsubsection{Linear Algebra of First-order Differential Equations}
		\bigskip
			\subsubsubsection{First Order}
		\begin{align*}
			&& \frac{dy}{dx} &= ax + b \\
			&\iff & \int \frac{dy}{dx} \dif x &= \int ax + b \dif x &\sidecomment{} \\
			&\iff & y &= a'x^2 + bx + c. &\sidecomment{${ a' = a/2}$, c is any constant}
		\end{align*}
		Integrating we see that a first order differential equation only determines the function upto a constant value. In order to determine a specific function we need a relation between a value of $x$ and a value of $y$ (i.e. a point, in graphical terms). Typically this is described as an initial condition.
		
		\bigskip\bigskip
		\subsubsubsection{Second Order}
		\begin{align*}
			&& \frac{d^2y}{dx^2} &= ax + b \\
			&\iff & \int \frac{d^2y}{dx^2} \dif x &= \int ax + b \dif x  &\sidecomment{} \\
			&\iff & \frac{dy}{dx} &= a'x^2 + bx + c  &\sidecomment{${ a' = a/2 }$, c is any constant} \\
			&\iff & \int \frac{dy}{dx} \dif x &= \int a'x^2 + bx + c \dif x  &\sidecomment{} \\
			&\iff & y &= a''x^3 + b'x^2 + cx + d. &\sidecomment{${ a'' = a/6,\, b' = b/2}$, d is any constant}
		\end{align*}
		Integrating a second order equation twice we see that we introduced two constants of integration, ${ c,d }$, and the last two terms ${ cx + d }$ are undetermined. So a second order equation of this type has only determined a function upto a first-degree polynomial (a line). To determine a specific function, in this case, we require two relations between $x$ and $y$ (two points determine a line).
		
		\biggerskip
		The derivative $\Dif_x p(x)$, of the univariate degree-$n$ polynomial ${ p \in P_n }$ can be described as a linear transformation as,
			\[ \Dif_x p(x) = (A\vtheta)^T \x = \x^T A\vtheta \]
			where $A$ is the ${ n \times (n+1) }$ matrix,
			\[ A = \begin{bmatrix}
					0 & 1 & 0 & \cdots & 0\\
					0 & 0 & 2 & \cdots & 0\\
					\vdots &  &  &  & \vdots\\
					0 & 0 & 0 & \cdots & n
					\end{bmatrix}.
			\]
		The matrix $A$ clearly has rank $n$ and a 1-dimensional kernel so $\Dif_x$ transforms from $n+1$-space to $n$-space. The nullspace of $A$ being,
		\[ nullspace(A) = t\begin{bmatrix}1\\0\\\vdots\\0\end{bmatrix} \hspace{20pt} t \in \R{} \]
		the interpretation of which is that the derivative of constant polynomials is zero.\\\\

		
		\medskip
		In general the differential equation,
		\begin{equation}
			\Dif_x y(x) = \alpha_0 + \alpha_1 x + \cdots + \alpha_{n-1} x^{n-1}
		\end{equation}
		has the vector form,
		\begin{equation}\label{eq:first-order-lin-diffeq}
			\x^T A\vtheta = \x^T \valpha \iff A\vtheta = \valpha
		\end{equation}
		where the solution is
		\[ y(x) = \theta_0 + \theta_1 x + \cdots + \theta_n x^n = \x^T \vtheta. \]
		\note{Note that we can say
			\[ \x^T A\vtheta = \x^T \valpha \iff A\vtheta = \valpha \]
			because $\x^T$ is a basis and therefore a linearly independent set. By linear independence, the equation on the left implies that the coefficients are equal.\\\\
			This is equivalent to analysing $P_n$, the ${ (n+1) }$-dimensional vector space of polynomials, by working in the coordinate space formed by the coefficients. This space is $\R{n+1}$ which is isomorphic to any ${ (n+1) }$-dimensional vector space by \autoref{prop:vector_space_isomorphic_to_coordinate_space_of_same_dimension}. 
		}
		
		\smallskip
		The matrix form of \autoref{eq:first-order-lin-diffeq} is,
		\begin{equation}\label{eq:matrix-general-first-order-lin-diff-eq}
		\begin{bmatrix}
		0 & 1 & 0 & \cdots & \cdots & 0\\
		0 & 0 & 2 & \cdots & \cdots & 0\\
		\vdots &  &  &  &  & \vdots\\
		0 & 0 & 0 & \cdots & n-1 & 0\\
		0 & 0 & 0 & \cdots & \cdots & n
		\end{bmatrix}\begin{bmatrix}\theta_0\\\theta_1\\\vdots\\\vdots\\\vdots\\\theta_n\end{bmatrix} = \begin{bmatrix}\alpha_0\\\alpha_1\\\alpha_2\\\vdots\\\alpha_{n-1}\end{bmatrix}.
		\end{equation}
		Constructing the augmented matrix and using Gaussian Elimination we obtain,
		\begin{align*}
		&\begin{amatrix}{6}
		0 & 1 & 0 & \cdots & \cdots & 0 & \alpha_0\\
		0 & 0 & 2 & \cdots & \cdots & 0 & \alpha_1\\
		\vdots &  &  &  &  &  & \vdots\\
		0 & 0 & 0 & \cdots & n-1 & 0 & \alpha_{n-2}\\
		0 & 0 & 0 & \cdots & \cdots & n & \alpha_{n-1}
		\end{amatrix} \\[20pt]
		\leadsto &\begin{amatrix}{6}
		0 & 1 & 0 & \cdots & \cdots & 0 & \alpha_0\\
		0 & 0 & 1 & \cdots & \cdots & 0 & \alpha_1/2\\
		\vdots &  &  &  &  &  & \vdots\\
		0 & 0 & 0 & \cdots & 1 & 0 & \alpha_{n-2}/n-1\\
		0 & 0 & 0 & \cdots & \cdots & 1 & \alpha_{n-1}/n
		\end{amatrix}
		\end{align*}
		so that $\theta_0$ is a free variable and we can read off the other values of the coefficients $\theta_i$ corresponding to the columns of the matrix as follows:
		\begin{equation}\label{eq:matrix-general-first-order-lin-diff-eq-soln}
			\begin{bmatrix}\theta_0\\\theta_1\\\theta_2\\\vdots\\\theta_{n-1}\\\theta_n\end{bmatrix} =
			\begin{bmatrix}t\\\alpha_0\\\alpha_1/2\\\vdots\\\alpha_{n-2}/n-1\\\alpha_{n-1}/n\end{bmatrix} \hspace{20pt} \text{for } t \in \R{}
		\end{equation}
		and this implies that the solution to the differential equation is:
		\begin{equation}\label{eq:general-first-order-lin-diff-eq-soln}
			y(x) = t + \alpha_0 x + \frac{\alpha_1}{2} x^2 + \cdots + \frac{\alpha_{n-1}}{n} x^n, \hspace{20pt} t \in \R{}.
		\end{equation}
		We can rewrite the result in \autoref{eq:matrix-general-first-order-lin-diff-eq-soln} as,
		\begin{equation}
			\begin{bmatrix}0\\\alpha_0\\\alpha_1/2\\\vdots\\\alpha_{n-2}/n-1\\\alpha_{n-1}/n\end{bmatrix} + t\begin{bmatrix}1\\0\\0\\\vdots\\0\\0\end{bmatrix}
			\hspace{20pt} \text{for } t \in \R{}
		\end{equation}
		which makes it clear that we have a particular solution (with ${ t = 0 }$) plus a 1-dimensional nullspace. However, in practical applications modeled by differential equations, there will typically be an initial condition --- an initial value of $y$ such as $y(0)$ for example --- and this will be used to determine a particular solution. In this situation (known as IVP or Initial Value Problems) the particular solution involves finding a value of the parameter $t$ that fits the initial condition. For this reason, the solution in \autoref{eq:general-first-order-lin-diff-eq-soln} is referred to as the general solution of the differential equation while the particular solution has a particular value of the parameter $t$.\\\\
		
		\bigskip
		\subsubsection{Linear Algebra of Second-order Differential Equations}
		\bigskip
		The most simple type of second-order linear differential equation looks like,
		\begin{equation}\label{eq:eq:second-order-lin-diffeq}
			 \Dif_x^2 y(x) = \alpha_0 + \alpha_1 x + \cdots + \alpha_{n-2} x^{n-2}.
		\end{equation}
		We can add an extra row of zeros to the matrix of $\Dif_x$ to make it square so that we can take powers of it,
		\[ A = \begin{bmatrix}
				0 & 1 & 0 & \cdots & 0\\
				0 & 0 & 2 & \cdots & 0\\
				\vdots &  &  &  & \vdots\\
				0 & 0 & 0 & \cdots & n \\
				0 & 0 & 0 & \cdots & 0
				\end{bmatrix}
		\]
		and output an extra coefficient with value zero.\\
		
		Then the vector form of \autoref{eq:eq:second-order-lin-diffeq} is
		\begin{equation}
			A^2 \vtheta = \valpha
		\end{equation}
		and the matrix form is
		\begin{equation}
			\begin{bmatrix}
			0 & 0 & 2 & 0 & \cdots & 0\\
			0 & 0 & 0 & 6 & \cdots & 0\\
			\vdots &  &  &  &  & \vdots\\
			0 & 0 & 0 & 0 & \cdots & n(n-1)\\
			0 & 0 & 0 & 0 & \cdots & 0\\
			0 & 0 & 0 & 0 & \cdots & 0
			\end{bmatrix}\begin{bmatrix}\theta_0\\\theta_1\\\vdots\\\vdots\\\vdots\\\theta_n\end{bmatrix} = \begin{bmatrix}\alpha_0\\\alpha_1\\\vdots\\\alpha_{n-2}\\0\\0\end{bmatrix}
		\end{equation}
		which gives the solution to the differential equation as:
		\begin{equation}\label{eq:general-first-order-lin-diff-eq-soln}
		y(x) = s + tx + \frac{\alpha_0}{2} x^2 + \frac{\alpha_1}{6} x^3 + \cdots + \frac{\alpha_{n-2}}{n(n-1)} x^n, \hspace{20pt} s,t \in \R{}.
		\end{equation}
		So, here the kernel is 2-dimensional and we need two initial values to determine a particular solution.
		
		\bigskip
		\subsubsection{Eigenvectors of the Differentiation Operator}\label{sss:eigenvectors-of-differentiation}
		\bigskip
		If we had the differential equation,
		\begin{equation}
		\Dif_x y(x) = \alpha_0 + \alpha_1 x + \alpha_2 x^2 + \alpha_3 x^3 + \cdots
		\end{equation}
		so that the derivative of $y(x)$ is an infinite power series (not an infinite polynomial as polynomials are by definition finite), then the general solution would take the form, for ${ t \in \R{} }$,
		\begin{equation}
			y(x) = t + \alpha_0 x + \frac{\alpha_1}{2} x^2 + \frac{\alpha_2}{3} x^3 + \cdots
		\end{equation}
		So, if we had
		\[ \alpha_0 = \alpha_1,\, \frac{\alpha_1}{2} = \alpha_2,\, \frac{\alpha_2}{3} = \alpha_3 \]
		then the general solution would be, for ${ t \in \R{} }$,
		\begin{equation}
			y(x) = t + \alpha_1 x + \alpha_2 x^2 + \alpha_3 x^3 + \cdots
		\end{equation}
		which is a family of functions which includes the derivative ${ \Dif_x y(x) }$ --- the derivative being the member of this set of functions with ${ t = \alpha_0 }$.\\
		
		\note{Note that, for polynomials, the derivative is a transformation between finite-dimensional vector spaces and so the Dimension Formula (\autoref{theo:linear_map_dimension_formula}) of linear transformations applies. We can see this in that the derivative has a one-dimensional kernel (the set of constant polynomials) and the derivative maps from $P_n$ to $P_{n-1}$, the image having one less dimension then the domain space because there is a one-dimensional kernel.\\
			However, the coordinate space of the power series above is isomorphic to an infinite-dimensional vector space where the Dimension Formula no longer applies (see \autoref{theo:no-dimension-formula-for-infinite-vector-spaces}). So when we differentiate it we get a result of the same dimensionality despite there being a non-trivial one-dimensional kernel.
		}
		
		In order to achieve this we need the coefficients to form the series,
		\begin{align*}
		&\alpha_0,\; \frac{\alpha_0}{1},\; \frac{\alpha_0}{1 \times 2},\; \frac{\alpha_0}{1 \times 2 \times 3},\; \dots
		\end{align*}
		so that
		\begin{align*}
		y(x) &= \alpha_0 + \frac{\alpha_0}{1}x + \frac{\alpha_0}{1 \times 2}x^2 + \frac{\alpha_0}{1 \times 2 \times 3}x^3 + \dots \\[8pt]
		&= \alpha_0 + \frac{\alpha_0}{1!}x + \frac{\alpha_0}{2!}x^2 + \frac{\alpha_0}{3!}x^3 + \dots &\sidecomment{} \\[8pt]
		&= \alpha_0\left(1 + \frac{x}{1!} + \frac{x^2}{2!} + \frac{x^3}{3!} + \dots \right) = \alpha_0 e^x.
		\end{align*}
		So, the general solution is,
		\[ y(x) = t e^x \hspace{20pt} t \in \R{} \]
		and the particular solution, for ${ t = \alpha_0 }$, is
		\[ y(x) = \alpha_0 e^x. \]
		
		\bigskip\bigskip
		\labeledTheorem{The derivative of $e^{f(x)}$ is ${ f'(x)e^{f(x)} }$ where $f'$ is the derivative of the function $f$.}{derivative-of-exp-function}
		\begin{proof}
			\begin{align*}
			&& e^{f(x)} &= 1 + \frac{f(x)}{1!} + \frac{(f(x))^2}{2!} + \frac{(f(x))^3}{3!} + \cdots \\[8pt]
			&\implies & \frac{\dif}{\dif x} e^{f(x)} &= f'(x) + f'(x)\frac{f(x)}{1!} + f'(x)\frac{(f(x))^2}{2!} + f'(x)\frac{(f(x))^3}{3!} + \cdots &\sidecomment{} \\[8pt]
			&&&= f'(x) e^{f(x)}.
			\end{align*}
		\end{proof}
		\begin{corollary}
			Any function of the form $Ae^{f(x)}$ is an eigenfunction of the differentiation operator with eigenvalue equal to $f'(x)$.
		\end{corollary}
		\begin{proof}
			\[ \frac{\dif}{\dif x} Ae^{f(x)} = A \frac{\dif}{\dif x} e^{f(x)} = A f'(x)e^{f(x)} = f'(x)(Ae^{f(x)}). \qedhere \]
		\end{proof}
		\begin{corollary}
			The set of functions $e^{\int f'(x) \dif x}$ form an eigenspace of the differentiation operator with eigenvalue $f'(x)$.
		\end{corollary}
	
		\bigskip
		\labeledProposition{For any ${ a,x \in \R{} }$, \[ \frac{\dif}{\dif x} a^x = (\ln{a}) a^x. \]}{derivative-of-a-to-power-x-is-ln-a-times-a-to-power-x}
		\begin{proof}
			\begin{align*}
			\frac{\dif}{\dif x}a^x &= \frac{\dif}{\dif x} e^{(\ln{a})x} &\sidecomment{by \autoref{prop:exp-x-is-inverse-of-ln}}\\[8pt]
			&= (\ln{a})e^{(\ln{a})x} &\sidecomment{by \autoref{theo:derivative-of-exp-function}} \\[8pt]
			&= (\ln{a})a^x &\sidecomment{by \autoref{prop:exp-x-is-inverse-of-ln}.} \qedhere\\[8pt]
			\end{align*}
		\end{proof}
	
	
		\bigskip
		\subsubsection{Problems with this approach to Differentiation}
		\bigskip
		We can describe polynomial functions as finite vectors defined against the basis of monomials ($1,x,x^2,\dots$) but to describe transcendental functions such as $e^x$ in the same manner we would need an infinite linear combination of monomials which cannot be generally defined to create a vector space of such vectors.
		
		\sep
		\subsubsection{The Polynomial Differential Operator}\label{sssection:polynomial-differential-operator}
		\bigskip
		\boxeddefinition{The $n$-th degree \textbf{polynomial differential operator} is defined as 
			\[ L = a_n D^n + a_{n-1} D^{n-1} + \cdots + a_1 D + a_0 \]
			so that, for example, a second-degree operator may be used to define the homogeneous differential equation
			\[ a_2 y'' + a_1 y' + a_0 y = 0 \]
			as
			\[ L y = 0 = a_2 D^2 y + a_1 D y + a_0 y. \]
		}
		\note{We can form polynomials of the differential operator $D$ because, as a linear operator, we can use \ref{defn:polynomials-of-linear-operators}. However, it should be noted that, if ${ u, v }$ are both functions of $t$, ${ u(t), v(t) }$,
			\[ Duv = v \centernot\implies Du = 1. \]
			This seems obvious because,
			\[ Duv = (Du)v + u(Dv) \]
			but some implications may not be so obvious. For example,
			\[ (D - a) uv = v \centernot\implies (D - a) u = 1. \]
		}
	}
\end{document}