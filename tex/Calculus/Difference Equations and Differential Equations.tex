\documentclass[../MathsNotesBase.tex]{subfiles}

\date{\vspace{-6ex}}

\begin{document}
	\searchableSection{\texorpdfstring{Difference and Differential\\ Equations}{Difference and Differential Equations}}{differential equations, difference equations}
	\bigskip\bigskip
	
	\searchableSubsection{First-order Difference Equations}{difference equations}{
		\bigskip\bigskip
		\note{A \textbf{difference equation} is also known as a \textbf{recurrence equation}.}
		
		\boxeddefinition{Let ${ y_t }$ be the $t$-th value in a sequence (typically $t$ represents time). Then,
			\[ y_t = ay_{t-1} + b, \hspace{20pt} t \geq 1 \]
			is called a \textbf{first-order linear difference equation with constant coefficients}. The value $y_0$ is called an initial condition.\\
			
			A solution to such an equation is an explicit -- or a closed-form (see: \href{https://en.wikipedia.org/wiki/Closed-form_expression}{wikipedia}) -- expression for $y_t$ in terms of $t$ and $y_0$.\\
			
			If ${ b = 0 }$ we have,
			\[ y_t = ay_{t-1} \iff y_t - ay_{t-1} = 0 \]
			which is known as a \textbf{homogeneous} first-order linear difference equation with constant coefficients.
		}
	
		\bigskip
		\labeledProposition{A first-order linear difference equation with constant coefficients of the form ${ y_t = ay_{t-1} + b }$ where ${ a = 1 }$ is a arithmetic progression.
		}{first-order-lin-diff-eqn-const-coeff-1-multiplier-is-arith-prog}
		\begin{proof}
			Let ${ y_t = y_{t-1} + b }$ then,
			\[ y_1 = y_0 + b,\, y_2 = y_1 + b = (y_0 + b) + b = y_0 + 2b,\, y_3 = y_0 + 3b, \dots \]
			so we have ${\bm{ y_t = y_0 + tb }}$. If we describe this as an arithmetic progression we have,
			\[ x_n = a + nd \]
			where the zeroth term $a$ corresponds to $y_0$, the common difference $d$ corresponds to $b$ and, clearly, $t$ and $n$ are both term indices.
		\end{proof}
		
		\bigskip\medskip
		\labeledProposition{A first-order linear difference equation with constant coefficients of the form ${ y_t = ay_{t-1} + b }$ where ${ b = 0 }$ is a geometric progression.}{first-order-lin-diff-eqn-const-coeff-0-const-term-is-geom-prog}
		\begin{proof}
			Let ${ y_t = ay_{t-1} + 0 }$ then,
			\[ y_1 = ay_0,\, y_2 = ay_1 = a(ay_0) = a^2y_0,\, y_3 = a^3y_0, \dots \]
			so we have ${\bm{ y_t = a^ty_0 }}$. If we describe this as a geometric progression we have,
			\[ x_n = ar^n \]
			where the zeroth term $a$ corresponds to $y_0$, the common ratio $r$ corresponds to $a$, and $n$ is the term index corresponding to $t$.
		\end{proof}
	
		\bigskip\medskip
		\labeledProposition{A first-order linear difference equation with constant coefficients of the form ${ y_t = ay_{t-1} + b }$ where ${ a \neq 1 }$ and ${ b \neq 0 }$ has solution
			\begin{align*}
			y_t &= a^t y_0 + b(a^{t-1} + a^{t-2} + \cdots + a + 1) \\
			&= a^t y_0 + b \sum_{i=0}^{t-1} a^i.
			\end{align*}
		}{first-order-lin-diff-eqn-const-coeff-soln-formula}
		\begin{proof}
			\begin{align*}
			y_t &= a y_{t-1} + b \\
			&= a(a y_{t-2} + b) + b = a^2 y_{t-2} + ab + b\\
			&= a^2 (a(y_{t-3} + b)) + ab + b = a^3 y_{t-3} + a^2 b + ab + b \\
			&= a^3 y_{t-3} + b(a^2 + a + 1) \\
			&= a^t y_0 + b(a^{t-1} + \cdots + a + 1).  \qedhere  
			\end{align*}
		\end{proof}
	
		\bigskip\medskip
		\labeledProposition{A first-order linear difference equation with constant coefficients of the form ${ y_t = ay_{t-1} + b }$ where ${ a \neq 1 }$ and ${ b \neq 0 }$ has solution
			\[ y_t = a^t \left(y_0 - \frac{b}{1-a}\right) + \frac{b}{1-a} \]
			where the value
			\[ \frac{b}{1-a} = y^* = a y^* + b \]
			is the \textbf{equilibrium} or \textbf{steady-state} value of the recurrence.
		}{first-order-lin-diff-eqn-const-coeff-soln-formula-wrt-steady-state-value}
		\begin{proof}
			From \autoref{prop:first-order-lin-diff-eqn-const-coeff-soln-formula} we know that the solution to the given recurrence is
			\[ y_t = a^t y_0 + b \sum_{i=0}^{t-1} a^i. \]
			The summation in the second term is the sum of a geometric progression,
			\begin{align*}
			\sum_{i=0}^{t-1} a^i = 1 + a + \cdots + a^{t-1} &= \frac{a^t - 1}{a - 1} \\[8pt]
			&= \frac{a^t}{a - 1} - \frac{1}{a - 1} \\[8pt]
			&= \frac{1}{1 - a} - \frac{a^t}{1 - a}.
			\end{align*}
			So we see that the sum of a geometric progression can be separated into two terms: one term depends on the number of elements in the progression (here $t$), and the other term does not. This is the explanation of the sum of convergent geometric series: if ${ \abs{a} < 1 }$ then, when ${ t \to \infty }$, the $t$-dependent term disappears leaving only the steady-state term.\\
			
			The solution to the recurrence therefore becomes
			\begin{align*}
			y_t &= a^t y_0 + b \left(\frac{1}{1 - a} - \frac{a^t}{1 - a}\right) \\[8pt]
			&= a^t \left(y_0 - \frac{b}{1-a}\right) + \frac{b}{1-a}. \qedhere \\
			\end{align*}
		\end{proof}
	
	
		\biggerskip
		\subsubsection{First-order Linear Difference Equations as Affine Transformations}
		\def\y{\V{y}}
		If we define a vectorized linear difference equation with constant coefficients,
		\[ \y_t = A\y_{t-1} + \b \]
		where $A$ is a matrix, then clearly $\y_t$ is an affine transformation of $\y_{t-1}$.\\
		
		We can linearize this by adding an extra dimension that takes the value 1 like so:
		\[	\begin{bmatrix}
				\y_t\\
				1
			\end{bmatrix} =
			\begin{bmatrix}
				A & \b\\
				0 & 1
	  		\end{bmatrix}
			\begin{bmatrix}
				\y_{t-1}\\
				1
			\end{bmatrix}.
		\]
		
		\bigskip
		A minimal, one-by-one matrix is just a scalar (see: \ref{ex:minimal-linear-transformation}) and a minimal vector can be just a field element (see: \ref{sssection:vectors-as-magnitude-and-direction}) so we can define ${ A = a }$ such that the linearized equation becomes
		\begin{align*}
			\begin{bmatrix}
				y_t\\
				1
			\end{bmatrix} &=
			\begin{bmatrix}
				a & b\\
				0 & 1
			\end{bmatrix}
			\begin{bmatrix}
				y_{t-1}\\
				1
			\end{bmatrix} \\[8pt]
			&=
			\begin{bmatrix}
				a & b\\
				0 & 1
			\end{bmatrix}^t
			\begin{bmatrix}
				y_0\\
				1
			\end{bmatrix}.
		\end{align*}
	
		\bigskip
		\subsubsubsection{Eigenvectors of the Transformation}
		In order to easily take powers of the transformation we find the eigenvectors.\\
		
		Let $M$ be the transformation matrix,
		\[ M = \begin{bmatrix}
				a & b\\
				0 & 1
			\end{bmatrix}
		\]
		
		and, for an eigenvector $\v$,
		\begin{align*}
		&& M \v &= \lambda \v \\
		&\iff & 
		\begin{bmatrix}
		a & b\\
		0 & 1
		\end{bmatrix} \v &= \lambda \v \\[8pt]
		&\iff & \left(
		\begin{bmatrix}
		a & b\\
		0 & 1
		\end{bmatrix} - \lambda I \right) \v &= \0 &\sidecomment{} \\[8pt]
		&\iff &
		\begin{bmatrix}
		a-\lambda & b\\
		0 & 1-\lambda
		\end{bmatrix} \v &= \0 &\sidecomment{} \\[8pt]
		\end{align*}
	
		\begin{align*}
		&& 
		\begin{vmatrix}
		a-\lambda & b\\
		0 & 1-\lambda
		\end{vmatrix} &= 0 \\[8pt]
		&\iff & (a - \lambda)(1 - \lambda) &= 0 &\sidecomment{} \\[8pt]
		&\iff & \lambda \in \{1, a\}.
		\end{align*}
		
		So, for eigenvalue 1:
		\begin{align*}
		&& \begin{bmatrix}
		a-1 & b\\
		0 & 0
		\end{bmatrix} \begin{bmatrix}v_1\\v_2\end{bmatrix} &= \begin{bmatrix}0\\0\end{bmatrix} \\[8pt]
		&\iff & (a - 1)v_1 + b v_2 &= 0 &\sidecomment{} \\[8pt]
		&\iff & (a - 1)v_1 &= -b &\sidecomment{letting ${ v_2 = 1 }$} \\[8pt]
		&\iff & v_1 &= \frac{-b}{a - 1} = \frac{b}{1 - a}\\[8pt]
		&\therefore & \v &= \begin{bmatrix}\frac{b}{1 - a}\\1\end{bmatrix}.
		\end{align*}
		\note{Note that ${ \frac{b}{1 - a} }$ is the steady-state solution of the difference equation.}
		
		For eigenvalue $a$:
		\begin{align*}
		&& \begin{bmatrix}
		0 & b\\
		0 & 1-a
		\end{bmatrix} \begin{bmatrix}v_1\\v_2\end{bmatrix} &= \begin{bmatrix}0\\0\end{bmatrix}  \\[8pt]
		&\iff & v_2 &= 0 &\sidecomment{}\\[8pt]
		&\therefore & \v &= c\begin{bmatrix}1\\0\end{bmatrix} \text{ for } c \in \R{}.
		\end{align*}
		\note{Note that the second component of this vector adds the translation of the affine transformation so this is telling us that if ${ b = 0 }$ then any vector is an eigenvector with eigenvalue $a$. This corresponds to the homogeneous solution.}
		
		\subsubsubsection{Diagonalization}
		\bigskip
		So, if $B$ is the set of two eigenvectors
		\[ \left\{ \begin{bmatrix}\frac{b}{1 - a}\\1\end{bmatrix}, \begin{bmatrix}1\\0\end{bmatrix} \right\} \]
		and we take this as a basis, then the change of basis matrix to this basis is
		\[ P = \inv{[B]} = \inv{\begin{bmatrix}\frac{b}{1 - a} & 1\\1 & 0\end{bmatrix}} 
			= \begin{bmatrix}0 & 1\\1 & \frac{-b}{1 - a}\end{bmatrix} 
		\]
		and the diagonal matrix that represents the transformation with respect to this basis is
		\begin{align*}
		D &= PM\inv{P} \\
		&= \begin{bmatrix}0 & 1\\1 & \frac{-b}{1 - a}\end{bmatrix}
		\begin{bmatrix}
		a & b\\
		0 & 1
		\end{bmatrix}
		\begin{bmatrix}\frac{b}{1 - a} & 1\\1 & 0\end{bmatrix}\\
		&= \begin{bmatrix}0 & 1\\1 & \frac{-b}{1 - a}\end{bmatrix}
		\begin{bmatrix}\frac{b}{1 - a} & a\\1 & 0\end{bmatrix}\\
		&= \begin{bmatrix}1 & 0\\0 & a\end{bmatrix}.
		\end{align*}

		Since $D$ is diagonal we have
		\[ {D}^t = \begin{bmatrix}1 & 0\\0 & a^t\end{bmatrix}. \]
		So, in order to calculate,
		\[ M^t = \begin{bmatrix}
					a & b\\
					0 & 1
				 \end{bmatrix}^t
		 \]
		 we can calculate,
		 \begin{align*}
		 && {D}^t &= (PM\inv{P})^t = PM^t\inv{P} \\
		 &\iff & \inv{P}{D}^tP &= M^t. \\
		 \end{align*}
		 
		 \begin{align*}
		 &\therefore & M^t &= 
		 	\begin{bmatrix}\frac{b}{1 - a} & 1\\1 & 0\end{bmatrix} 
		 	\begin{bmatrix}1 & 0\\0 & a^t\end{bmatrix}
		 	\begin{bmatrix}0 & 1\\1 & \frac{-b}{1 - a}\end{bmatrix} \\
		 &\iff & M^t &= 
		 	\begin{bmatrix}\frac{b}{1 - a} & 1\\1 & 0\end{bmatrix}
		 	\begin{bmatrix}0 & 1\\a^t & \frac{-a^t b}{1 - a}\end{bmatrix} \\
		 &\iff & M^t &= 
		 	\begin{bmatrix}a^t & \frac{b(1 - a^t)}{1 - a}\\0 & 1\end{bmatrix}. \\
		 \end{align*}
		 
		 So, the solution to the first-order linear recurrence is found to be
		 \begin{align*}
		 \begin{bmatrix}
		 y_t\\
		 1
		 \end{bmatrix} &=
		 \begin{bmatrix}
		 a & b\\
		 0 & 1
		 \end{bmatrix}^t
		 \begin{bmatrix}
		 y_0\\
		 1
		 \end{bmatrix} \\
		 &= \begin{bmatrix}a^t & \frac{b(1 - a^t)}{1 - a}\\0 & 1\end{bmatrix} 
		 \begin{bmatrix}
		 y_0\\
		 1
		 \end{bmatrix}
		 \end{align*}
		 
		 so that 
		 \[ y_t = a^t y_0 + \frac{b(1 - a^t)}{1 - a} = a^t\left(y_0 - \frac{b}{1 - a}\right) + \frac{b}{1 - a}. \]
	
		\biggerskip
		\subsubsection{Difference as Rate of change}
		\biggerskip
		\textbf{Geometric progression}: Say we have a first-order linear difference equation with constant coefficients of the form ${ y_t = ay_{t-1} + b }$ where ${ b = 0 }$ so that the terms follow a geometric progression:
		\[ y_1 = ay_0,\, y_2 = a^2y_0,\, y_3 = a^3y_0, \dots \]
		Then the rate of change is
		\[ \frac{\Delta y}{\Delta t} = y_t - y_{t-1} = ay_{t-1} - y_{t-1} = (a - 1)y_{t-1}. \]
		\note{Note that the ratio of consecutive terms remains constant,
			\[ \frac{y_t}{y_{t-1}} = a \]
			which is the "geometric" characteristic of a geometric progression, but the rate of change is proportional to the value.
		}
		The rate of change of the rate of change is, therefore,
		\[ \frac{\Delta^2 y}{\Delta t^2} = (a - 1)\frac{\Delta y_{t-1}}{\Delta t} = (a - 1)\frac{\Delta y}{\Delta t} = (a - 1)^2 y_{t-1}. \]
	
		\bigskip
		\subsubsection{Examples of first-order linear difference equations w/ const. coefficients}
		\begin{exe}
			\item{Let $y_t$ be an account balance after $t$ years and $r$ be the annual interest rate paid on the account. Suppose also, that the interest is compounded $n$ times per year. Then the formula for $y_t$ is,
				\[ y_t = \left(1 + \frac{r}{n}\right)^n y_{t-1} = y_0 \left(1 + \frac{r}{n}\right)^{nt}. \]
				The rate of change per year is,
				\[ \frac{\Delta y}{\Delta t} = y_t - y_{t-1} = \left(1 + \frac{r}{n}\right)^n y_{t-1} - y_{t-1} = \left(\left(1 + \frac{r}{n}\right)^n - 1\right) y_{t-1} \]
				as expected for a geometric progression.\\
				
				If, instead, we let $t$ be continuous we can find the instantaneous rate of change by considering the values for ${ t, t + \Delta t }$,
				\begin{align*}
				\frac{\Delta y}{\Delta t} = \frac{y_{(t + \Delta t)} - y_t}{\Delta t} &=  \frac{y_0 \left(1 + \frac{r}{n}\right)^{n(t + \Delta t)} - y_0 \left(1 + \frac{r}{n}\right)^{nt}}{\Delta t} \\[8pt]
				&= \frac{y_0\left(1 + \frac{r}{n}\right)^{nt}\left(\left(1 + \frac{r}{n}\right)^{n\Delta t} - 1\right)}{\Delta t}
				\end{align*}
				and then letting ${ \Delta t \to 0 }$ to obtain,
				\begin{align*}
				\frac{\dif y}{\dif t} &= \lim_{\Delta t \to 0} \frac{y_0\left(1 + \frac{r}{n}\right)^{nt}\left(\left(1 + \frac{r}{n}\right)^{n\Delta t} - 1\right)}{\Delta t} \\[8pt]
				&= y_0\left(1 + \frac{r}{n}\right)^{nt} \lim_{\Delta t \to 0} \frac{\left(1 + \frac{r}{n}\right)^{n\Delta t} - 1}{\Delta t} &\sidecomment{} \\[8pt]
				&= y_0\left(1 + \frac{r}{n}\right)^{nt} \lim_{\Delta t \to 0} \left[\frac{\dif}{\dif (\Delta t)} \left(1 + \frac{r}{n}\right)^{n\Delta t}\right] &\sidecomment{by L'H\^{o}pital's rule} \\[8pt]
				&= y_0\left(1 + \frac{r}{n}\right)^{nt} \lim_{\Delta t \to 0} \left[n\ln{\left(1 + \frac{r}{n}\right)}\left(1 + \frac{r}{n}\right)^{n\Delta t}\right] &\sidecomment{by \autoref{prop:derivative-of-a-to-power-x-is-ln-a-times-a-to-power-x}} \\[8pt]
				&= ny_0 \ln{\left(1 + \frac{r}{n}\right)} \left(1 + \frac{r}{n}\right)^{nt}
				\end{align*}

				But the question is: How meaningful is this really? If the interest is being compounded only $n$ times per year and these are the only moments when the account balance changes, then change happens at certain discrete moments rather than continuously so is it really meaningful to talk about the instantaneous rate of change? We can recover the discrete-time rate of change per year from this instantaneous one by integrating over a year.\\
				
				Now suppose that $n$, the number of times per year the interest is compounded, goes to infinity. Then (see \ref{sssection:e-to-the-power-x}) we have,
				\[ y_t = y_0 e^{rt}. \]
				For discrete time we have,
				\[ \frac{\Delta y}{\Delta t} = y_0 e^{rt} - y_0 e^{r(t-1)} = y_0 e^{r(t-1)} (e^r - 1) = y_{t-1} (e^r - 1) \]
				which should be no surprise as we still have a geometric progression as
				\[ \frac{y_t}{y_{t-1}} = e^r. \]
				If we now let $t$ be continuous as before then, by a similar logic using L'H\^{o}pital's rule, the instantaneous rate of change obtained is
				\[ \frac{\dif y}{\dif x} = r y_0 e^{rt}. \]
				Note that this is wholly consistent with the discrete time version as we can obtain the discrete time rate of change per year from this instantaneous rate of change by integrating over a year as follows,
				\begin{align*}
				\int_{t-1}^t r y_0 e^{rx} \dif x &= r y_0 \int_{t-1}^t e^{rx} \dif x \\[8pt]
				&= r y_0 \left[\frac{e^{rx}}{r}\right]_{t-1}^t &\sidecomment{} \\[8pt]
				&= y_0 \left[e^{rx}\right]_{t-1}^t &\sidecomment{} \\[8pt]
				&= y_0 (e^{rt} - e^{r(t-1)}) &\sidecomment{} \\[8pt]
				&= y_0  e^{r(t-1)} (e^r - 1). &\sidecomment{}
				\end{align*}
				
				\bigskip
				Now suppose $b$ is deposited at the end of each year so that,
				\begin{align*}
					y_t &= \left(1 + \frac{r}{n}\right)^n y_{t-1} + b \\
					&=  \left(1 + \frac{r}{n}\right)^n \left[ \left(1 + \frac{r}{n}\right)^n y_{t-2} + b \right] + b \\
					&=  \left(1 + \frac{r}{n}\right)^{nt} y_0 + b \sum_{i=0}^{t-1} \left(1 + \frac{r}{n}\right)^{ni}.
				\end{align*}
				But if we, again, let the number of compounds $n$, go to infinity, then,
				\begin{align*}
					y_t &= e^{r} y_{t-1} + b \\
					&=  e^{r} ( e^{r} y_{t-2} + b ) + b \\[6pt]
					&=  y_0 e^{rt} + b \sum_{i=0}^{t-1} e^{ri} \\[6pt]
					&=  y_0 e^{rt} + b \left( \frac{e^{rt} - 1}{e^r - 1} \right) \\[6pt]
					&=  y_0 e^{rt} + b \frac{e^{rt}}{e^r - 1} - b \frac{1}{e^r - 1} \\[6pt]
					&=  \left( y_0 - \frac{b}{1 - e^r} \right) e^{rt} + \frac{b}{1 - e^r}
				\end{align*}	
				where ${ \frac{b}{1 - e^r} }$ is the steady-state value.
			}
		
			\biggerskip
			\item{Let $M_t$ be an account balance at the end of year $t$. Let $Q(t)$ be an amount that is deposited into the account (or withdrawn if the value is negative) one time, at the end of year $t$ and let the interest rate be a fixed rate of $I$. Then,
				\begin{align*}
					M_t &= I M_{t-1} + Q(t) \\
					&= I (I M_{t-2} + Q(t-1)) + Q(t) \\
					&= I (I (I M_{t-3} + Q(t-2)) + Q(t-1)) + Q(t) \\
					&= M_0 I^t + \sum_{i=0}^{t-1} Q(t-i) I^i
				\end{align*}
				and the rate of change is,
				\[ M_t - M_{t-1} = M_0 I^{t-1} (I - 1) + Q(1) I^{t-1} = I^{t-1} (M_0(I - 1) + Q(1)) \]
				which is proportional to $I^{t-1}$.\\
				
				\note{If the interest rate $I$, is also a function of $t$, then -- if we define ${ I(0) = 1 }$ -- we end up with:
					\[ M_t = M_0 \prod_{i=0}^t I(i) + \sum_{i=0}^{t-1} Q(t-i) \prod_{j=0}^i I(i) \]
					which is getting pretty messy. At this point it becomes easier to work with continuous time.
				}
			}
		\end{exe}
	
	}


% ------------------- 	
	
	\pagebreak
	\searchableSubsection{Second-order Difference Equations}{difference equations}{
		\bigskip
		
		\boxeddefinition{A recurrence equation of the form
			\[ y_t = a y_{t-1} + b y_{t-2} + c \]
			where ${ a,b,c \in \R{} }$, is known as a \textbf{second-order linear recurrence with constant coefficients}.
		}
		Following the same approach as with the first-order equations we have,
		\[
			\begin{bmatrix}y_t\\y_{t-1}\\1\end{bmatrix} =
			\begin{bmatrix}
				a & b & c\\
				1 & 0 & 0\\
				0 & 0 & 1
			\end{bmatrix}
			\begin{bmatrix}y_{t-1}\\y_{t-2}\\1\end{bmatrix}.
		\]
		Determining eigenvalues we get,
		\begin{align*}
		&&	\begin{vmatrix}
				a-\lambda & b & c\\
				1 & -\lambda & 0\\
				0 & 0 & 1-\lambda
			\end{vmatrix} &= 0 \\[3pt]
		&\iff & (1-\lambda)((-\lambda)(a-\lambda) - b) &= 0 &\sidecomment{} \\[3pt]
		&\therefore & \lambda \in \left\{ 1, \frac{a + \sqrt{a^2 + 4b}}{2}, \frac{a - \sqrt{a^2 + 4b}}{2} \right\}.
		\end{align*}
		
		Due to the translation represented by the lone 1 in the final row of the matrix, there will always be the eigenvalue 1. This corresponds to the steady state value which may be thought of as the eigenvector of the translation. The other two eigenvalues are the eigenvectors of the linear transformation part of the affine transformation. The block structure is as follows.
		
		\begin{align*}
		&&	\begin{bmatrix}
			\begin{bmatrix}a & b\\1 & 0\end{bmatrix} & \begin{bmatrix}c\\0\end{bmatrix} \\
			\begin{bmatrix}0 & 0\end{bmatrix} & \begin{bmatrix}1\end{bmatrix}	
		\end{bmatrix}
		\begin{bmatrix}y_{t-1}\\y_{t-2}\\1\end{bmatrix} &= 
		\begin{bmatrix}a & b\\1 & 0\end{bmatrix}\begin{bmatrix}y_{t-1}\\y_{t-2}\end{bmatrix} +
		\begin{bmatrix}c\\0\end{bmatrix} \\
		&&  &= A\V{y} + \V{t}. \\
		\end{align*}
		
		\subsubsubsection{Steady-state value}
		Considering the case of the eigenvalue ${ \lambda = 1 }$ we have,
		\[
			\begin{bmatrix}
				a-1 &  b & c \\
				1   & -1 & 0 \\
				0   &  0 & 0
			\end{bmatrix}
			\begin{bmatrix}y_{t-1}\\y_{t-2}\\1\end{bmatrix} =
			\begin{bmatrix}0\\0\\0\end{bmatrix}. 
	    \]
	    So, to find the nullspace of the matrix we can find the row-reduced echelon form,
	    \begin{align*}
	    && 	\begin{bmatrix}
		    	a-1 &  b & c \\
		    	1   & -1 & 0 \\
		    	0   &  0 & 0	
		    \end{bmatrix} &\leadsto
		    \begin{bmatrix}
			    1 &  \frac{b}{a-1} & \frac{c}{a-1} \\
			    1   & -1 & 0 \\
			    0   &  0 & 0	
		    \end{bmatrix} \\[4pt]
	    &\leadsto & 
		    \begin{bmatrix}
			    1 &  \frac{b}{a-1} & \frac{c}{a-1} \\
			    0  & -1-\frac{b}{a-1} & -\frac{c}{a-1} \\
			    0   &  0 & 0	
		    \end{bmatrix} &\leadsto 
		    \begin{bmatrix}
			    1 &  \frac{b}{a-1} & \frac{c}{a-1} \\
			    0   & 1 & \frac{c}{a+b-1} \\
			    0   &  0 & 0	
		    \end{bmatrix}  \\[4pt]
		&\leadsto &
			\begin{bmatrix}
				1 & 0 & \frac{c}{a+b-1} \\
				0 & 1 & \frac{c}{a+b-1} \\
				0 & 0 & 0	
			\end{bmatrix}
	    \end{align*}
	    where the last step is because
	  	\begin{align*}
	  	\frac{c}{a-1} - \left(\frac{b}{a-1}\right)\frac{c}{a+b-1} &= \frac{c}{a-1}\left(1 - \frac{b}{a+b-1}\right) \\
	  	&= \frac{c}{a-1}\left(\frac{a-1}{a+b-1}\right). \\
	  	\end{align*} 
	  	
	  	So the nullspace is
	  	\[ t\begin{bmatrix}
		  		\frac{-c}{a+b-1} \\[4pt] \frac{-c}{a+b-1} \\[4pt] 1	
		  	\end{bmatrix} 
		\]
		for any ${ t \in \R{} }$ and the steady-state value is
		\[ \frac{-c}{a+b-1}. \]
		\note{This is also sometimes referred to as a \textbf{particular solution of the non-homogeneous equation}.}
		
		\bigskip
		\subsubsubsection{Eigenvalues of the linear transformation}
		Let the discriminant ${ d = \sqrt{a^2 + 4b} }$. Then the eigenvalues of the linear transformation are
		\[ \frac{a + d}{2} \eqand \frac{a - d}{2}. \]
		In the case of the eigenvalue ${ \frac{a + d}{2} }$ we have,
		\[
		\begin{bmatrix}
			\frac{a - d}{2} &  b & c \\
			1   & -(\frac{a + d}{2}) & 0 \\
			0   &  0 & 1-(\frac{a + d}{2})
		\end{bmatrix}
		\begin{bmatrix}y_{t-1}\\y_{t-2}\\1\end{bmatrix} =
		\begin{bmatrix}0\\0\\0\end{bmatrix}. 
		\]
		Again using row reduction to find the nullspace:
		\begin{align*}
		&& 	\begin{bmatrix}
			\frac{a - d}{2} &  b & c \\[4pt]
			1   & -(\frac{a + d}{2}) & 0 \\[4pt]
			0   &  0 & 1-(\frac{a + d}{2})	
		\end{bmatrix} &\leadsto
		\begin{bmatrix}
			1   & -(\frac{a + d}{2}) & \frac{2c}{a - d} \\[4pt]
			1   & -(\frac{a + d}{2}) & 0 \\[4pt]
			0   &  0 & 1-(\frac{a + d}{2})
		\end{bmatrix} &\sidecomment{using ${ \frac{2b}{a - d}\frac{a + d}{a + d} = -(\frac{a + d}{2}) }$}\\[4pt]
		&\leadsto & 
		\begin{bmatrix}
			1   & -(\frac{a + d}{2}) & \frac{2c}{a - d} \\[4pt]
			0   &  0 & \frac{-2c}{a - d} \\[4pt]
			0   &  0 & 1-(\frac{a + d}{2})	
		\end{bmatrix} &\leadsto 
		\begin{bmatrix}
			1   & -(\frac{a + d}{2}) & \frac{2c}{a - d} \\[4pt]
			0   &  0 & 1 \\[4pt]
			0   &  0 & 1	
		\end{bmatrix}  \\[4pt]
		&\leadsto &
		\begin{bmatrix}
			1   & -(\frac{a + d}{2}) & 0 \\[4pt]
			0   &  0 & 1 \\[4pt]
			0   &  0 & 0
		\end{bmatrix}
		\end{align*}
		
		So the nullspace is
		\[ t\begin{bmatrix}
				\frac{a + d}{2} \\[4pt] 1 \\[4pt] 0	
			\end{bmatrix} 
		\]
		for any ${ t \in \R{} }$ and the value
		\[ \frac{a + d}{2} = \frac{a + \sqrt{a^2 + 4b}}{2} \]
		is the solution to the homogeneous equation corresponding to the eigenvalue ${ (a+d)/2 }$. It's also not hard to see that the other eigenvalue ${ (a-d)/2 }$ results in the solution to the homogeneous equation
		\[ \frac{a - d}{2} = \frac{a - \sqrt{a^2 + 4b}}{2}. \]
		
		\medskip
		\note{Another common way of finding these solutions in this case is to form what is known as the \textbf{auxiliary equation} as:
			\begin{align*}
			&& y_t - a y_{t-1} - b y_{t-2} &= 0 \\
			&\leadsto & m^t - a m^{t-1} - b m^{t-2} &= 0 &\sidecomment{for some ${ m \neq 0 \in \R{} }$} \\
			&\iff & m^{t-2}(m^2 - a m - b) &= 0 &\sidecomment{} \\
			&\iff & m^2 - a m - b &= 0. &\sidecomment{since ${ m \neq 0 }$} \\
			\end{align*} 
		Note that this is the characteristic polynomial of the linear transformation part of the matrix:
		\[ (-\lambda)(a-\lambda) - b = \lambda^2 - a \lambda - b. \]}
	
		\medskip
		In this case, determining the final equation for ${ y_t }$ using matrix powers results in a very complex matrix and formula which is only reasonably performed on a computer. But we can infer that the formula will involve the eigenvalues of the linear transformation, raised to the power $t$, and the steady-state value,
		\[ y_t = C_1 \frac{(a+d)^t}{2^t} + C_2 \frac{(a-d)^t}{2^t} + \frac{c}{a+b-1}. \]
		
		Then we can use the initial values to solve for the constants ${ C_1, C_2 }$,
		\[ y_0 = C_1 + C_2 + \frac{c}{a+b-1} \eqand y_1 = C_1 \frac{a+d}{2} + C_2 \frac{a-d}{2} + \frac{c}{a+b-1}. \]
		
		The values 
		\[ \frac{a+d}{2} = \frac{a + \sqrt{a^2 + 4b}}{2} \eqand \frac{a-d}{2} = \frac{a - \sqrt{a^2 + 4b}}{2} \]
		may be two distinct real values, one real value (if ${ a^2 + 4b = 0 }$) or two distinct complex values (if ${ a^2 + 4b < 0 }$).
		
		\paragraph{In the case of one real value:} the formula becomes,
		\[ y_t = (C_1 + C_2 t) \frac{(a+d)^t}{2^t} + \frac{c}{a+b-1}. \]
		The explanation of this appears to be that the matrix ${ A - \lambda I }$ has cyclic order 2. \TODO{? eh?}
		
		\paragraph{In the case of two complex values:} expressing the eigenvalues in exponential form we have,
		\begin{align*}
		\frac{a}{2} \pm \frac{\sqrt{-a^2 - 4b}}{2} i &= \sqrt{ \frac{a^2}{4} + \frac{-a^2 - 4b}{4} }\, \exp\left(i\arctan{\pm\frac{\sqrt{-a^2 - 4b}}{a}}\right) \\[4pt]
		&= \sqrt{-b}\, \exp\left(i\arctan{\pm\sqrt{-1 - \frac{4b}{a^2}}}\right).
		\end{align*}
		Let 
		\[ \theta = \arctan{\sqrt{-1 - \frac{4b}{a^2}}} \eqand -\theta = \arctan{-\sqrt{-1 - \frac{4b}{a^2}}}. \]
		Then the eigenvalues raised to the power of $t$ are:
		\[ (\sqrt{-b})^t\, e^{i \theta t} \eqand (\sqrt{-b})^t\, e^{-i \theta t} \]
		which means that the formula becomes,
		\begin{align*}
		y_t &= C_1(\sqrt{-b})^t\, (\cos(\theta t) + i \sin(\theta t))\\[8pt]
			&\hspace{18pt} +  C_2(\sqrt{-b})^t\, (\cos(-\theta t) + i \sin(-\theta t)) \\[8pt]
			&\hspace{18pt} +  \frac{c}{a+b-1} \\[8pt]
		&= C_1(\sqrt{-b})^t\, (\cos(\theta t) + i \sin(\theta t))\\[8pt]
			&\hspace{18pt} +  C_2(\sqrt{-b})^t\, (\cos(\theta t) - i \sin(\theta t)) \\[8pt]
			&\hspace{18pt} +  \frac{c}{a+b-1} &\sidecomment{} \\[8pt]
		&= (\sqrt{-b})^t\, (C_1 + C_2)\cos(\theta t) \\[8pt]
			&\hspace{18pt} + i (\sqrt{-b})^t\, (C_1 - C_2) \sin(\theta t) \\[8pt]
			&\hspace{18pt} +  \frac{c}{a+b-1} &\sidecomment{} \\[8pt]
		&= (\sqrt{-b})^t\, (C_3\cos(\theta t) + i C_4 \sin(\theta t)) +  \frac{c}{a+b-1}.
		\end{align*}
		
		But, since we are looking for real-valued solutions and any linear combination of the homogeneous solutions is also a homogeneous solution, we can generate other, real-valued homogeneous solutions from linear combinations of these ones. In fact, we can just divide the imaginary solution by $i$ so that our real-valued solution is:
		\[ (\sqrt{-b})^t\, (C_3\cos(\theta t) + C_4 \sin(\theta t)) +  \frac{c}{a+b-1}. \]
		\TODO{eh? if divide by $i$ then the real part will be divided by $i$ also}
		
	}




% ----------------------------------


	\pagebreak
	\searchableSubsection{Markov Chains}{difference equations}{
		\biggerskip
		
		\subsubsection{Markov Matrices (a.k.a. Stochastic Matrices)}
		\note{This section taken from \href{http://people.math.harvard.edu/~knill/teaching/math19b_2011/handouts/lecture33.pdf}{Harvard}.}
		
		\boxeddefinition{An \( n \times n \) matrix is called a \textbf{Markov} or \textbf{Stochastic} matrix if all entries are nonnegative and the sum of each column vector is equal to 1.}
		
		The matrix
		\[
		A=\left[\begin{array}{ll}
		1 / 2 & 1 / 3 \\
		1 / 2 & 2 / 3
		\end{array}\right]
		\]
		is a Markov matrix.\\
		
		\note{Many authors write the transpose of the matrix and apply the matrix to the right of a row vector.}
		
		Let's call a vector with nonnegative entries \( p_{k} \) for which all the \( p_{k} \) add up to 1 a stochastic vector. For a stochastic matrix, every column is a stochastic vector.
		
		\labeledTheorem{If \( p \) is a stochastic vector and \( A \) is a stochastic matrix, then \( A p \) is a stochastic vector.}{stochastic-vector-times-stochastic-matrix-is-stochastic}
		\begin{proof}
			Let \( v_{1}, \ldots, v_{n} \) be the column vectors of \( A . \) Then
			\[
			A p=\left[\begin{array}{c}
			p_{1} \\
			p_{2} \\
			\cdots \\
			p_{n}
			\end{array}\right]=p_{1} v_{1}+\ldots+v_{n} v_{n}
			\]
			If we sum this up we get \( p_{1}+p_{2}+\ldots+p_{n}=1 \).
		\end{proof}
	
		\biggerskip
		\labeledTheorem{A Markov matrix \( A \) always has an eigenvalue \( 1 . \) All other eigenvalues are in absolute value smaller or equal to \( 1 . \)}{stochastic-matrix-eigenvalues-are-1-and-absolutely-less-than-1}
		\begin{proof}
			For the transpose matrix \( A^{T}, \) the sum of the row vectors is equal to \( 1 . \) The matrix \( A^{T} \) therefore has the eigenvector
			\[
			\left[\begin{array}{c}
			1 \\
			1 \\
			\cdots \\
			1
			\end{array}\right].
			\]
			Because \( A \) and \( A^{T} \) have the same determinant, also \( A-\lambda I_{n} \) and \( A^{T}-\lambda I_{n} \) have the same determinant so that the eigenvalues of \( A \) and \( A^{T} \) are the same (\TODO{we can reference the proof that a matrix is similar to its transpose but does this explanation also make sense?}). With \( A^{T} \) having an eigenvalue 1 also \( A \) has an eigenvalue 1 Assume now that \( v \) is an eigenvector with an eigenvalue \( |\lambda|>1 . \) Then \( A^{n} v=|\lambda|^{n} v \) has exponentially growing length for \( n \rightarrow \infty . \) This implies that there is for large \( n \) one coefficient \( \left[A^{n}\right]_{i j} \) which is larger than \( 1 . \) But \( A^{n} \) is a stochastic matrix (see homework) and has all entries \( \leq 1 . \) The assumption of an eigenvalue larger than 1 can not be valid.
		\end{proof}
	
		\note{For there to be a long-term distribution of the markov chain it is necessary that the eigenvalue 1 in the markov matrix have multiplicity 1. Otherwise, there may be more than one eigenvector corresponding with eigenvalue 1 or the matrix may not be diagonalizable. In the case that it is diagonalizable, there will only be one eigenvector with eigenvalue 1 that is also a distribution vector (stochastic vector).}
	}




% ----------------------------------


	\pagebreak
	\searchableSubsection{Differential Equations}{differential equations}{
		\bigskip\bigskip
		
		\subsubsection{Linear Ordinary Differential Equations}
		\bigskip
		
		\subsubsubsection{First Order}
		\begin{align*}
		&& \frac{dy}{dx} &= ax + b \\
		&\iff & \int \frac{dy}{dx} \dif x &= \int ax + b \dif x &\sidecomment{} \\
		&\iff & y &= a'x^2 + bx + c. &\sidecomment{${ a' = a/2}$, c is any constant}
		\end{align*}
		Integrating we see that a first order differential equation only determines the function upto a constant value. In order to determine a specific function we need a relation between a value of $x$ and a value of $y$ (i.e. a point, in graphical terms). Typically this is described as an initial condition.
		
		\bigskip\bigskip
		\subsubsubsection{Second Order}
		\begin{align*}
		&& \frac{d^2y}{dx^2} &= ax + b \\
		&\iff & \int \frac{d^2y}{dx^2} \dif x &= \int ax + b \dif x  &\sidecomment{} \\
		&\iff & \frac{dy}{dx} &= a'x^2 + bx + c  &\sidecomment{${ a' = a/2 }$, c is any constant} \\
		&\iff & \int \frac{dy}{dx} \dif x &= \int a'x^2 + bx + c \dif x  &\sidecomment{} \\
		&\iff & y &= a''x^3 + b'x^2 + cx + d. &\sidecomment{${ a'' = a/6,\, b' = b/2}$, d is any constant}
		\end{align*}
		Integrating a second order equation twice we see that we introduced two constants of integration, ${ c,d }$, and the last two terms ${ cx + d }$ are undetermined. So a second order equation of this type has only determined a function upto a first-degree polynomial (a line). To determine a specific function, in this case, we require two relations between $x$ and $y$ (two points determine a line).
		
		\biggerskip\biggerskip
		\subsubsubsection{Differential Equations of Linear Dynamical Systems}\\
		Differential equations are most often used to describe the evolving state of dynamical systems --- that is, systems whose future state is a function of its current state.\\
	
		Since the eigenfunction of differentiation is the exponent function ${ f(x) = e^x }$ (see \autoref{sss:eigenvectors-of-differentiation}), the solution to these differential equations will always involve the exponent function.
		
		\bigskip\bigskip
		\subsubsubsection{The form ${ \frac{\dif y}{\dif x} = f(x)y }$}
		The most simple form has a single $y$-term whose coefficient may be a function of $x$. This form is separable as,
		\begin{align*}
		&& \frac{\dif y}{\dif x} &= f(x)y \\[8pt]
		&\iff & \frac{1}{y} \frac{\dif y}{\dif x} &= f(x) &\sidecomment{} \\[8pt]
		&\iff & \int \frac{1}{y} \frac{\dif y}{\dif x} \dif x &= \int f(x) \dif x &\sidecomment{} \\[8pt]
		&\iff & \ln \abs{y} &= F(x) + c &\sidecomment{${ y \neq 0 }$, $F$ is an antiderivative of $f$} \\[8pt]
		&\iff & \abs{y} &= e^{F(x)}\cdot e^c &\sidecomment{} \\[8pt]
		&\iff & y &= ke^{F(x)} &\sidecomment{${ k \in \R{}. }$}
		\end{align*}
		Check solution:
		\[ \frac{\dif y}{\dif x} = f(x)ke^{F(x)} = f(x)y. \]
		\note{Note that the solution has the form,
			\[ y = ke^{F(x)} \]
			where $F(x)$ is an antiderivative of $f(x)$, the coefficient of $y$ in the original differential equation. Since the antiderivative is unique upto a constant factor, the other possible antiderivatives are achieved by the value of the coefficient $k$ because,
			\[ e^{F(x) + c} = e^{F(x)}\cdot e^c = ke^{F(x)}. \]
		}
	
		\bigskip
		\subsubsubsection{Separable Equations}
		Any equation of the form ${ \frac{\dif y}{\dif x} = f(x)y }$ is separable into the form,
		\[ f(x) \dif x = g(y) \dif y \]
		and can then be solved by integrating both sides. As a result, the solution moves on level sets of ${ \int f(x) \dif x - \int g(y) \dif y }$. This value, then, is invariant (i.e. constant) across all solutions and so, tends to represent a conserved quantity in physical systems. Therefore, these type of equations, when modelling physical systems, could be described as modelling closed systems with no external influence.
	
		\biggerskip
		\subsubsubsection{Examples}
		\begin{exe}
			\ex{Say we have an IVP (Initial Value Problem) such that our independent variable is $t$ beginning at 0, with ${ y(0) = y_0 }$, and
				\[ \frac{\dif y}{\dif t} = f(t) y.  \]
				Then if we look at what happens at integral intervals of $t$ we can see that:
				\begin{align*}
					y(0) &= y_0 \\
					y(1) &= y_0 e^{\int_0^1 f(t) dt} \\
					y(2) &= y_0 e^{\int_0^1 f(t) dt} e^{\int_1^2 f(t) dt} = y_0 e^{\int_0^2 f(t) dt} \\
					\vdots
				\end{align*}
				So, in general, at time $t$,
				\[ y(t) = y_0 e^{\int_0^t f(u) du}. \]
			}\label{ex:diff-eq-basic-ivp}
			\biggerskip
			\ex{\textbf{The Logistic:} This example isn't necessarily as realistic, but is commonly used. The logistic model of population
				growth takes $P(t)$ to be the population at time $t$. The model takes some additional factor that illustrates that a space can
				only hold a fixed carrying capacity of the population, producing the equation
				\[ \frac{\dif P}{\dif t} = c P(t) \left( 1 - \frac{P(t)}{A} \right). \]
				
				\subsubsubsection{From looking at the differential equation}
				Whatever the value of the derivative, we can see that the scalar $c$ will increase its magnitude thereby amplifying changes. So, $c$ is the growth rate and larger values of $c$ cause the population to change more rapidly.\\
				
				The steady-states of $P(t)$ are at the values such that the derivative is 0. Therefore,
				\[  c P(t) \left( 1 - \frac{P(t)}{A} \right) = 0 \implies P(t) \in \{0,1\}. \]
				Looking at the steady-states one-by-one:
				\begin{itemize}
					\item{$\bm{ P(t) = 0 }$: If $P(t)$ climbs above 0 then the derivative will be positive and so the function will be increasing away from the steady-state at 0. This is therefore an \textit{unstable} steady-state.
						\note{Note that, since $P(t)$ is a population, negative values don't make sense.}
					}
					\item{$\bm{ P(t) = A }$: If $P(t)$ is less than $A$ then the derivative will be positive and so the function will be increasing toward the steady-state at A. This is therefore a \textit{stable} steady-state from below. Also, if $P(t)$ is greater than $A$, the derivative is negative and so the function is decreasing towards the steady-state value at $A$ -- so this a \textit{stable} steady-state from above also. However, if the initial population value is less than $A$, then the population will never arrive at values above $A$ and this is the way that this function is normally used -- taking values between 0 and $A$. From either side, the larger the value of the growth rate $c$, the faster the function will approach the steady-state.
					}
				\end{itemize}
				
				\bigskip
				\subsubsubsection{Finding the solution}
				The equation is separable so we have,
				\begin{align*}
					&& \frac{\dif P}{\dif t} &= c P \left( 1 - \frac{P}{A} \right) \\[6pt]
					&\iff & \frac{\dif P}{\dif t} &= \left(\frac{c}{A}\right) P ( A - P) &\sidecomment{} \\[6pt]
					&\iff & \frac{1}{P ( A - P)} \dif P &= \left(\frac{c}{A}\right) \dif t  &\sidecomment{} \\[6pt]
					&\iff & \int \frac{1}{AP} + \frac{1}{A(A - P)} \dif P &= \int \left(\frac{c}{A}\right) \dif t  &\sidecomment{by partial fractions} \\[6pt]
					&\iff & \int \frac{1}{P} + \frac{1}{(A - P)} \dif P &= \int c \dif t  &\sidecomment{by partial fractions} \\[6pt]
					&\iff & \ln{P} - \ln{(A-P)} &= c t + D  &\sidecomment{$D$ is const. of integration} \\[6pt]
					&\iff & \ln{\left(\frac{P}{A-P}\right)} &= c t + D  &\sidecomment{} \\[6pt]
					&\iff & \frac{P}{A-P} &= B e^{ct}  &\sidecomment{$B=e^D$} \\[6pt]
					&\iff & P &= B e^{ct} (A - P)  &\sidecomment{} \\[6pt]
					&\iff & P(1 + B e^{ct}) &= AB e^{ct}  &\sidecomment{} \\[6pt]
					&\iff & P &= A \left( \frac{B e^{ct}}{1 + B e^{ct}} \right).
				\end{align*}
				\note{Note that:
					\begin{itemize}
						\item{Another common way to write the logistic function is to divide by ${  B e^{ct} }$ to get:
							\[ A \left( \frac{1}{E e^{-ct} + 1} \right) \]
							where ${ E = \frac{1}{B} }$.
						}
						\item{As has been noted: ${ 0 \leq P(t) \leq A }$. So,
							\begin{align*}
								&& 0 \leq  A &\left( \frac{B e^{ct}}{1 + B e^{ct}} \right) \leq A \\
								&\iff & 0 \leq  &\left( \frac{B e^{ct}}{1 + B e^{ct}} \right) \leq 1.  \\
							\end{align*}
							The value,
							\[ \frac{B e^{ct}}{1 + B e^{ct}} = \frac{1}{E e^{-ct} + 1} \]
							is a proportion, a value in ${ [0,1] }$.
						}
						\item{Since we have,
							\[ P(t) = A \theta \]
							where $\theta$ is a proportion and the derivative is given by,
							\[ \frac{\dif P(t)}{\dif t} = c A \theta (1 - \theta) = cA\theta - cA\theta^2 \]
							we can see that if $\theta$ is small then the derivative will be approximately,
							\[ cA\theta = cP(t). \]
							For this reason, when $\theta$ is small -- which happens when $t$ is small -- the growth of the function is approximately exponential.
						}
					\end{itemize}	 
				}
			}
		\end{exe}
		
		\bigskip\bigskip
		\subsubsubsection{The form ${ \frac{\dif y}{\dif x} = f(x)y + g(x) }$}
		This form is not separable as it is. But if we multiply both sides by $e^{-F(x)}$, where $F(x)$ is an antiderivative of $f(x)$, then
		\begin{align*}
		&& \frac{\dif y}{\dif x} &= f(x)y + g(x) \\[8pt]
		&\iff & e^{-F(x)}\frac{\dif y}{\dif x} &= e^{-F(x)}f(x)y + e^{-F(x)}g(x) &\sidecomment{} \\[8pt]
		&\iff & e^{-F(x)}\frac{\dif y}{\dif x} - e^{-F(x)}f(x)y &= e^{-F(x)}g(x) &\sidecomment{} \\[8pt]
		&\iff & \frac{\dif}{\dif x} \left(e^{-F(x)}y\right) &= e^{-F(x)}g(x) &\sidecomment{} \\[8pt]
		&\iff & \int \frac{\dif}{\dif x} \left(e^{-F(x)}y\right) \dif x &= \int e^{-F(x)}g(x) \dif x &\sidecomment{} \\[8pt]
		&\iff & e^{-F(x)}y &= \int e^{-F(x)}g(x) \dif x \\[8pt]
		&\iff & y &= e^{F(x)}\int \frac{g(x)}{e^{F(x)}} \dif x
		\end{align*}
		where the constant of integration of the integral on the left has been absorbed into the integral on the right.\\
		
		\note{Note, also, that
			\[ ke^{F(x)}\int \frac{g(x)}{ke^{F(x)}} \dif x = ke^{F(x)}\frac{1}{k}\int \frac{g(x)}{e^{F(x)}} \dif x = e^{F(x)}\int \frac{g(x)}{e^{F(x)}} \dif x. \]
		}
		
		\medskip
		So, the solution has the form,
		\[ y = k e^{F(x)} + h(x) e^{F(x)} \]
		where $h(x)$ is an antiderivative of ${ g(x)/e^{F(x)} }$ and $k$ is the constant of integration.\\
		
		Check solution:
		\begin{align*}
			y &= e^{F(x)}\int \frac{g(x)}{e^{F(x)}} \dif x \implies \\[8pt]
			\frac{\dif y}{\dif x} &= f(x)\left(e^{F(x)}\int \frac{g(x)}{e^{F(x)}} \dif x\right) + e^{F(x)}\frac{g(x)}{e^{F(x)}} \\[8pt]
			&= f(x)\left(e^{F(x)}\int \frac{g(x)}{e^{F(x)}} \dif x\right) + g(x) \\[8pt]
			&= f(x)y + g(x).
		\end{align*}
		and also,
		\begin{align*}
			y &= k e^{F(x)} + h(x) e^{F(x)} \implies \\[8pt]
			\frac{\dif y}{\dif x} &= f(x) k e^{F(x)} + f(x) h(x) e^{F(x)} + g(x) e^{-F(x)} e^{F(x)} \\[8pt]
			&= f(x) k e^{F(x)} + f(x) h(x) e^{F(x)} + g(x) \\[8pt]
			&= f(x)y + g(x)
		\end{align*}
		where we have used the fact that $h(x)$ is an antiderivative of ${ g(x) e^{-F(x)} }$.
		
		\biggerskip\hrule\biggerskip
		\paragraph{Examples}
		\begin{exe}
			\ex{${\bm{ x\frac{\dif y}{\dif x} - 2y = 6 }}$:\\\\
				The first step is to rearrange it to obtain an expression for the derivative. 
				\begin{align*}
				&& x\frac{\dif y}{\dif x} - 2y &= 6 \\
				&\iff & \frac{\dif y}{\dif x} &= \frac{2}{x}y + \frac{6}{x}. &\sidecomment{} \\
				\end{align*}
				Then we can apply the derived formula using the antiderivative ${ F(x) = 2\ln{x} }$.
				\begin{align*}
				y &= e^{2\ln{x}}\int \frac{6/x}{e^{2\ln{x}}} \dif x \\
				&= 6x^2 \int \frac{1}{x^3} \dif x &\sidecomment{} \\
				&= 6x^2\left(-\frac{1}{2x^2} + c\right) &\sidecomment{} \\
				&= -3 + c'x^2. &\sidecomment{${ c' = 6c }$} \\
				\end{align*}
				We can confirm this result by performing the calculation.
				\begin{align*}
				&& e^{-2\ln{x}}\frac{\dif y}{\dif x} &= e^{-2\ln{x}}\frac{2}{x}y + e^{-2\ln{x}}\frac{6}{x} \\
				&\iff & x^{-2}\frac{\dif y}{\dif x} - x^{-2}\frac{2}{x}y &= x^{-2}\frac{6}{x} &\sidecomment{} \\
				&\iff & x^{-2}\frac{\dif y}{\dif x} - \frac{2}{x^3}y &= \frac{6}{x^3} &\sidecomment{} \\
				&\iff & \frac{\dif }{\dif x}(x^{-2}y) &= \frac{6}{x^3} &\sidecomment{} \\
				&\iff & \int \frac{\dif }{\dif x}(x^{-2}y) \dif x &= 6 \int \frac{1}{x^3} \dif x &\sidecomment{} \\
				&\iff & x^{-2}y &= 6(-\frac{1}{2x^2} + c) &\sidecomment{} \\
				&\iff & x^{-2}y &= -3\frac{1}{x^2} + c' &\sidecomment{${ c' = 6c }$} \\
				&\iff & y &= -3 + c'x^2. &\sidecomment{} \\
				\end{align*}
			}
			\biggerskip
			\ex{Consider a bank account with variable interest. Let $M(t)$ be the amount of money in the account at time $t$, measured in years (though the specific unit isn't important conceptually), and let $I(t)$ be the rate of interest at time $t$: for example, $3 \%$ interest corresponds to $I(t)=0.03 .$ Finally, let $Q(t)$ be the amount of money put in (or negative for removing money) in year $t$. Thus, $M(t)$ obeys the differential equation
				\[ \frac{\dif M}{\dif t} = I(t) M(t) + Q(t). \]
				This illustrates a common trend: the first term indicates how it would grow independent of external forces, and the second term represents external influences.
				
				This can be solved in different ways:\\
				
				First, suppose $Q(t)=0$ and $I(t)=I$ is constant. Then ${ M(t)=M(0) e^{I t} }$.\\
				
				On the other hand, if $I(t)$ can vary, then ${ M(t) = M(0) e^{\int_{0}^{t} I(u) \dif u} }$ as explained in \ref{ex:diff-eq-basic-ivp}.\\
				
				When $Q(t)$ isn't zero, we can begin to understand it by considering the case where money is only put in once at the end of each year --- so we have ${ Q(0), Q(1), \dots }$ --- giving the solution:
				\[ M(t) = M(0) e^{\int_{0}^{t} I(u) \dif u} + Q(1) e^{\int_{1}^{t} I(u) \dif u} + Q(2) e^{\int_{2}^{t} I(u) \dif u} + \cdots \]
				In the case of continuous deposit and withdrawal from the account we end up with:
				\[ M(t) = M(0) e^{\int_{0}^{t} I(u) \dif u} + \int_0^t Q(x) e^{\int_x^t I(u) \dif u} \dif x. \]
				
				We can relate this to the general formula for this form of differential equation using \ref{ssection:definite-and-indefinite-integration} as follows:
				\begin{align*}
					M(t) &= e^{\int I(t) \dif t} \int Q(t) e^{\uminus\int I(u) \dif u} \dif t \\[8pt]
					&= e^{\int_0^t I(u) \dif u} \left( \int_0^t Q(x) e^{\uminus\int_0^x I(u) \dif u} \dif x + C \right) \\[8pt]
					&= C e^{\int_0^t I(u) \dif u}  + \int_0^t Q(x) e^{\int_x^t I(u) \dif u} \dif x
				\end{align*}
				which gives us ${ M(0) = C }$ when we substitute in ${ t = 0 }$.
				\biggerskip
			}
		\end{exe}


% -------------

	
		\pagebreak
		\subsubsubsection{Autonomous Equations and Exact Equations}
		\boxeddefinition{An \textbf{autonomous} equation refers to a differential equation with no \textbf{explicit} dependence on the independent variable (typically in dynamical systems, $t$ for time). These differential equations express change in a system based solely on the current value of the system. For example:
			\[ \frac{\dif y}{\dif x} = -k y \eqand \frac{\dif y}{\dif t} = r y(1-y). \]
			An equation of the form,
			\[ \frac{\dif y}{\dif t} = f(t) y(1-y) \]
			for example, would express that the growth rate intrinsic to the system is varying over time. Meanwhile, an equation of the form,
			\[ \frac{\dif y}{\dif x} = -k y + f(x) \]
			for example, would express that the rate of change of the system is being influenced by something that varies with the independent variable and is, in some way, extrinsic to the system (because it depends only on the independent variable and not on the state of the system).
		}
		
		\boxeddefinition{An \textbf{exact} equation refers to a differential equation whose solutions all conserve the value of some property. All separable equations are exact equations because,
			\[ f(y) \dif y = g(x) \dif x \implies \int f(y) \dif y - \int g(x) \dif x = 0. \]
			However, there are also exact equations that are not separable such as,
			\[ y' \sin{x} + y \cos{x} + 2x = 0 \implies (y \sin{x} + x^2)' = 0 \implies y = \frac{C - x^2}{\sin{x}}. \]
		}
	
		\note{All autonomous equations are separable (though the reverse is not generally true) and all separable equations are exact equations (although some exact equations are not separable). So we have,
			\[ \{\text{Autonomous}\} \subset \{\text{Separable}\} \subset \{\text{Exact}\}. \]
		}
	
		\TODO{For all autonomous equations, if ${ y = g(x) }$ is a solution then so is ${ y = g(x + c) }$ for constant $c$? In what type of equations is this not the case?}
		\begin{itemize}
			\item{Autonomous equations have direction fields that look the same across the independent variable - this is because the derivative doesn't depend on the independent variable.}
			\item{Non-separable equations cannot have steady-states because the derivative depends on a term with only the dependent variable so it cannot remain zero for all values of the independent variable greater than a certain value.}
		\end{itemize}	
		\TODO{include matlab direction fields}
	
	
		\biggerskip\biggerskip\hrule\biggerskip\biggerskip
		\subsubsubsection{Comparison of Differential Equations with Difference Equations}
		\TODO{}
		
		
		\biggerskip\biggerskip\hrule\biggerskip\biggerskip
		\subsubsubsection{Homogeneous Differential Equations}
		\boxeddefinition{A \textbf{homogeneous differential equation} is an equation of the form,
			\[ f(x,y)\frac{\dif y}{\dif x} = g(x,y) \]
			where the functions $f,g$ are both homogeneous of degree $d$.
		}
	
		\bigskip\note{The functions $f,g$ need not be linear and so these differential equations are not necessarily linear.}
		
		The key insight here is that, due to the homogeneous function property (see: \ref{section:homogeneous-functions}), if we define the function $y$ to be ${ y(x) = x \cdot v(x) }$ --- which we may do because, due to the non-trivial kernel of differentiation, we are only able to determine a solution to a differential equation upto a constant term so we can set the constant term to 0 for convenience during the calculation --- then,
		\[ f(\lambda x, \lambda y) = \lambda^d f(x,y) \implies f(x,xv) = x^d g(v). \]
		When this is applied in a differential equation of the above form we obtain,
		\begin{align*}
		&& f(x,y)\frac{\dif y}{\dif x} &= g(x,y) \\[8pt]
		&\iff & x^d f_1(v) \left(v + x\frac{\dif v}{\dif x}\right) &= x^d f_2(v) &\sidecomment{${y = xv,\,\, y' = v + xv'}$} \\[8pt]
		&\iff & f_1(v) \left(v + x\frac{\dif v}{\dif x}\right) &= f_2(v) &\sidecomment{} \\[8pt]
		&\iff & v + x\frac{\dif v}{\dif x} &= \frac{f_2(v)}{f_1(v)} = f_3(v) &\sidecomment{} \\[8pt]
		&\iff & x\frac{\dif v}{\dif x} &= f_3(v) - v = f_4(v) &\sidecomment{} \\[8pt]
		&\iff & \frac{1}{f_4(v)}\frac{\dif v}{\dif x} &= \frac{1}{x} &\sidecomment{} \\[8pt]
		&\iff & \int \frac{1}{f_4(v)}\frac{\dif v}{\dif x} \dif x &= \int \frac{1}{x} \dif x = \ln{\abs{x}} + c &\sidecomment{} \\[8pt]
		&\iff & \int \frac{1}{f_4(v)} \dif v &= \ln{\abs{x}} + c &\sidecomment{} \\[8pt]
		&\iff & \frac{1}{f_4'(v)} \ln{\abs{f_4(v)}} &= \ln{\abs{x}} + c. &\sidecomment{} \\[8pt]
		\end{align*}
	

	

% -----------------	
	
		\pagebreak
		\subsubsection{Nonlinear Differential Equations}
		\bigskip
		\note{
			\begin{itemize}
				\item{Initial conditions do not necessarily determine a unique solution \TODO{Harvard[8]}}
			\end{itemize}
		}
	}

\end{document}